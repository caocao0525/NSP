{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76c36b48",
   "metadata": {},
   "source": [
    "# CSS utility\n",
    "\n",
    "Functions that can be exploited for data pre-processing and downstream analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "706ee542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### To convert the file into .py\n",
    "# !jupyter nbconvert --to script css_utility_working.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "caaf2acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import operator\n",
    "import itertools\n",
    "from itertools import cycle\n",
    "\n",
    "import pickle\n",
    "import glob\n",
    "import ast\n",
    "import collections\n",
    "from collections import defaultdict, OrderedDict, Counter\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib import cm\n",
    "from matplotlib import rcParams\n",
    "from matplotlib.font_manager import FontProperties\n",
    "from matplotlib.textpath import TextPath\n",
    "from matplotlib.patches import PathPatch\n",
    "import matplotlib.transforms as transforms\n",
    "import networkx as nx\n",
    "\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from tslearn.metrics import dtw\n",
    "\n",
    "from tqdm import tqdm, notebook\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "\n",
    "import logomaker\n",
    "from wordcloud import WordCloud\n",
    "# import stylecloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d192315",
   "metadata": {},
   "source": [
    "### Useful Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc3d1d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict={1:\"A\", 2:\"B\", 3:\"C\", 4:\"D\", 5:\"E\",6:\"F\",7:\"G\",8:\"H\" ,\n",
    "                9:\"I\" ,10:\"J\",11:\"K\", 12:\"L\", 13:\"M\", 14:\"N\", 15:\"O\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ea36844",
   "metadata": {},
   "outputs": [],
   "source": [
    "css_name=['TssA','TssAFlnk','TxFlnk','Tx','TxWk','EnhG','Enh','ZNF/Rpts',\n",
    "          'Het','TssBiv','BivFlnk','EnhBiv','ReprPC','ReprPcWk','Quies']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6222e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "css_dict=dict(zip(list(state_dict.values()), css_name))  # css_dict={\"A\":\"TssA\", \"B\":\"TssAFlnk\", ... }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2be39465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Color dict update using the info from https://egg2.wustl.edu/roadmap/web_portal/chr_state_learning.html\n",
    "css_color_dict={'TssA':(255,0,0), # Red\n",
    "                'TssAFlnk': (255,69,0), # OrangeRed\n",
    "                'TxFlnk': (50,205,50), # LimeGreen\n",
    "                'Tx': (0,128,0), # Green\n",
    "                'TxWk': (0,100,0), # DarkGreen\n",
    "                'EnhG': (194,225,5), # GreenYellow \n",
    "                'Enh': (255,255,0),# Yellow\n",
    "                'ZNF/Rpts': (102,205,170), # Medium Aquamarine\n",
    "                'Het': (138,145,208), # PaleTurquoise\n",
    "                'TssBiv': (205,92,92), # IndianRed\n",
    "                'BivFlnk': (233,150,122), # DarkSalmon\n",
    "                'EnhBiv': (189,183,107), # DarkKhaki\n",
    "                'ReprPC': (128,128,128), # Silver\n",
    "                'ReprPCWk': (192,192,192), # Gainsboro\n",
    "                'Quies': (240, 240, 240)}  # White -> bright gray "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec02d56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_col_dict_num={'A': (1.0, 0.0, 0.0),\n",
    " 'B': (1.0, 0.271, 0.0),\n",
    " 'C': (0.196, 0.804, 0.196),\n",
    " 'D': (0.0, 0.502, 0.0),\n",
    " 'E': (0.0, 0.392, 0.0),\n",
    " 'F': (0.761, 0.882, 0.02),\n",
    " 'G': (1.0, 1.0, 0.0),\n",
    " 'H': (0.4, 0.804, 0.667),\n",
    " 'I': (0.541, 0.569, 0.816),\n",
    " 'J': (0.804, 0.361, 0.361),\n",
    " 'K': (0.914, 0.588, 0.478),\n",
    " 'L': (0.741, 0.718, 0.42),\n",
    " 'M': (0.502, 0.502, 0.502),\n",
    " 'N': (0.753, 0.753, 0.753),\n",
    " 'O': (0.941, 0.941, 0.941)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c3b8136",
   "metadata": {},
   "outputs": [],
   "source": [
    "def colors2color_dec(css_color_dict):\n",
    "    colors=list(css_color_dict.values())\n",
    "    color_dec_list=[]\n",
    "    for color in colors:\n",
    "        color_dec=tuple(rgb_elm/255 for rgb_elm in color)\n",
    "        color_dec_list.append(color_dec)        \n",
    "    return color_dec_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8075cefe",
   "metadata": {},
   "source": [
    "**scale 0 to 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd2ae382",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_col_dict=dict(zip(list(state_dict.values()),colors2color_dec(css_color_dict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a680b71",
   "metadata": {},
   "source": [
    "**scale 0 to 255**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0997385b",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_col_255_dict=dict(zip(list(state_dict.values()),list(css_color_dict.values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0558bb48",
   "metadata": {},
   "source": [
    "**hexacode**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "053b36be",
   "metadata": {},
   "outputs": [],
   "source": [
    "hexa_state_col_dict={letter: \"#{:02x}{:02x}{:02x}\".format(*rgb) for letter,rgb in state_col_255_dict.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2f9e4f",
   "metadata": {},
   "source": [
    "**name instead of alphabets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a6c55c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "css_name_col_dict=dict(zip(css_name,state_col_dict.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83df787",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "97bfb8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatLst(lst):\n",
    "    flatten_lst=[elm for sublst in lst for elm in sublst]\n",
    "    return flatten_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c717fd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Produce colorful letter-represented chromatin state sequences\n",
    "def colored_css_str_as_is(sub_str):   # convert space into space\n",
    "    col_str=\"\"\n",
    "    for letter in sub_str:\n",
    "        if letter==\" \":\n",
    "            col_str+=\" \"\n",
    "        else:                \n",
    "            for state in list(state_col_255_dict.keys()):\n",
    "                if letter==state:\n",
    "                    r=state_col_255_dict[letter][0]\n",
    "                    g=state_col_255_dict[letter][1]\n",
    "                    b=state_col_255_dict[letter][2]\n",
    "                    col_letter=\"\\033[38;2;{};{};{}m{}\\033[38;2;255;255;255m\".format(r,g,b,letter)\n",
    "                    col_str+=col_letter\n",
    "    return print(\"\\033[1m\"+col_str+\"\\033[0;0m\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1323fb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2kmer(seq, k):\n",
    "    \"\"\"\n",
    "    Convert original sequence to kmers\n",
    "    \"\"\"\n",
    "    kmer = [seq[x:x+k] for x in range(len(seq)+1-k)]\n",
    "    kmers = \" \".join(kmer)\n",
    "    return kmers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "52c8905b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmer2seq(kmers):\n",
    "    \"\"\"\n",
    "    Convert kmers to original sequence\n",
    "    \"\"\"\n",
    "    kmers_list = kmers.split(\" \")\n",
    "    bases = [kmer[0] for kmer in kmers_list[0:-1]]\n",
    "    bases.append(kmers_list[-1])\n",
    "    seq = \"\".join(bases)\n",
    "    assert len(seq) == len(kmers_list) + len(kmers_list[0]) - 1\n",
    "    return seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9414c972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe from bed file\n",
    "# bed file here means: EXXX_15_coreMarks_stateno.bed\n",
    "\n",
    "def bed2df_as_is(filename):    \n",
    "    \n",
    "    \"\"\"Create dataframe from the .bed file, as is.\n",
    "    Dataframe contains following columns:\n",
    "    chromosome |  start |  end  | state \"\"\"\n",
    "    \n",
    "    df_raw=pd.read_csv(filename, sep='\\t', lineterminator='\\n', header=None, low_memory=False)\n",
    "    df=df_raw.rename(columns={0:\"chromosome\",1:\"start\",2:\"end\",3:\"state\"})\n",
    "    df=df[:-1]\n",
    "    df[\"start\"]=pd.to_numeric(df[\"start\"])\n",
    "    df[\"end\"]=pd.to_numeric(df[\"end\"])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf42620d",
   "metadata": {},
   "source": [
    "### Main functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4c7cff50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bed2df_expanded(filename):\n",
    "    \n",
    "    \"\"\"Create an expanded dataframe from the .bed file.\n",
    "    Dataframe contains following columns:\n",
    "    chromosome |  start |  end  | state | length | unit | state_seq | state_seq_full\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        raise FileNotFoundError(\"Please provide a valid file path.\")\n",
    "\n",
    "    df_raw=pd.read_csv(filename, sep='\\t', lineterminator='\\n', header=None, low_memory=False)\n",
    "    df=df_raw.rename(columns={0:\"chromosome\",1:\"start\",2:\"end\",3:\"state\"})\n",
    "    df=df[:-1]\n",
    "    df[\"start\"]=pd.to_numeric(df[\"start\"])\n",
    "    df[\"end\"]=pd.to_numeric(df[\"end\"])\n",
    "    df[\"state\"]=pd.to_numeric(df[\"state\"])\n",
    "    df[\"length\"]=df[\"end\"]-df[\"start\"]\n",
    "    df[\"unit\"]=(df[\"length\"]/200).astype(int)  # chromatin state is annotated every 200 bp (18th May 2022)\n",
    "               \n",
    "    df[\"state_seq\"]=df[\"state\"].map(state_dict)\n",
    "    df[\"state_seq_full\"]=df[\"unit\"]*df[\"state_seq\"]\n",
    "    \n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f130e072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test for bed2df_expanded\n",
    "# test_path_bed='../database/bed/unzipped/E001_15_coreMarks_stateno.bed'\n",
    "# test_bed2df_expanded=bed2df_expanded(test_path_bed)\n",
    "# test_bed2df_expanded.head()\n",
    "# # test passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a9050028",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzipped_to_df(path_unzipped, output_path=\"./\"):\n",
    "    \"\"\"\n",
    "    Store the DataFrame converted from .bed file, cell-wise\n",
    "    - path_unzipped: the directory of your .bed files\n",
    "    - output_path: the directory where the file will be saved. Dafaults to the current working directory.\n",
    "    \"\"\"\n",
    "    unzipped_epi=sorted(os.listdir(path_unzipped))\n",
    "    unzipped_epi_files=[os.path.join(path_unzipped,file) for file in unzipped_epi]\n",
    "    for file in unzipped_epi_files:\n",
    "        cell_id=file.split(\"/\")[-1][:4]\n",
    "        # print(cell_id) ###### for test\n",
    "        \n",
    "        output_name=os.path.join(output_path,cell_id+\"_df_pickled.pkl\")\n",
    "        df=bed2df_expanded(file)\n",
    "        df.to_pickle(output_name)\n",
    "        # if cell_id==\"E002\":  ###### for test\n",
    "        #     break\n",
    "    return print(\"Files saved to {}\".format(output_path))\n",
    "# unzipped_to_df(unzipped_epi_files, output_path=\"../database/roadmap/df_pickled/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f4b98c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test for unzipped_to_df\n",
    "# path_unzipped='../database/bed/unzipped'\n",
    "# test_unzipped_to_df=unzipped_to_df(path_unzipped,output_path=\"../database/final_test\")\n",
    "# # test passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "663b00f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, learn where one chromosome ends in the df\n",
    "# this is just a prerequisite function for df2css_chr\n",
    "\n",
    "def df2chr_index(df):\n",
    "    \n",
    "    \"\"\"Create a list of smaller piece of string of the state_seq_full per chromosome\n",
    "    This function generates a list of chromatin state sequence strings chromosome-wise\"\"\"\n",
    "    \n",
    "    total_row=len(df)\n",
    "    chr_len=[]\n",
    "    chr_check=[]\n",
    "    chr_index=[]\n",
    "\n",
    "    for i in range(total_row):\n",
    "        if (df[\"start\"].iloc[i]==0) & (i >0):\n",
    "            chr_len.append(df[\"end\"].iloc[i-1]) # chr_len stores the end position of each chromosome\n",
    "            chr_check.append(df[\"start\"].iloc[i]) # for assertion : later check chr_check are all zero\n",
    "            chr_index.append(i-1) # the index (row number)\n",
    "\n",
    "    end_len=df[\"end\"].iloc[-1] # add the final end position\n",
    "    end_index=total_row-1 # add the final end index (row number)\n",
    " \n",
    "    chr_len.append(end_len)\n",
    "    chr_index.append(end_index)\n",
    "\n",
    "    assert len(chr_len)==df[\"chromosome\"].nunique() #assert the length of the list corresponds to no. of chromosome\n",
    "    assert len(chr_index)==df[\"chromosome\"].nunique()\n",
    "    \n",
    "    return chr_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "80e1e1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df2chr_df(df):\n",
    "   \n",
    "    \"\"\"Create a list of dataframes, each of which containing \n",
    "    the the whole expanded type of dataframe per chromosome\"\"\"\n",
    "    \n",
    "    start=0\n",
    "    df_chr_list=[]\n",
    "    chr_index=df2chr_index(df)\n",
    "    \n",
    "    for index in chr_index:\n",
    "        df_chr=df[start:index+1] # note that python [i:j] means from i to j-1\n",
    "        chr_name=df[\"chromosome\"].iloc[start] # string, such as chr1, chr2, ...\n",
    "        df_name='df_'+chr_name  # the chromosome-wise data stored like df_chr1, df_chr2, ...\n",
    "        locals()[df_name]=df_chr # make a string into a variable name\n",
    "        df_chr_list.append(df_chr)\n",
    "        start=index+1\n",
    "    \n",
    "    return df_chr_list   # elm is the df of each chromosome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d824533e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a long string of the css (unit length, not the real length)\n",
    "def df2unitcss(df):\n",
    "    \"\"\"\n",
    "    Create a list of 24 lists of chromatin states in string, reduced per 200 bps\n",
    "    \"\"\"\n",
    "    df_lst_chr=df2chr_df(df)\n",
    "    # remove the microchondria DNA from df_lst_chr\n",
    "    if df_lst_chr[-3][\"chromosome\"].iloc[0]==\"chrM\":\n",
    "        del df_lst_chr[-3]\n",
    "#         assert df_lst_chr[-3][\"chromosome\"].iloc[0]==\"chr22\"\n",
    "#     else:   \n",
    "#         assert df_lst_chr[-3][\"chromosome\"].iloc[0]==\"chr22\"\n",
    "    all_unit_css=[]\n",
    "    for i in range(len(df_lst_chr)):\n",
    "        df_chr=df_lst_chr[i]\n",
    "        css_chr=''\n",
    "        for j in range(len(df_chr)):\n",
    "            css_chr+=df_chr[\"unit\"].iloc[j]*df_chr[\"state_seq\"].iloc[j]\n",
    "        all_unit_css.append(css_chr)  \n",
    "    return all_unit_css"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ee62c9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test for df2unitcss\n",
    "# with open(\"../database/final_test/E001_df_pickled.pkl\",\"rb\") as f:\n",
    "#     test_df=pickle.load(f)\n",
    "# all_unit_css=df2unitcss(test_df)\n",
    "# print(len(all_unit_css))\n",
    "# print(type(all_unit_css))\n",
    "# # test passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "adb27022",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shorten_string(s, factor):\n",
    "    # This regular expression matches groups of the same character.\n",
    "    pattern = re.compile(r'(.)\\1*')\n",
    "\n",
    "    # This function will be used to replace each match.\n",
    "    def replacer(match):\n",
    "        # The group that was matched.\n",
    "        group = match.group()\n",
    "\n",
    "        # Calculate the new length, rounding as necessary.\n",
    "        new_length = round(len(group) / factor)\n",
    "\n",
    "        # Return the character repeated the new number of times.\n",
    "        return group[0] * new_length\n",
    "\n",
    "    # Use re.sub to replace each match in the string.\n",
    "    return pattern.sub(replacer, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "21719db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Convert2unitCSS_main_new(css_lst_all, unit=200):# should be either css_gene_lst_all or css_Ngene_lst_all\n",
    "    \"\"\"\n",
    "    Input: css_gene_lst_all or css_Ngene_lst_all, the list of chromosome-wise list of the css in genic, intergenic regions.\n",
    "    Output: css_gene_unit_lst_all or css_Ngene_unit_lst_all\n",
    "    \"\"\"\n",
    "    reduced_all=[]\n",
    "    for i in range(len(css_lst_all)):\n",
    "        reduced_chr=[]\n",
    "        for j in range(len(css_lst_all[i])):\n",
    "            reduced=shorten_string(css_lst_all[i][j], unit)\n",
    "            reduced_chr.append(reduced)\n",
    "        reduced_all.append(reduced_chr)\n",
    "    return reduced_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f3da5e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a long string of the css (not using unit, but the real length)\n",
    "def df2longcss(df):\n",
    "    \"\"\"\n",
    "    Create a list of 24 lists of chromatin states in string, in real length\n",
    "    \"\"\"\n",
    "    df_lst_chr=df2chr_df(df)\n",
    "    # remove the microchondria DNA from df_lst_chr\n",
    "    if df_lst_chr[-3][\"chromosome\"].iloc[0]==\"chrM\":\n",
    "        del df_lst_chr[-3]\n",
    "#         assert df_lst_chr[-3][\"chromosome\"].iloc[0]==\"chr22\"\n",
    "    elif df_lst_chr[-2][\"chromosome\"].iloc[0]==\"chrM\":\n",
    "        del df_lst_chr[-2]\n",
    "#         assert df_lst_chr[-3][\"chromosome\"].iloc[0]==\"chr22\"\n",
    "    \n",
    "    all_css=[]\n",
    "    for i in range(len(df_lst_chr)):\n",
    "        df_chr=df_lst_chr[i]\n",
    "        css_chr=''\n",
    "        for j in range(len(df_chr)):\n",
    "            css_chr+=df_chr[\"length\"].iloc[j]*df_chr[\"state_seq\"].iloc[j]\n",
    "        all_css.append(css_chr)  \n",
    "    return all_css"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a7bc9f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for preprocess the whole gene data and produce chromosome-wise gene lists\n",
    "# each element is dataframe\n",
    "\n",
    "# def whGene2GLChr(whole_gene_file='../database/RefSeq/RefSeq.WholeGene.bed'):\n",
    "def whGene2GLChr(whole_gene_file):\n",
    "    \"\"\"\n",
    "    For pre-processing the whole gene data and produce chromosome-wise gene lists\n",
    "    \"\"\"\n",
    "    print(\"Extracting the gene file ...\")\n",
    "    g_fn=whole_gene_file\n",
    "    g_df_raw=pd.read_csv(g_fn, sep='\\t', lineterminator='\\n', header=None, low_memory=False)\n",
    "    g_df_int=g_df_raw.rename(columns={0:\"chromosome\",1:\"TxStart\",2:\"TxEnd\",3:\"name\",4:\"unk0\",\n",
    "                                  5:'strand', 6:'cdsStart', 7:'cdsEnd',8:\"unk1\",9:\"exonCount\",\n",
    "                                  10:\"unk2\",11:\"unk3\"})\n",
    "    g_df=g_df_int[[\"chromosome\",\"TxStart\",\"TxEnd\",\"name\"]]\n",
    "    \n",
    "    # Remove other than regular chromosomes\n",
    "    chr_lst=['chr1','chr2','chr3','chr4','chr5','chr6','chr7','chr8','chr9','chr10',\n",
    "             'chr11','chr12','chr13','chr14','chr15','chr16','chr17','chr18','chr19',\n",
    "             'chr20','chr21','chr22','chrX','chrY']\n",
    "    g_df=g_df.loc[g_df[\"chromosome\"].isin(chr_lst)]\n",
    "    \n",
    "    # Create a list of chromosome-wise dataframe \n",
    "    g_df_chr_lst=[]\n",
    "    for num in range(len(chr_lst)):\n",
    "        chr_num=chr_lst[num]\n",
    "        g_chr_df='g_'+chr_num\n",
    "        locals()[g_chr_df]=g_df[g_df[\"chromosome\"]==chr_num]\n",
    "        g_chr_df=locals()[g_chr_df]\n",
    "        g_chr_df=g_chr_df.sort_values(\"TxStart\")\n",
    "        g_df_chr_lst.append(g_chr_df)\n",
    "    print(\"Done!\")\n",
    "    \n",
    "    return g_df_chr_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "961300db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Merging the gene table #### modified June. 29. 2023\n",
    "\n",
    "def merge_intervals(df_list):\n",
    "    merged_list = []  # List to hold merged DataFrames\n",
    "\n",
    "    for df in df_list:\n",
    "        # Sort by 'TxStart'\n",
    "        df = df.sort_values(by='TxStart')\n",
    "\n",
    "        # Initialize an empty list to store the merged intervals\n",
    "        merged = []\n",
    "\n",
    "        # Iterate through the rows in the DataFrame\n",
    "        for _, row in df.iterrows():\n",
    "            # If the list of merged intervals is empty, or the current interval does not overlap with the previous one,\n",
    "            # append it to the list\n",
    "            if not merged or merged[-1]['TxEnd'] < row['TxStart']:\n",
    "                merged.append({'TxStart': row['TxStart'], 'TxEnd': row['TxEnd']})  # Only keep 'TxStart' and 'TxEnd'\n",
    "            else:\n",
    "                # Otherwise, there is an overlap, so we merge the current and previous intervals\n",
    "                merged[-1]['TxEnd'] = max(merged[-1]['TxEnd'], row['TxEnd'])\n",
    "\n",
    "        # Convert the merged intervals back into a DataFrame and append it to the list\n",
    "        merged_list.append(pd.DataFrame(merged))\n",
    "\n",
    "    return merged_list  # a list of DF, containing only TxStart and TxEnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "78eb1d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_chrM_and_trim_gene_file_accordingly(whole_gene_file,df):\n",
    "    \n",
    "    ########### Gene without overlap ###########\n",
    "    g_df_chr_lst=whGene2GLChr(whole_gene_file)  ##### fixed June 29. 2023\n",
    "    new_gene_lst_all=merge_intervals(g_df_chr_lst) ##### fixed June 29. 2023\n",
    "    ############################################################\n",
    "\n",
    "    #### Remove chrM ###########################################\n",
    "    contains_chrM = df['chromosome'].str.contains('chrM').any()  #check whether it contains M\n",
    "    if contains_chrM:\n",
    "        df= df[~df['chromosome'].str.contains('chrM')]\n",
    "\n",
    "    contains_chrY = df['chromosome'].str.contains('chrY').any()\n",
    "\n",
    "    ##### if the target file does not contain Y, remove Y in the gene list file\n",
    "    if not contains_chrY:\n",
    "        new_gene_lst_all=new_gene_lst_all[:-1] ## the final element is for Y\n",
    "    ############################################################\n",
    "\n",
    "    assert len(df[\"chromosome\"].unique())==len(new_gene_lst_all)\n",
    "    return new_gene_lst_all, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f5bb1c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_TSS_by_loc(whole_gene_file, input_path=\"./\",output_path=\"./\",file_name=\"upNkdownNk\", up_num=2000, down_num=4000, unit=200):\n",
    "    \"\"\"\n",
    "    extract TSS region by location estimation. \n",
    "    input: (1) whole_gene_file: the raw gene bed file (e.g. RefSeq.WholeGene.bed)\n",
    "           (2) input_path: pickled df per cell\n",
    "    output: save tss_by_loc_css_unit_all at the output path\n",
    "    \"\"\"\n",
    "    file_lst=os.listdir(input_path)\n",
    "    all_files=[os.path.join(input_path,file) for file in file_lst]\n",
    "    for file in all_files:\n",
    "        cell_num=file.split(\"/\")[-1][:4]\n",
    "#         if cell_num==\"E002\": break  # for test \n",
    "        with open(file,\"rb\") as f:\n",
    "            df_pickled=pickle.load(f)\n",
    "        # align the gene file and the df file according to their availability(some cells does not have chr Y)\n",
    "        new_gene_lst_all, trimmed_df=remove_chrM_and_trim_gene_file_accordingly(whole_gene_file,df_pickled)\n",
    "        css_lst_chr = df2longcss(trimmed_df) # list of long css per chromosome\n",
    "        total_chr = len(new_gene_lst_all)       \n",
    "        tss_by_loc_css_all = []\n",
    "        for i in range(total_chr):\n",
    "            gene_start_lst = new_gene_lst_all[i][\"TxStart\"]\n",
    "            css_lst = css_lst_chr[i]\n",
    "            tss_by_loc_css_chr = []\n",
    "            for j in range(len(gene_start_lst)):\n",
    "                gene_start = gene_start_lst[j]\n",
    "                win_start = max(0, gene_start - up_num)  # use max to prevent negative index\n",
    "                win_end = min(len(css_lst), gene_start + down_num)  # use min to prevent index out of range\n",
    "                tss_by_loc_css = css_lst[win_start:win_end]\n",
    "                tss_by_loc_css_chr.append(tss_by_loc_css)               \n",
    "            tss_by_loc_css_all.append(tss_by_loc_css_chr)\n",
    "        tss_by_loc_css_unit_all=Convert2unitCSS_main_new(tss_by_loc_css_all, unit=unit)  \n",
    "        output_file_name=os.path.join(output_path,cell_num+\"_prom_\"+file_name+\".pkl\")\n",
    "        with open(output_file_name,\"wb\") as g:\n",
    "            pickle.dump(tss_by_loc_css_unit_all,g)\n",
    "\n",
    "    return print(\"All done!\") #tss_by_loc_css_unit_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b1b48ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test for save_TSS_by_loc\n",
    "# whole_gene_file='../database/RefSeq/RefSeq.WholeGene.bed'\n",
    "# save_TSS_by_loc(whole_gene_file=whole_gene_file, input_path=\"../database/roadmap/df_pickled/\",output_path=\"../database/final_test/\",file_name=\"up2kdown4k\", up_num=2000, down_num=4000, unit=200)\n",
    "# # test passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b1785a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretrain data preprocessing and storing\n",
    "\n",
    "# Preprocessing for removing continuous O state for pretrain dataset\n",
    "# 1. Sace the CSS per cell, per chromosome\n",
    "def save_css_by_cell_wo_continuous_15state(path_to_css_unit_pickled, output_path,k=4):\n",
    "    # read files from css_unit_pickled\n",
    "    files=os.listdir(path_to_css_unit_pickled)\n",
    "    file_path_lst=[os.path.join(path_to_css_unit_pickled,file) for file in files]\n",
    "    for file_path in file_path_lst:\n",
    "        file_name=os.path.basename(file_path)\n",
    "        if file_name[0] == 'E' and file_name[1:4].isdigit():\n",
    "            file_id = file_name[:4]\n",
    "        else:\n",
    "            pass\n",
    "        # ##########################\n",
    "        # if str(file_id)==\"E003\":\n",
    "        #     break  # for test\n",
    "        # ##########################\n",
    "        with open(file_path,\"rb\") as f:\n",
    "            css=pickle.load(f)\n",
    "        css_kmer=[]\n",
    "        for css_chr in css:\n",
    "            css_chr_kmer=seq2kmer(css_chr,k)\n",
    "            target_to_remove=\"O\"*k   # get rid of the word with continuous 15th state \"o\"\n",
    "            css_chr_kmer_trim = css_chr_kmer.replace(target_to_remove, \"\")\n",
    "            # clean up extra spaces\n",
    "            css_chr_kmer_trim = ' '.join(css_chr_kmer_trim.split())\n",
    "            css_kmer.append(css_chr_kmer_trim)\n",
    "        output_file_name=os.path.join(output_path,file_id+\"_unitcss_wo_all\"+str(k)+\"O_state.pkl\")    \n",
    "        with open(output_file_name, \"wb\") as g:\n",
    "            pickle.dump(css_kmer, g)  # note that it is chromosome-wise list (each element corresponds to each chromosome)\n",
    "\n",
    "        print(\"trimmed css by cell saved: \",file_id)\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ea1834fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing for removing continuous O state for pretrain dataset\n",
    "# 2. Concatenate all the cells and create one .txt file\n",
    "# (Note. new line joining chromosome-wise and cell-wise)\n",
    "def kmerCSS_to_pretrain_data(path_to_kmer_css_unit_pickled,output_path):\n",
    "    files=os.listdir(path_to_kmer_css_unit_pickled)\n",
    "    file_path_lst=[os.path.join(path_to_kmer_css_unit_pickled,file) for file in files]\n",
    "\n",
    "    css_all=[]\n",
    "    for file_path in file_path_lst:\n",
    "        file_name=os.path.basename(file_path)\n",
    "        if file_name[0] == 'E' and file_name[1:4].isdigit():\n",
    "            file_id = file_name[:4]\n",
    "        else:\n",
    "            pass\n",
    "        # ##########################\n",
    "        # if str(file_id)==\"E003\":\n",
    "        #     break  # for test\n",
    "        # # ##########################\n",
    "        with open(file_path,\"rb\") as f:\n",
    "            css=pickle.load(f)\n",
    "\n",
    "        css_per_cell='\\n'.join(css)   # join the chromosome by new line\n",
    "\n",
    "        css_all.append(css_per_cell)   \n",
    "\n",
    "    css_all_cell='\\n'.join(css_all)  # join the cell by new line\n",
    "\n",
    "    output_name=os.path.join(output_path,\"pretrain_genome_all.txt\") \n",
    "    with open(output_name, \"w\") as g:\n",
    "        g.write(css_all_cell)\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8586b674",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prom_css_Kmer_by_cell(path=\"./\", output_path=\"./\",k=4):\n",
    "    output_dir=str(k)+\"mer/\"\n",
    "    output_path_fin=os.path.join(output_path, output_dir)\n",
    "    os.makedirs(output_path_fin, exist_ok=True)\n",
    "\n",
    "    all_files=sorted([os.path.join(path, file) for file in os.listdir(path)]) \n",
    "    \n",
    "    for file in all_files:\n",
    "        prom_kmer_all=[]\n",
    "        cell_id=file.split(\"/\")[-1][:4]\n",
    "\n",
    "        # if cell_id==\"E003\": break # for test use\n",
    "        \n",
    "        with open(file, \"rb\") as f:\n",
    "            prom=pickle.load(f)\n",
    "        prom_css=flatLst(prom)  # make a list from list of a list\n",
    "        prom_kmer=[seq2kmer(item,k) for item in prom_css]\n",
    "        prom_kmer_all.append(prom_kmer)\n",
    "        prom_kmer_all_flt=flatLst(prom_kmer_all)\n",
    "        prom_kmer_all_flt_not_zero=[item for item in prom_kmer_all_flt if item!=\"\"]\n",
    "        output_name=cell_id+\"_all_genes_prom_\"+str(k)+\"merized.txt\"\n",
    "        with open(output_path_fin+output_name, \"w\") as g:\n",
    "            g.write(\"\\n\".join(prom_kmer_all_flt_not_zero))\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "51bb37ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test for prom_css_Kmer_by_cell\n",
    "# path=\"../database/roadmap/prom/up2kdown4k/all_genes/\"\n",
    "# output_path=\"../database/final_test/\"\n",
    "# prom_css_Kmer_by_cell(path=path, output_path=output_path, k=4)\n",
    "# test passed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd4d073",
   "metadata": {},
   "source": [
    "#### Pipeline \n",
    "\n",
    "(1) `prom_expGene2css` : cut the prom regions of long css <br>\n",
    "(2) `extProm_wrt_g_exp` : transform css into unit length css <br>\n",
    "(3) `extNsaveProm_g_exp` : load the required file and process all, and save"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62aaf0f",
   "metadata": {},
   "source": [
    "#### Function: `Gexp_Gene2GLChr`\n",
    "\n",
    "* This function only checks a single file.\n",
    "* Usage: After the gene expression files such as `gene_highlyexpressed.refFlat` are acquired by `/database/bed/gene_expression/classifygenes_ROADMAP_RPKM.py`, apply this function to obtain the list of dataframe per chromosome contains the transcription start and end indices.\n",
    "* Input: gene expression (high/low/not) file\n",
    "* Output: a chromosome-wise list of dataframe containing `TxStart` and `TxEnd`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2e7f9232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for preprocess the whole gene data and produce chromosome-wise gene lists\n",
    "# each element is dataframe\n",
    "\n",
    "### this function is not essential, but just to check by create df from .refFlat\n",
    "def Gexp_Gene2GLChr(exp_gene_file='../database/bed/gene_expression/E050/gene_highlyexpressed.refFlat'):\n",
    "    print(\"Extracting the gene file ...\")\n",
    "    g_fn=exp_gene_file\n",
    "    g_df_raw=pd.read_csv(g_fn, sep='\\t', index_col=False, header=0)\n",
    "    g_df=g_df_raw\n",
    "    g_df=g_df.iloc[:,1:]\n",
    "    g_df.rename(columns={\"name\":\"gene_id\"}, inplace=True)\n",
    "    g_df.rename(columns={\"#geneName\":\"geneName\"}, inplace=True)\n",
    "    g_df.rename(columns={\"txStart\":\"TxStart\"}, inplace=True) # to make it coherent to my previous codes\n",
    "    g_df.rename(columns={\"txEnd\":\"TxEnd\"}, inplace=True)\n",
    "#     g_df=g_df_raw.rename(columns={0:\"geneName\",1:\"gene_id\",2:\"chrom\",3:\"strand\",4:\"txStart\",5:\"txEnd\",\n",
    "#                                       6:\"cdsStart\",7:\"cdsEnd\",8:\"exonCount\",9:\"exonStart\",10:\"exonEnds\",\n",
    "#                                       11:\"gene type\",12:\"transcript type\",13:\"reference transcript name\",\n",
    "#                                       14:\"reference transcription id\"})\n",
    "    ## string to the list of \"int\", for exon start/end ##\n",
    "    g_df_temp=g_df # copy for processing\n",
    "    exon_start_int_lst=[]\n",
    "    for i, str_lst in enumerate(g_df_temp[\"exonStarts\"]):\n",
    "        int_lst=[int(elm) for elm in str_lst.replace(\"[\",\"\").replace(\"]\",\"\").split(\",\")]\n",
    "        assert g_df_temp[\"exonCount\"][i]==len(int_lst) # make sure the no. element in exon st count\n",
    "        exon_start_int_lst.append(int_lst)    \n",
    "    g_df_temp[\"exonStarts\"]=exon_start_int_lst\n",
    "\n",
    "    exon_end_int_lst=[]\n",
    "    for i, str_lst in enumerate(g_df_temp[\"exonEnds\"]):\n",
    "        int_lst=[int(elm) for elm in str_lst.replace(\"[\",\"\").replace(\"]\",\"\").split(\",\")]\n",
    "        assert g_df_temp[\"exonCount\"][i]==len(int_lst) # make sure the no. element in exon start = count\n",
    "        exon_end_int_lst.append(int_lst)    \n",
    "    g_df_temp[\"exonEnds\"]=exon_end_int_lst    \n",
    "    g_df=g_df_temp # and make it back the original name\n",
    "        \n",
    "    g_df=g_df[[\"geneName\",\"gene_id\",\"chrom\",\"TxStart\",\"TxEnd\"]] # extract these only\n",
    "    \n",
    "    # Remove other than regular chromosomes\n",
    "    chr_lst=['chr1','chr2','chr3','chr4','chr5','chr6','chr7','chr8','chr9','chr10',\n",
    "             'chr11','chr12','chr13','chr14','chr15','chr16','chr17','chr18','chr19',\n",
    "             'chr20','chr21','chr22','chrX','chrY']\n",
    "    g_df=g_df.loc[g_df[\"chrom\"].isin(chr_lst)]\n",
    "    \n",
    "    # Create a list of chromosome-wise dataframe \n",
    "    g_df_chr_lst=[]\n",
    "    for num in range(len(chr_lst)):\n",
    "        chr_num=chr_lst[num]\n",
    "        g_chr_df='g_'+chr_num  # name it as \"g_\"\n",
    "        locals()[g_chr_df]=g_df[g_df[\"chrom\"]==chr_num]\n",
    "        g_chr_df=locals()[g_chr_df]\n",
    "        g_chr_df=g_chr_df.sort_values(\"TxStart\")\n",
    "        g_df_chr_lst.append(g_chr_df)\n",
    "        \n",
    "    # Remove the overlapped area (using removeOverlapDF function in css_utility.py)\n",
    "    g_df_chr_collapsed_lst=[]\n",
    "    for g_df_chr in g_df_chr_lst:\n",
    "        g_df_chr_collapsed=removeOverlapDF(g_df_chr)\n",
    "        assert len(g_df_chr)>=len(g_df_chr_collapsed)\n",
    "        g_df_chr_collapsed_lst.append(g_df_chr_collapsed)\n",
    "    print(\"Done!\")\n",
    "    \n",
    "    return g_df_chr_collapsed_lst  # list of dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcb0768",
   "metadata": {},
   "source": [
    "#### Function `prom_expGene2css`\n",
    "* This function produces a long list (not unit length) of css according to the gene expression table, per cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "596c4a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prom_expGene2css(g_lst_chr_merged,df, up_num=2000, down_num=4000):   # df indicates css, created by bed2df_expanded\n",
    "    \"\"\"\n",
    "    modified from `compGene2css`\n",
    "    Input: Reference gene file trimmed for gene expresseion level, df (CSS)\n",
    "    Output: list of chromosome-wise list that contains the css at (expressed) genic area with prom only.\n",
    "    \"\"\"\n",
    "    g_lst_chr=g_lst_chr_merged\n",
    "    df = df[df['chromosome'] != 'chrM']\n",
    "    css_lst_chr=df2longcss(df) # list of long css per chromosome\n",
    "    \n",
    "    g_lst_chr = g_lst_chr[:len(css_lst_chr)]  # adjust the length of list according to length of df (might not have chrY)\n",
    "    total_chr=len(css_lst_chr)\n",
    "    \n",
    "    print(\"Matching to the chromatin state sequence data ...\")\n",
    "    css_prom_lst_all=[]\n",
    "    # for i in tqdm_notebook(range(total_chr)):\n",
    "    for i in range(total_chr):\n",
    "        css=css_lst_chr[i]   # long css of i-th chromosome\n",
    "        gene_df=g_lst_chr[i] # gene df of i-th chromosome\n",
    "        \n",
    "        css_prom_lst_chr=[]\n",
    "        for j in range(len(gene_df)):\n",
    "            prom_start=gene_df[\"TxStart\"].iloc[j]-1-up_num  # python counts form 0\n",
    "            prom_end=prom_start+up_num+down_num+1      # python excludes the end\n",
    "            if gene_df[\"TxEnd\"].iloc[j]<prom_end:  # if longer than gene body, then just gene body\n",
    "                prom_end=gene_df[\"TxEnd\"].iloc[j]+1\n",
    "    \n",
    "            css_prom=css[prom_start:prom_end]           # cut the gene area only\n",
    "            css_prom_lst_chr.append(css_prom)     # store in the list\n",
    "          \n",
    "        css_prom_lst_all.append(css_prom_lst_chr)  # list of list\n",
    "    \n",
    "    assert len(css_prom_lst_all)==total_chr\n",
    "    \n",
    "    # remove chromosome if it is empty (e.g. chrY for female)\n",
    "    css_prom_lst_all=[elm for elm in css_prom_lst_all if elm!=[]] \n",
    "    \n",
    "    print(\"Done!\")\n",
    "    return css_prom_lst_all "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7a7e2660",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extProm_wrt_g_exp(exp_gene_file, df, up_num=2000, down_num=4000,unit=200):\n",
    "    \"\"\"\n",
    "    extract promoter regions of genes according to gene expression level\n",
    "    \"\"\"\n",
    "    df = df[df['chromosome'] != 'chrM']\n",
    "    g_lst_chr=Gexp_Gene2GLChr(exp_gene_file)\n",
    "    g_lst_chr_merged=merge_intervals(g_lst_chr)\n",
    "    \n",
    "    css_prom_lst_all=prom_expGene2css(g_lst_chr_merged,df, up_num=up_num, down_num=down_num)\n",
    "    css_prom_lst_unit_all=Convert2unitCSS_main_new(css_prom_lst_all, unit=unit)\n",
    "    return css_prom_lst_unit_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8690a34f",
   "metadata": {},
   "source": [
    "#### Function: `removeOverlapDF` and `gene_removeDupl`\n",
    "\n",
    "* Main function: `gene_removeDupl`\n",
    "* `removeOverlapDF`: function used inside the main function.\n",
    "* To acquire final collapsed gene table, run `gene_removeDupl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "678e4b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeOverlapDF(test_df):    \n",
    "    new_lst=[]\n",
    "    for i in range(len(test_df)):\n",
    "        start=test_df[\"TxStart\"].iloc[i]\n",
    "        end=test_df[\"TxEnd\"].iloc[i]\n",
    "\n",
    "        exist_pair=(start,end)\n",
    "\n",
    "        if i==0:\n",
    "            new_pair=exist_pair\n",
    "            new_lst.append(new_pair)        \n",
    "        else:\n",
    "            start_pre=test_df[\"TxStart\"].iloc[i-1]\n",
    "            end_pre=test_df[\"TxEnd\"].iloc[i-1]\n",
    "\n",
    "            # first, concatenate all the shared start\n",
    "            if start==start_pre:\n",
    "                new_end=max(end, end_pre)\n",
    "                new_pair=(start, new_end)\n",
    "            # second, concatenate all the shared end\n",
    "            elif end==end_pre:\n",
    "                new_start=min(start, start_pre)\n",
    "                new_pair=(new_start, end)\n",
    "            else:    \n",
    "                new_pair=exist_pair\n",
    "\n",
    "        new_lst.append(new_pair) \n",
    "    new_lst=list(dict.fromkeys(new_lst))\n",
    "    \n",
    "    mod_lst=[[start, end] for (start, end) in new_lst] # as a list element\n",
    "\n",
    "    for j, elm in enumerate(mod_lst):\n",
    "        start, end = elm[0], elm[1]\n",
    "\n",
    "        if j==0:\n",
    "            continue\n",
    "        else:\n",
    "            start_pre=mod_lst[j-1][0]\n",
    "            end_pre=mod_lst[j-1][1]\n",
    "\n",
    "            if end_pre>=end:\n",
    "                mod_lst[j][0]=mod_lst[j-1][0]  # if end_pre is larger than end, replace start as start_pre\n",
    "                mod_lst[j][1]=mod_lst[j-1][1]  # if end_pre is larger than end, replace end as end_pre\n",
    "\n",
    "            elif start <=end_pre:\n",
    "                mod_lst[j][0]=mod_lst[j-1][0]  # current start=start_pre\n",
    "                mod_lst[j-1][1]=max(mod_lst[j][1],mod_lst[j-1][1])  # end_pre = end\n",
    "\n",
    "            else:\n",
    "                continue\n",
    "           \n",
    "    mod_lst=[tuple(elm) for elm in mod_lst]\n",
    "    fin_lst=list(dict.fromkeys(mod_lst))\n",
    "    gene_collapsed_df=pd.DataFrame(fin_lst, columns=[\"TxStart\", \"TxEnd\"])\n",
    " \n",
    "    return gene_collapsed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5d13ae3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gene_removeDupl(whole_gene_file='../database/RefSeq/RefSeq.WholeGene.bed'):\n",
    "    g_df_chr_lst=whGene2GLChr(whole_gene_file)\n",
    "    new_gene_lst_all=[]\n",
    "    for chr_no in range(len(g_df_chr_lst)):\n",
    "        gene_df=g_df_chr_lst[chr_no]\n",
    "        gene_collapsed_df=removeOverlapDF(gene_df)\n",
    "        new_gene_lst_all.append(gene_collapsed_df)\n",
    "    return new_gene_lst_all # list of chromosome-wise dataframe for collapsed gene table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a422fc9a",
   "metadata": {},
   "source": [
    "#### Function `extNsaveProm_g_exp`\n",
    "* This function processes the above works (cut the prom region and make it unit length css) per cell\n",
    "* Input\n",
    "    * `exp_gene_dir`: directory where refFlat for each cell (subdir means the sub directory for different gene expression level)\n",
    "    * `df_pickle_dir`: dataframe of each cell\n",
    "    * `rpkm_val`: RPKM value, 10, 20, 30, or 50\n",
    "    * `up_num`: upstream of gene\n",
    "    * `down_num`: from TSS (gene initial part) to cut\n",
    "    * `unit`: because chromatin states are annotated by 200 bps\n",
    "* Output: save the file according to the `rpkm_val` at the output path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b0d7dcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extNsaveProm_g_exp(exp_gene_dir=\"./\", df_pickle_dir=\"./\",output_path=\"./\",file_name=\"up2kdown4k\",rpkm_val=50, up_num=2000, down_num=4000,unit=200):\n",
    "    exp_gene_subdir=os.listdir(exp_gene_dir)\n",
    "    exp_gene_tardir=[os.path.join(exp_gene_dir, subdir) for subdir in exp_gene_subdir if str(rpkm_val) in subdir][0]    \n",
    "       \n",
    "    if rpkm_val==0:\n",
    "        exp_gene_tardir=os.path.join(exp_gene_dir, \"rpkm0\")\n",
    "        \n",
    "    exp_gene_files=sorted([os.path.join(exp_gene_tardir,file) for file in os.listdir(exp_gene_tardir)])\n",
    "\n",
    "    for exp_gene_file in exp_gene_files:\n",
    "        cell_id=exp_gene_file.split(\"/\")[-1][:4]\n",
    "\n",
    "        # print(cell_id)   ## for test\n",
    "        # if cell_id==\"E004\":break ## for test\n",
    "\n",
    "        df_name=[file for file in os.listdir(df_pickle_dir) if cell_id in file][0]\n",
    "        df_path=os.path.join(df_pickle_dir,df_name)\n",
    "        with open(df_path,\"rb\") as f:\n",
    "            df=pickle.load(f)\n",
    "        css_prom_lst_unit_all=extProm_wrt_g_exp(exp_gene_file, df, up_num=up_num, down_num=down_num,unit=unit)\n",
    "           \n",
    "        output_name=output_path+\"rpkm\"+str(rpkm_val)+\"/\"+cell_id+\"_prom_\"+file_name+\".pkl\"\n",
    "        output_dir = os.path.dirname(output_name)\n",
    "\n",
    "        # print(output_name) ### test\n",
    "        \n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "        with open(output_name, \"wb\") as g:\n",
    "            pickle.dump(css_prom_lst_unit_all,g)\n",
    "    return print(\"Saved at \",output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "52222a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test for extNsaveProm_g_exp\n",
    "# extNsaveProm_g_exp(exp_gene_dir=\"../database/roadmap/gene_exp/refFlat_byCellType/\", df_pickle_dir=\"../database/roadmap/df_pickled/\",output_path=\"../database/final_test/\",file_name=\"up2kdown4k\",rpkm_val=50, up_num=2000, down_num=4000,unit=200)\n",
    "# test passed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d2505c",
   "metadata": {},
   "source": [
    "### Extract Promoter regions from not expressed genes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8eb98e1",
   "metadata": {},
   "source": [
    "#### Pipeline \n",
    "\n",
    "(1) `extWholeGeneRef` : Just extract the whole gene location files from `chr.gene.refFlat` <br>\n",
    "(2) `extNOTexp_by_compare` : Extract the not expressed genes by comparing with whole gene with rpkm>0 <br>\n",
    "(3) `extNsaveNOTexp_by_compare` : load the required file and process all, and save refFlat (.pkl) and prom-region css (.pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "59c7ddb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extWholeGeneRef(whole_gene_ref):\n",
    "    ###### modified from Gexp_Gene2GLChr, this function provides the df of whole genes\n",
    "    ###### note that this file contains Y chromosome\n",
    "    g_fn=whole_gene_ref\n",
    "    g_df=pd.read_csv(g_fn, sep='\\t', index_col=False, header=0)\n",
    "    g_df=g_df.iloc[:,1:]\n",
    "    g_df.rename(columns={\"name\":\"gene_id\"}, inplace=True)\n",
    "    g_df.rename(columns={\"#geneName\":\"geneName\"}, inplace=True)\n",
    "    g_df.rename(columns={\"txStart\":\"TxStart\"}, inplace=True) # to make it coherent to my previous codes\n",
    "    g_df.rename(columns={\"txEnd\":\"TxEnd\"}, inplace=True)     \n",
    "    g_df=g_df[[\"chrom\",\"TxStart\",\"TxEnd\"]] # extract these only\n",
    "    # Remove other than regular chromosomes\n",
    "    chr_lst=['chr1','chr2','chr3','chr4','chr5','chr6','chr7','chr8','chr9','chr10',\n",
    "             'chr11','chr12','chr13','chr14','chr15','chr16','chr17','chr18','chr19',\n",
    "             'chr20','chr21','chr22','chrX','chrY']\n",
    "    g_df=g_df.loc[g_df[\"chrom\"].isin(chr_lst)]\n",
    "    \n",
    "    # Create a list of chromosome-wise dataframe \n",
    "    g_df_chr_lst=[]\n",
    "    for num in range(len(chr_lst)):\n",
    "        chr_num=chr_lst[num]\n",
    "        g_chr_df='g_'+chr_num  # name it as \"g_\"\n",
    "        locals()[g_chr_df]=g_df[g_df[\"chrom\"]==chr_num]\n",
    "#         print(chr_num)\n",
    "        g_chr_df=locals()[g_chr_df]\n",
    "        g_chr_df=g_chr_df.sort_values(\"TxStart\")\n",
    "        g_df_chr_lst.append(g_chr_df)\n",
    "    \n",
    "    # remove any overlap\n",
    "    g_df_chr_lst=merge_intervals(g_df_chr_lst)\n",
    "    return g_df_chr_lst  # list of chromosome-wise df for all gene start and end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "88c8fa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extNOTexp_by_compare(whole_gene_ref, cell_exp_ref):\n",
    "    \"\"\"\n",
    "    whole_gene_ref: e.g.) chr.gene.refFlat\"\n",
    "    \"\"\"\n",
    "    whole_gene_ref_lst=extWholeGeneRef(whole_gene_ref)\n",
    "    cell_exp_lst=Gexp_Gene2GLChr(cell_exp_ref)\n",
    "    cell_exp_lst=merge_intervals(cell_exp_lst) \n",
    "    if len(whole_gene_ref_lst)!=len(cell_exp_lst):\n",
    "        whole_gene_ref_lst=whole_gene_ref_lst[:-1]   \n",
    "    non_exp_gene_lst=[]\n",
    "    for i, whole_gene_chr in enumerate(whole_gene_ref_lst):\n",
    "        exp_gene_mark = whole_gene_chr.merge(cell_exp_lst[i], on=['TxStart', 'TxEnd'])\n",
    "        non_exp_gene_chr=whole_gene_chr.drop(exp_gene_mark.index)\n",
    "        non_exp_gene_lst.append(non_exp_gene_chr)\n",
    "    print(\"total length of non_expressed genes in this cell: \",len(pd.concat(non_exp_gene_lst)))\n",
    "    return non_exp_gene_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5438815f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extNsaveNOTexp_by_compare(whole_gene_ref_path,\n",
    "                              exp_ref_path=\"./\",\n",
    "                              df_pickle_dir=\"./\",\n",
    "                              output_path_ref=\"./\",\n",
    "                              output_path_prom=\"./\",\n",
    "                              up_num=2000,down_num=4000,unit=200):\n",
    "    \"\"\"\n",
    "    whole_gene_ref: e.g.) chr.gene.refFlat\"\n",
    "    \"\"\"\n",
    "    exp_ref_file_all=sorted([os.path.join(exp_ref_path,file) for file in os.listdir(exp_ref_path)])\n",
    "    \n",
    "    for exp_ref_file in exp_ref_file_all:\n",
    "        cell_id=exp_ref_file.split(\"/\")[-1][:4]\n",
    "#         if cell_id==\"E004\":break # for test\n",
    "        print(cell_id+\" is now processing...\")\n",
    "            \n",
    "        df_name=[file for file in os.listdir(df_pickle_dir) if cell_id in file][0]\n",
    "        df_path=os.path.join(df_pickle_dir,df_name)\n",
    "        with open(df_path,\"rb\") as f:\n",
    "            df=pickle.load(f)\n",
    "        \n",
    "        non_exp_gene_lst=extNOTexp_by_compare(whole_gene_ref_path, exp_ref_file) # a list of chromosome-wise df\n",
    "        #### refFlat for NOT expressed is pickled as a list of dataframe ####\n",
    "        not_exp_ref_path=output_path_ref+cell_id+\"_gene_not_expressed.pkl\"\n",
    "        with open(not_exp_ref_path,\"wb\") as g:\n",
    "            pickle.dump(non_exp_gene_lst,g)        \n",
    "        \n",
    "        css_prom_lst_all=prom_expGene2css(non_exp_gene_lst, df, up_num=up_num, down_num=down_num)\n",
    "        css_prom_lst_unit_all=Convert2unitCSS_main_new(css_prom_lst_all, unit=unit)\n",
    "        \n",
    "        output_name=output_path_prom+cell_id+\"_not_exp_gene_prom_up2kdown4k.pkl\"\n",
    "        with open(output_name,\"wb\") as h:\n",
    "            pickle.dump(css_prom_lst_unit_all,h)\n",
    "    \n",
    "    return print(\"refFlat is saved at {} and prom is saved at {}.\".format(output_path_ref, output_path_prom))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ddd7213b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # test for extNsaveNOTexp_by_compare\n",
    "# extNsaveNOTexp_by_compare(whole_gene_ref_path=\"../database/roadmap/gene_exp/chr.gene.refFlat\",\n",
    "#                               exp_ref_path=\"../database/roadmap/gene_exp/refFlat_byCellType/rpkm0/\",\n",
    "#                               df_pickle_dir=\"../database/roadmap/df_pickled/\",\n",
    "#                               output_path_ref=\"../database/roadmap/gene_exp/refFlat_byCellType/not_exp/\",\n",
    "#                               output_path_prom=\"../database/final_test/\",\n",
    "#                               up_num=2000,down_num=4000,unit=200)\n",
    "# # # test passed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8426cf",
   "metadata": {},
   "source": [
    "#### Function `prom_css_Kmer_by_cell`\n",
    "* This function saves the kmerized promoter regions (of all genes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d71b349b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prom_css_Kmer_by_cell(path=\"./\", output_path=\"./\",k=4):\n",
    "    output_dir=str(k)+\"mer/\"\n",
    "    output_path_fin=os.path.join(output_path, output_dir)\n",
    "\n",
    "    os.makedirs(output_path_fin, exist_ok=True)\n",
    "\n",
    "    all_files=sorted([os.path.join(path, file) for file in os.listdir(path)]) \n",
    "    \n",
    "    for file in all_files:\n",
    "        prom_kmer_all=[]\n",
    "        cell_id=file.split(\"/\")[-1][:4]\n",
    "        # if cell_id==\"E004\": break # for test use\n",
    "        with open(file, \"rb\") as f:\n",
    "            prom=pickle.load(f)\n",
    "        prom_css=flatLst(prom)  # make a list from list of a list\n",
    "        prom_kmer=[seq2kmer(item,k) for item in prom_css]\n",
    "        prom_kmer_all.append(prom_kmer)\n",
    "        prom_kmer_all_flt=flatLst(prom_kmer_all)\n",
    "        prom_kmer_all_flt_not_zero=[item for item in prom_kmer_all_flt if item!=\"\"]\n",
    "        output_name=cell_id+\"_all_genes_prom_\"+str(k)+\"merized.txt\"\n",
    "        with open(output_path_fin+output_name, \"w\") as g:\n",
    "            g.write(\"\\n\".join(prom_kmer_all_flt_not_zero))\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ed9ee5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test for prom_css_Kmer_by_cell\n",
    "# prom_css_Kmer_by_cell(path=\"../database/roadmap/prom/up2kdown4k/all_genes/\", output_path=\"../database/final_test/\",k=4)\n",
    "# test passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0817863",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6904cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "174bb21d",
   "metadata": {},
   "source": [
    "### Prepare fine tuning data using chromatin state sequence lists\n",
    " Note that these functions are designed for universal applicability and can be used across a wide range of general cases, not just for specific data types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c204c9ce",
   "metadata": {},
   "source": [
    "#### Pipeline \n",
    "\n",
    "(1) `css_CUT_Kmer` : Cut and K-merize the chromatin state sequence list  <br>\n",
    "(2) `prep_by_merge_cell` : Merge multiple chromatin state sequence lists into a single list <br>\n",
    "(3) `kmerize_and_cut` : Process all functions above <br><br>\n",
    "\n",
    "(4) `process_save_TF` : Prepare binary classification data, after finishing above processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e72eb87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut if it is longer than 510\n",
    "def css_CUT_Kmer(css, cut_thres=510, k=4):\n",
    "    \"\"\" \n",
    "    Prepare kmer dataset for unit_css, as is if length<=510, else cut it to be length>510   \n",
    "    Usage: css_CUT_Kmer(css, cut_thres, k)\n",
    "    \n",
    "    - css: unit-length css (e.g. comp_gene_css_all)\n",
    "    - cut_thres: length of split, default=510\n",
    "    - k: kmer\n",
    "    \n",
    "    Output: 1. splitted (before kmerization) 2. kmerized_unit_css (after kmerization) \n",
    "    \"\"\"    \n",
    "    splitted=[] # bucket for the all the splitted strings   \n",
    "    for css_elm in css:\n",
    "        if len(css_elm) <k:  # if the length of css_elm is shorter than k (cannot create k-mer)\n",
    "            continue\n",
    "        elif len(css_elm) <=cut_thres:\n",
    "            splitted.append(css_elm)\n",
    "        else:  \n",
    "            prev=0\n",
    "            while True:\n",
    "                splitted.append(css_elm[prev:prev+cut_thres])\n",
    "                prev+=cut_thres\n",
    "                if prev>=len(css_elm)-1:\n",
    "                    break      \n",
    "\n",
    "    kmerized_unit_css_raw=[seq2kmer(item, k) for item in splitted] # k-merize here\n",
    "    \n",
    "    ### this part is updated to prevent any empty string to be generated ###\n",
    "    kmerized_unit_css=[item for item in kmerized_unit_css_raw if item!=\"\"]\n",
    "    ########################################################################\n",
    "    \n",
    "    return splitted, kmerized_unit_css"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c07e7c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_by_merge_cell(input_path):  # css is the list of chromatin state sequence list in the region of interest\n",
    "    \"\"\"\n",
    "    This function merges multiple .pkl data (unit-length of Chromatin state sequence, per cell) into one\n",
    "    \n",
    "    Output\n",
    "    - A list of merged chromatin state sequences in the \"input_path\"\n",
    "    \"\"\"\n",
    "    file_list=os.listdir(input_path)\n",
    "    file_path_list=[os.path.join(input_path,file) for file in file_list]\n",
    "    # print(file_path_list)\n",
    "    \n",
    "    def contains_sublists(input_list):\n",
    "        # Check if any element in the list is a sublist (list type)\n",
    "        return any(isinstance(element, list) for element in input_list)\n",
    "    \n",
    "    css_concat=[]\n",
    "    for file_path in file_path_list:\n",
    "        with open(file_path,\"rb\") as f:\n",
    "            css=pickle.load(f)\n",
    "        # if it contains a sublist, then flatten it (normally it does)\n",
    "        # print(contains_sublists(css))\n",
    "        if contains_sublists(css):\n",
    "            \n",
    "            css=flatLst(css)\n",
    "        # print(type(css))\n",
    "        css_concat.extend(css)  \n",
    "    return css_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "263de132",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmerize_and_cut(input_path,output_path, output_file_name, k=4, cut_thres=510):\n",
    "    \"\"\"\n",
    "    This function merges all the file under input_path, and cut if an entry is longer than cut_thres,\n",
    "    and k-merize before save it at the output_path\n",
    "\n",
    "    Parameters:\n",
    "    - input_path: path to the chromatin state data in .pkl format\n",
    "    - output_path: path to the output\n",
    "    - output_file_name: desired file name.  e.g.) \"output.txt\"\n",
    "    - k: desired number for k-merization, default=4\n",
    "    - cut_thres: desired number for threshold for cutting long entries, default=510\n",
    "    \"\"\"\n",
    "    css_mergerd=prep_by_merge_cell(input_path)\n",
    "    _, kmerized_unit_css=css_CUT_Kmer(css_mergerd, cut_thres=cut_thres, k=k)\n",
    "    # print(type(kmerized_unit_css))\n",
    "    # print(len(kmerized_unit_css))\n",
    "    # Open a text file for writing\n",
    "    output_file_path=os.path.join(output_path,output_file_name)\n",
    "    with open(output_file_path, 'w') as file:\n",
    "        # Write each item on a new line\n",
    "        for item in kmerized_unit_css:\n",
    "            file.write(f\"{item}\\n\")\n",
    "    return print(\"The files in {} were merged, {}-merized, and saved at{}\".format(input_path,k,output_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8d313112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test \n",
    "# kmerize_and_cut(input_path=\"../database/test_box/input_test\",output_path=\"../database/test_box/output_test/\",\n",
    "#                 output_file_name=\"test.txt\", k=4, cut_thres=510)\n",
    "# # test passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fafa86bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_save_TF(cl1_path, cl2_path, output_path, k=4, wo_cont_o_state=True, len_tr=20000, len_dev=1000):\n",
    "    \"\"\"\n",
    "    When you have k-merized .txt file for each class, process and save it for fine-tuning data \n",
    "\n",
    "    Parameters:\n",
    "    - cl1_path (str): Path to class 1 (the class of interest) \n",
    "    - cl2_path (str): Path to class 2 (the class of comparison)\n",
    "    - output_path (str): Path to save the result file (train.tsv, dev.tsv)\n",
    "    - k (int): The length of k-mer, default=4\n",
    "    - wo_cnt_o_state (bool): whether to remove continuous O state (True to remove)\n",
    "    - len_tr: The length of train.tsv file. Default=20000\n",
    "    - len_dev: The length of dev.tsv file. Default=1000\n",
    "\n",
    "    Output:\n",
    "    - Fine-tuning data file named \"train.tsv\" and \"dev.tsv\" are saved at output_path\n",
    "\n",
    "    \"\"\"\n",
    "    cl1=pd.read_csv(cl1_path, header=None, names=[\"sequence\"])\n",
    "    cl1_list=cl1[\"sequence\"].tolist()\n",
    "    cl2=pd.read_csv(cl2_path, header=None, names=[\"sequence\"])\n",
    "    cl2_list=cl2[\"sequence\"].tolist()\n",
    "\n",
    "    if wo_cont_o_state:\n",
    "        cl1_list = [item for item in cl1_list if k*\"O\" not in item]\n",
    "        cl2_list = [item for item in cl2_list if k*\"O\" not in item]\n",
    "    \n",
    "    print(\"class 1 has {} elements.\".format(len(cl1_list)))\n",
    "    print(\"class 2 has {} elements.\".format(len(cl2_list)))\n",
    "\n",
    "    # make it dataframe\n",
    "    df_cl1=pd.DataFrame(cl1_list, columns=[\"sequence\"])\n",
    "    df_cl1[\"label\"]=1\n",
    "    df_cl2=pd.DataFrame(cl2_list, columns=[\"sequence\"])\n",
    "    df_cl2[\"label\"]=0\n",
    "    \n",
    "    # make them have the same length\n",
    "    if len(df_cl1)>len(df_cl2):\n",
    "        df_cl1=df_cl1[:len(df_cl2)] \n",
    "    elif len(df_cl1)<len(df_cl2):\n",
    "        df_cl2=df_cl2[:len(df_cl1)]\n",
    "    assert len(df_cl1)==len(df_cl2), \"Check the data length.\"\n",
    "    \n",
    "    # shuffling \n",
    "    df_all=pd.concat([df_cl1,df_cl2]).sample(frac=1).reset_index(drop=True)  \n",
    "\n",
    "    # cutting into train and dev\n",
    "    assert len(df_all)> len_tr+len_dev, \"Not enough data length.\"\n",
    "    df_train=df_all[:len_tr]\n",
    "    df_dev=df_all[len_tr:len_tr+len_dev]    \n",
    "\n",
    "    train_name=os.path.join(output_path,\"train.tsv\")\n",
    "    dev_name=os.path.join(output_path,\"dev.tsv\")\n",
    "    \n",
    "    df_train.to_csv(train_name, sep=\"\\t\", index=False)\n",
    "    df_dev.to_csv(dev_name, sep=\"\\t\", index=False)\n",
    "\n",
    "    return print(\"Fine-tuning data are saved at {}.\".format(output_path))\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "358b4c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test\n",
    "# cl1_path=\"../database/pretrain/crm/lim10/crm_lim10_allcell_4merized.txt\"\n",
    "# cl2_path=\"../database/pretrain/not_crm/lim10/not_crm_lim10_allcell_4merized.txt\"\n",
    "# output_path=\"../database/test_box/output_test/\"\n",
    "# process_save_TF(cl1_path=cl1_path, cl2_path=cl2_path, output_path=output_path, k=4, wo_cont_o_state=True, len_tr=20000, len_dev=1000)\n",
    "# # test passed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9d340e",
   "metadata": {},
   "source": [
    "#### CRM Dataset preparation\n",
    "\n",
    "The CRM regions are usually very short (for unit css length, everage is almost near 2). So the regions are screened to be longer than 6, 7, 8, 9, 10 (in terms of unit). Following functions extract CRM regions according to user-defined length and save it to the designated path. Final function saves the CRM regions with the designated length, k-mer. \n",
    "\n",
    "(1) `crm_df_maker` : prepare the dataframe of CRM from the raw bed file <br>\n",
    "(2) `extCRMfromCell` : cut the CRM regions of unit css for a sample cell <br>\n",
    "(2) `extCRMfromCell_all` : cut the CRM regions of unit css for all cells <br>\n",
    "(3) `saveCRMforPREall` : save the CRM extracted for various limit length (from 6 to 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "59713882",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crm_df_maker(crm_path=\"../database/remap2022/remap2022_crm_macs2_hg19_v1_0.bed\", limit_len=3):\n",
    "    # Load the data\n",
    "    crm_raw = pd.read_csv(crm_path, sep='\\t', header=None, names=[\"chromosome\", \"start\", \"end\", \"name\", \"score\", \"strand\", \"thickStart\", \"thickEnd\", \"itemRgb\"])\n",
    "    \n",
    "    # Convert start and end locations to units of 200 bps and calculate length\n",
    "    crm_raw['start'] = (crm_raw['start'] / 200).round().astype(int)\n",
    "    crm_raw['end'] = (crm_raw['end'] / 200).round().astype(int)\n",
    "    crm_raw['length'] = crm_raw['end'] - crm_raw['start'] + 1\n",
    "\n",
    "    # Filter by chromosome and length\n",
    "    crm_df = crm_raw[crm_raw[\"chromosome\"].str.contains('^chr[0-9XY]+$') & (crm_raw['length'] >= limit_len)].copy()\n",
    "    \n",
    "    # Define chromosome order\n",
    "    chromosome_order = ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10',\n",
    "                        'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19',\n",
    "                        'chr20', 'chr21', 'chr22', 'chrX', 'chrY']\n",
    "\n",
    "    # Convert 'chromosome' to a categorical type with the defined order\n",
    "    crm_df['chromosome'] = pd.Categorical(crm_df['chromosome'], categories=chromosome_order, ordered=True)\n",
    "\n",
    "    # Sort by 'chromosome' and 'start'\n",
    "    crm_df = crm_df.sort_values(['chromosome', 'start'])\n",
    "    crm_df_fin = crm_df[[\"chromosome\",\"start\" ,\"end\",\"length\",\"name\"]]\n",
    "\n",
    "    # Print summary\n",
    "    print(\"{} out of total {} CRM entries are longer than 200x{}, which is approx. {} %\".format(len(crm_df), len(crm_raw), limit_len, round(len(crm_df)/len(crm_raw), 3)))\n",
    "    \n",
    "    return crm_df_fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ad5dea02",
   "metadata": {},
   "outputs": [],
   "source": [
    "### cut the css according to the CRM position\n",
    "def extCRMfromCell(css_sample_path=\"../database/roadmap/css_unit_pickled/E003_unitcss_woChrM.pkl\",crm_path=\"../database/remap2022/remap2022_crm_macs2_hg19_v1_0.bed\",limit_len=4):\n",
    "    #### load unit-css per chromosome of one cell\n",
    "    with open(css_sample_path, \"rb\") as s:\n",
    "        unit_css=pickle.load(s)\n",
    "    #### make CRM as a dataframe\n",
    "    crm_df_fin=crm_df_maker(crm_path=crm_path,limit_len=limit_len)\n",
    "    \n",
    "    cut_lst_all=[]\n",
    "    for chr in range(len(unit_css)):\n",
    "        unit_css_chr=unit_css[chr]\n",
    "        chromosome_order = ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10',\n",
    "                            'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19',\n",
    "                            'chr20', 'chr21', 'chr22', 'chrX', 'chrY']\n",
    "        crm_df_chr=crm_df_fin[crm_df_fin[\"chromosome\"]==chromosome_order[chr]]\n",
    "\n",
    "        cut_lst=[]\n",
    "        for i in range(len(crm_df_chr)):\n",
    "            start_loc=crm_df_chr[\"start\"].iloc[i]\n",
    "            end_loc=crm_df_chr[\"end\"].iloc[i]\n",
    "            # length=crm_df_fin[\"length\"].iloc[i]     \n",
    "            cut=unit_css_chr[start_loc:end_loc+1]\n",
    "            cut_lst.append(cut)\n",
    "        cut_lst_all.append(cut_lst)\n",
    "    return cut_lst_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6bbb36e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extCRMfromCell_all(input_path=\"../database/roadmap/css_unit_pickled/\", crm_path=\"../database/remap2022/remap2022_crm_macs2_hg19_v1_0.bed\", output_path=\"../database/remap2022/crm/\", limit_len=6):\n",
    "    files=os.listdir(input_path)\n",
    "    css_paths=[os.path.join(input_path, file) for file in files if \"E\" in file]  # list of paths for css of cells\n",
    "    for css_sample_path in css_paths:\n",
    "        cut_lst_all=extCRMfromCell(css_sample_path=css_sample_path,crm_path=crm_path,limit_len=limit_len)\n",
    "         \n",
    "        file_name=re.search(r'E\\d{3}_unitcss_', css_sample_path).group(0)\n",
    "        output_file=os.path.join(output_path,file_name+\"limit_len\"+str(limit_len)+\".pkl\")\n",
    "        with open(output_file,\"wb\") as f:\n",
    "            pickle.dump(cut_lst_all,f)\n",
    "    return print(\"All files are saved at {}, with limit_len={}\".format(output_path, limit_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4dc89765",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveCRMforPREall_mod(input_path=\"../database/remap2022/crm/\",output_path=\"../database/pretrain/crm/\",limit_len=10, k=4): \n",
    "    files=os.listdir(os.path.join(input_path,\"lim\"+str(limit_len)))\n",
    "    css_all=[]\n",
    "    for file in files:\n",
    "        file_name=os.path.basename(file)\n",
    "        if file_name[0] == 'E' and file_name[1:4].isdigit():\n",
    "            file_id = file_name[:4]\n",
    "        else:\n",
    "            pass\n",
    "        # ##########################\n",
    "        # if str(file_id)==\"E003\":\n",
    "        #     break  # for test\n",
    "        # ##########################\n",
    "        with open(os.path.join(input_path,\"lim\"+str(limit_len),file), \"rb\") as f:\n",
    "            css_lst=pickle.load(f)\n",
    "        css=flatLst(css_lst)\n",
    "        css_kmer=[]\n",
    "        for css_chr in css:\n",
    "            css_chr_kmer=seq2kmer(css_chr,k)\n",
    "            target_to_remove=\"O\"*k   # get rid of the word with continuous 15th state \"o\"\n",
    "            css_chr_kmer_trim = css_chr_kmer.replace(target_to_remove, \"\")\n",
    "            # clean up extra spaces\n",
    "            css_chr_kmer_trim = ' '.join(css_chr_kmer_trim.split())\n",
    "            css_kmer.append(css_chr_kmer_trim)\n",
    "        css_all.append(css_kmer)\n",
    "    css_all_flt=flatLst(css_all)\n",
    "    os.makedirs(os.path.join(output_path, \"lim\" + str(limit_len)), exist_ok=True)\n",
    "    output_name=\"crm_lim\"+str(limit_len)+\"_allcell_wo_cnt_o\"+str(k)+\"merized.txt\"\n",
    "    with open(os.path.join(output_path,\"lim\"+str(limit_len),output_name), \"w\") as g:\n",
    "            g.write(\"\\n\".join(css_all_flt))\n",
    "    return print(\"File is saved at {}\".format(output_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf5d395",
   "metadata": {},
   "source": [
    "#### Motif logo visualization\n",
    "1. Logo style visualization using attention score <br><br>\n",
    "Usage: `motif_logo(mat_path, dev_path, motif=\"GBBBG\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74744ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dev_conv(dev_file_path):\n",
    "    \"\"\"\n",
    "    convert dev.tsv file to dataframe with restored sequence (original sequence)\n",
    "    \"\"\"\n",
    "    dev_df=pd.read_csv(dev_file_path,sep=\"\\t\")\n",
    "    dev_df.fillna(\" \", inplace=True) # change the nan into empty string\n",
    "    assert dev_df[\"sequence\"].isnull().sum()==0, \"check the dev file for nan values\"\n",
    "    \n",
    "    def kmer2seq_or_space(seq):\n",
    "        if seq == \" \":\n",
    "            return \" \"\n",
    "        else:\n",
    "            return kmer2seq(seq)\n",
    "    \n",
    "    dev_df[\"ori_seq\"] = dev_df[\"sequence\"].apply(kmer2seq_or_space)\n",
    "\n",
    "    return dev_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26177232",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matWcss(mat_path,dev_path):\n",
    "    \"\"\"\n",
    "    Read atten.npy and dev.tsv to create two dictionaries:\n",
    "        all_dict_1: attention matrix for label 1 per data strip\n",
    "        all_dict_0: attention matrix for label 0 per data strip\n",
    "    \"\"\"\n",
    "    atten_raw=np.load(mat_path)\n",
    "    atten=pd.DataFrame(atten_raw)\n",
    "    dev_raw=dev_conv(dev_path) #dev_conv is a function in css_utility\n",
    "    dev=dev_raw[[\"ori_seq\",\"label\"]]\n",
    "    dev.reset_index(drop=True, inplace=True)  # remove the header\n",
    "    \n",
    "    dev_label_1=dev[dev[\"label\"]==1]\n",
    "    dev_label_0=dev[dev[\"label\"]==0]\n",
    "\n",
    "    atten[\"label\"] = dev[\"label\"].values\n",
    "    atten_label_1=atten[atten[\"label\"]==1]\n",
    "    dev_label_1.pop(\"label\") # remove the label column\n",
    "    atten_label_1.pop(\"label\") # remove the label column\n",
    "    atten_label_0=atten[atten[\"label\"]==0]\n",
    "    dev_label_0.pop(\"label\") # remove the label column\n",
    "    atten_label_0.pop(\"label\") # remove the label column\n",
    "    \n",
    "    assert len(dev_label_1)==len(atten_label_1)\n",
    "    assert len(dev_label_0)==len(atten_label_0)\n",
    "    \n",
    "    all_dict_1={i:(dev_label_1.loc[i], atten_label_1.loc[i]) for i in dev_label_1.index}\n",
    "    all_dict_0={i:(dev_label_0.loc[i], atten_label_0.loc[i]) for i in dev_label_0.index}\n",
    "\n",
    "    ### to use, apply following \n",
    "#     for index, (dev_entry, atten_entry) in list(atten_dict_1.items()):\n",
    "#         dev_tar=dev_entry['ori_seq']\n",
    "#         atten_tar=atten_entry.values\n",
    "    \n",
    "    return all_dict_1, all_dict_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a08aa203",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### draw the designated entry only ######################\n",
    "def get_motifWScore(all_dict_1, motif, extend_len=0):   # modify the code for showing extended version\n",
    "    \"\"\" \n",
    "    For a class of interest (labeled either 1 or 0, but normally 1), find the motif of interest with the corresponding attention scores.\n",
    "    Outputs:\n",
    "        - motif_found_all: a list of the list of found motif in the class of interest\n",
    "        - score_found_all: a list of the list of corresponding scores (not normalized)\n",
    "        - score_found_norm_all: a list of the list of corresponding scores (normalized, per each data strip)\n",
    "\n",
    "    \"\"\"\n",
    "    #### small function to correctly show the counting number\n",
    "    def get_ordinal_suffix(number):\n",
    "        if 10 <= number % 100 <= 20:\n",
    "            suffix = 'th'\n",
    "        else:\n",
    "            suffix = {1: 'st', 2: 'nd', 3: 'rd'}.get(number % 10, 'th')\n",
    "        return suffix\n",
    "    \n",
    "    # all_dict_1 is the dictionary (key:index, value=tuple of motif and )\n",
    "    motif_found_all=[]\n",
    "    score_found_all=[]\n",
    "    score_found_norm_all=[]\n",
    "\n",
    "    cnt=0\n",
    "    for index, (dev_entry, atten_entry) in list(all_dict_1.items()):\n",
    "        dev_tar = dev_entry['ori_seq']\n",
    "        atten = atten_entry.values.reshape(-1, 1).T  # Reshape to 2D array for heatmap\n",
    "        atten_tar = atten[0] # atten is a list of list, with one element\n",
    "\n",
    "#         if motif in dev_tar:\n",
    "#             start_index=dev_tar.find(motif)\n",
    "#             end_index=start_index+len(motif)\n",
    "        if motif in dev_tar:\n",
    "            start_index=dev_tar.find(motif)-extend_len\n",
    "            end_index=start_index+len(motif)+(extend_len*2)\n",
    "            if end_index <= len(atten_tar)+1:\n",
    "                cnt+=1\n",
    "                print(f\"{motif}: {cnt}{get_ordinal_suffix(cnt)} appearance\")\n",
    "                print(\"- Index in attention matrix: \",index)\n",
    "                print(\"- Score min-max: {} ~ {}\".format(round(min(atten_tar),3), round(max(atten_tar),3)))\n",
    "                motif_found=dev_tar[start_index:end_index]\n",
    "                score_found=atten_tar[start_index:end_index].tolist()\n",
    "                score_found_norm=[item/sum(atten_tar) for item in score_found]  # normalize\n",
    "                \n",
    "                motif_found_all.append(motif_found)\n",
    "                score_found_all.append(score_found)\n",
    "                score_found_norm_all.append(score_found_norm)\n",
    "\n",
    "                ### create the motif figures \n",
    "                show_len=end_index-start_index\n",
    "                fig_width=show_len*0.35\n",
    "                \n",
    "                ######## Data strip with colored scores (to show the motif on the colored strip)\n",
    "                \n",
    "                # plt.figure(figsize=(fig_width, 1))  # Adjusted height to give space for text\n",
    "                # # Add colored text for each letter in dev_tar above the heatmap\n",
    "                # for i, letter in enumerate(motif_found):                    \n",
    "                #     plt.text(i + 0.5, -0.2, letter, color=state_col_dict.get(letter, 'black'),\n",
    "                #              ha='center', va='center', fontsize=32, family='monospace')\n",
    "\n",
    "                # sns.heatmap(data=[score_found], cmap=\"viridis\", yticklabels=False, cbar=False)\n",
    "                # plt.show()\n",
    "    return motif_found_all, score_found_all, score_found_norm_all #list of list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bec86ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score2logo(motif_found_all, score_found_all, score_found_norm_all, norm=False):\n",
    "    \"\"\"\n",
    "    Use the result of get_motifWScore, create logos\n",
    "    \"\"\"\n",
    "    import logomaker\n",
    "    num_row=len(motif_found_all[0])\n",
    "    columns=[chr(i) for i in range(ord(\"A\"), ord(\"O\")+1)]\n",
    "    \n",
    "    # df for creating logo\n",
    "    df_logo_all=[]\n",
    "    df=pd.DataFrame(0.0, index=range(num_row), columns=columns)\n",
    "    for i, motif_found in enumerate(motif_found_all):\n",
    "        if norm:\n",
    "            for j, letter in enumerate(motif_found):\n",
    "                df.loc[j, letter]=round(score_found_norm_all[i][j],3)\n",
    "        elif not norm:\n",
    "            for j, letter in enumerate(motif_found):\n",
    "                df.loc[j, letter]=round(score_found_all[i][j],3)   \n",
    "        df_logo=df.copy()\n",
    "        df_logo_all.append(df_logo)\n",
    "\n",
    "        fig_width=num_row*0.5\n",
    "        logo=logomaker.Logo(df_logo,color_scheme=state_col_dict_num, figsize=(fig_width,0.8))\n",
    "        logo.style_spines(visible=False)\n",
    "        logo.style_spines(spines=[\"left\",\"bottom\"], visible=True)\n",
    "        plt.show()\n",
    "    \n",
    "    \n",
    "    # Calculate the average\n",
    "    total_df = pd.DataFrame()\n",
    "    for df_logo in df_logo_all:\n",
    "        total_df = total_df.add(df_logo, fill_value=0)\n",
    "    average_df = total_df / len(df_logo_all)\n",
    "    \n",
    "    extracted_values = average_df.max(axis=1).tolist()   # make the average score into a list\n",
    "    extracted_values = [round(item,3) for item in  extracted_values] \n",
    "    \n",
    "    df_logo_all.append(average_df)\n",
    "    \n",
    "    print(\"---------- Average Score Motif Logo ----------\")\n",
    "    logo2=logomaker.Logo(average_df,color_scheme=state_col_dict_num, figsize=(fig_width+1,1))\n",
    "    logo2.style_spines(visible=False)\n",
    "    logo2.style_spines(spines=[\"left\",\"bottom\"], visible=True)\n",
    "    plt.show()\n",
    "#     print(average_df)\n",
    "    logo_score = df_logo_all[-1].max(axis=1).tolist()\n",
    "    logo_score = [round(item,3) for item in  extracted_values] \n",
    "    print(\"Average logo score:\",logo_score)\n",
    "    \n",
    "    return logo_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5988c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def motif_logo(mat_path, dev_path, motif):\n",
    "    all_dict_1, all_dict_0=get_matWcss(mat_path,dev_path)\n",
    "    motif_found_all,score_found_all,score_found_norm_all=get_motifWScore(all_dict_1, motif=motif)\n",
    "    logo_score=score2logo(motif_found_all,score_found_all,score_found_norm_all)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "91cb98db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### test\n",
    "# mat_path=\"../database/ft_result/pred/4_gene_exp/test02_double_data/Ghexp_rpkm30_or_not/atten.npy\"\n",
    "# dev_path=\"../database/fine_tune/gene_exp/4mer/Ghexp_rpkm30_or_not/tr_len_40k/dev.tsv\"\n",
    "# motif_logo(mat_path, dev_path, motif=\"GBBBG\")\n",
    "# #### test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f4e273",
   "metadata": {},
   "source": [
    "#### Motif Word Cloud Visualization\n",
    "\n",
    "* Usage: `motif2wordcloud(motif_dir, color_map=\"viridis\")`\n",
    "* Note that `motif_dir` is a directory where the motif files are collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1104ac94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def motif2wordcloud(path, color_map=\"viridis\"):\n",
    "    target=[word for word in path.split(\"/\")[-2:] if word !=\"\"][0]\n",
    "    print(\"target\", target)\n",
    "    file_lst=[os.path.join(path,file) for file in os.listdir(path) if \".txt\" in file]\n",
    "    motifs={}\n",
    "    for file_name in file_lst:\n",
    "        motif, num_txt=file_name.split(\"/\")[-1].split(\"_\")[1:3]\n",
    "        freq=num_txt.split(\".\")[0]\n",
    "        motifs[motif]=int(freq)\n",
    "    print(\"motifs = \", motifs)\n",
    "    wc=WordCloud(width=800, height=400, background_color=\"white\", colormap=color_map)\n",
    "    wordcloud=wc.generate_from_frequencies(motifs)\n",
    "    plt.figure(figsize=(6,2), facecolor=None)\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout(pad=0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b433275f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### test\n",
    "# old_path=\"../database/motif/motif_test/\"\n",
    "# motif2wordcloud(old_path, color_map=\"viridis\")\n",
    "# #### test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e1218f",
   "metadata": {},
   "source": [
    "#### Motif Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9b2dc8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def motif_init2df(input_path=\"./init_concat.csv\"):\n",
    "    \"\"\"\n",
    "    Read init.csv file and convert it to \n",
    "    \"\"\"\n",
    "    df=pd.read_csv(input_path)\n",
    "    data_lst=df[\"motif\"].to_list()\n",
    "    def convert_sequence(sequence, mapping):\n",
    "        return [mapping[letter] for letter in sequence]\n",
    "    letter_to_num = {'A': 1,'B': 2,'C': 3,'D': 4,'E': 5,'F': 6,'G': 7,\n",
    "                     'H': 8,'I': 9,'J': 10,'K': 11,'L': 12,'M': 13,'N': 14,'O': 15}\n",
    "    numerical_sequences=[convert_sequence(seq, letter_to_num) for seq in data_lst]\n",
    "    df_sequences = pd.DataFrame(numerical_sequences).astype('Int64').T\n",
    "    # Add an 'entry' column at the beginning of the DataFrame with labels 'Entry 1', 'Entry 2', etc.\n",
    "    df_sequences.insert(0, 'position', ['Pos ' + str(i+1) for i in range(df_sequences.shape[0])])\n",
    "    return df_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5270e145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test for motif_init2df\n",
    "# df_sequences=motif_init2df(input_path=\"./init_concat.csv\")\n",
    "# df_sequences\n",
    "# test passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7438019e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def motif_init2pred_with_dendrogram(input_path=\"./init_concat.csv\", categorical=False, fillna_method=\"ffill\", n_clusters=None, linkage_method=\"complete\", threshold=35):\n",
    "    \"\"\"\n",
    "    Read init.csv file and directly predict the class using DTW and Agglomerative Clustering.\n",
    "    This version includes forward-reverse comparison. A dendrogram is provided to help the user \n",
    "    determine the optimal number of clusters.\n",
    "\n",
    "    To run without specifying n_clusters, set n_clusters to None and adjust the threshold (e.g., 10 to 100).\n",
    "    To run with a specified number of clusters, set n_clusters to the desired number.\n",
    "    \"\"\"\n",
    "\n",
    "    def dataframe_reverse_and_push_nan(df):\n",
    "        # Reverse rows of each column except 'position'\n",
    "        df_rev = df.loc[:, df.columns != 'position'].apply(lambda col: col[::-1].values, axis=0)\n",
    "        # Add the 'position' column back without changing its order\n",
    "        df_rev.insert(0, 'position', df['position'])\n",
    "        # Reset row index to ensure continuous row index\n",
    "        df_rev = df_rev.reset_index(drop=True)\n",
    "\n",
    "        # Push NaN values to the end of each column except 'position'\n",
    "        for col in df_rev.columns:\n",
    "            if col != 'position':\n",
    "                non_nan = df_rev[col].dropna()\n",
    "                nan_count = df_rev[col].isna().sum()\n",
    "                df_rev[col] = pd.concat([non_nan, pd.Series([pd.NA] * nan_count)], ignore_index=True)\n",
    "        \n",
    "        return df_rev\n",
    "\n",
    "    df_sequences = motif_init2df(input_path=input_path)\n",
    "    # print(f\"Initial number of entries: {len(df_sequences)}\")  # Debug print\n",
    "\n",
    "    X_train = df_sequences.loc[:, df_sequences.columns != 'position']\n",
    "    if fillna_method == 0:\n",
    "        X_train_filled = X_train.fillna(0)\n",
    "    elif fillna_method == \"ffill\":\n",
    "        X_train_filled = X_train.fillna(method=fillna_method)\n",
    "    # Add more conditions for other fillna methods if needed\n",
    "    n_columns = X_train_filled.shape[1]\n",
    "    # print(f\"Number of entries after filling NaNs: {n_columns}\")  # Debug print\n",
    "\n",
    "    dtw_distance_matrix = np.zeros((n_columns, n_columns))\n",
    "\n",
    "    df_sequences_rev = dataframe_reverse_and_push_nan(df_sequences)  # Reverse\n",
    "    X_train_rev = df_sequences_rev.loc[:, df_sequences_rev.columns != 'position']\n",
    "    if fillna_method == 0:\n",
    "        X_train_filled_rev = X_train_rev.fillna(0)\n",
    "    elif fillna_method == \"ffill\":\n",
    "        X_train_filled_rev = X_train_rev.fillna(method=fillna_method)\n",
    "    # Add more conditions for other fillna methods if needed\n",
    "\n",
    "    def categorical_distance(a, b):\n",
    "        return 0 if a == b else 1\n",
    "\n",
    "    def dtw_categorical(seq1, seq2):\n",
    "        n, m = len(seq1), len(seq2)\n",
    "        dtw = np.full((n+1, m+1), np.inf)\n",
    "        dtw[0, 0] = 0\n",
    "        for i in range(1, n+1):\n",
    "            for j in range(1, m+1):\n",
    "                cost = categorical_distance(seq1[i-1], seq2[j-1])\n",
    "                dtw[i, j] = cost + min(dtw[i-1, j],    # insertion\n",
    "                                    dtw[i, j-1],    # deletion\n",
    "                                    dtw[i-1, j-1])  # match\n",
    "        return dtw[n, m]\n",
    "\n",
    "    import time\n",
    "    start_dtw = time.time()\n",
    "    for i in range(n_columns):\n",
    "        for j in range(i, n_columns):  # No need to compute the distance twice for (i, j) and (j, i)\n",
    "            if categorical:\n",
    "                distance_ff=dtw_categorical(X_train_filled.iloc[:, i].values, X_train_filled.iloc[:, j].values)  # Forward-forward\n",
    "                distance_fr=dtw_categorical(X_train_filled.iloc[:, i].values, X_train_filled_rev.iloc[:, j].values)  # Forward-reverse\n",
    "            else:\n",
    "                distance_ff = dtw(X_train_filled.iloc[:, i].values, X_train_filled.iloc[:, j].values)  # Forward-forward\n",
    "                distance_fr = dtw(X_train_filled.iloc[:, i].values, X_train_filled_rev.iloc[:, j].values)  # Forward-reverse\n",
    "\n",
    "                # Select the minimum distance and assign symmetrically\n",
    "            min_distance = min(distance_ff, distance_fr)\n",
    "            dtw_distance_matrix[i, j] = min_distance\n",
    "            dtw_distance_matrix[j, i] = min_distance\n",
    "            \n",
    "            \n",
    "    end_dtw = time.time()\n",
    "    print(f\"DTW computation time: {end_dtw - start_dtw} seconds\")\n",
    "\n",
    "    # Measure time for clustering\n",
    "    start_clustering = datetime.now()\n",
    "    \n",
    "    # Added part for dendrogram\n",
    "    # Create the linkage matrix\n",
    "    Z = linkage(dtw_distance_matrix, method=linkage_method)\n",
    "    \n",
    "    # ############\n",
    "    # max_distance = max(Z[:, 2])\n",
    "    # threshold_test = max_distance / 2\n",
    "    # print(\"threshold_test\",threshold_test)\n",
    "    # ############\n",
    "\n",
    "    # Plot the dendrogram\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    # dendrogram(Z)\n",
    "    dendrogram(Z, color_threshold=threshold)\n",
    "    plt.title('Dendrogram')\n",
    "    plt.xlabel('Sample index')\n",
    "    plt.ylabel('Distance')\n",
    "    plt.show()\n",
    "    # End of added part for dendrogram\n",
    "\n",
    "    if n_clusters is None:\n",
    "        # Determine clusters using fcluster with a distance threshold\n",
    "        y_pred = fcluster(Z, threshold, criterion='distance')\n",
    "        estimated_clusters = len(np.unique(y_pred))\n",
    "        print(f\"Estimated number of clusters: {estimated_clusters}\")\n",
    "    else:\n",
    "        # Use Agglomerative Clustering with the precomputed DTW distance matrix\n",
    "        clustering = AgglomerativeClustering(n_clusters=n_clusters, metric='precomputed', linkage=linkage_method)\n",
    "        y_pred = clustering.fit_predict(dtw_distance_matrix)\n",
    "        estimated_clusters = n_clusters\n",
    "    \n",
    "    end_clustering = datetime.now()\n",
    "    print('Clustering Duration: {}'.format(end_clustering - start_clustering))\n",
    "    \n",
    "    print(f\"Number of cluster estimated by dendrogram with designated threshold {threshold}: [{estimated_clusters}] clusters\")\n",
    "\n",
    "\n",
    "    # print(\"threshold_test\",threshold_test)\n",
    "    \n",
    "    return y_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5dcb858f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test for motif_init2pred_with_dendrogram\n",
    "# motif_init2pred_with_dendrogram(categorical=True)\n",
    "# # test passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "77a820ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def motif_init2pred(input_path=\"./init_concat.csv\", categorical=False, fillna_method=\"ffill\", n_clusters=11, linkage_method=\"complete\"):\n",
    "    \"\"\"\n",
    "    Read init.csv file and directly predict the class using DTW and Agglomerative Clustering.\n",
    "    This version includes forward-reverse comparison.\n",
    "    This version considers the difference in state as categorical if categorical=True.\n",
    "\n",
    "    Parameters:\n",
    "    - input_path (str): Path to the input CSV file.\n",
    "    - categorical (bool): Whether to treat the data as categorical.\n",
    "    - fillna_method (str or int): Method to fill missing values. Use \"ffill\" for forward fill or 0 to fill with zeros.\n",
    "    - n_clusters (int): Number of clusters for Agglomerative Clustering.\n",
    "    - linkage_method (str): Linkage method for Agglomerative Clustering. Options are \"complete\", \"average\", or \"single\".\n",
    "    \n",
    "    Returns:\n",
    "    - dtw_distance_matrix (np.ndarray): DTW distance matrix.\n",
    "    - y_pred (np.ndarray): Predicted cluster labels.\n",
    "\n",
    "    \"\"\"\n",
    "    def dataframe_reverse_and_push_nan(df):\n",
    "        # Reverse rows of each column except 'position'\n",
    "        df_rev = df.loc[:, df.columns != 'position'].apply(lambda col: col[::-1].values, axis=0)\n",
    "        # Add the 'position' column back without changing its order\n",
    "        df_rev.insert(0, 'position', df['position'])\n",
    "        # Reset row index to ensure continuous row index\n",
    "        df_rev = df_rev.reset_index(drop=True)\n",
    "        # Push NaN values to the end of each column except 'position'\n",
    "        for col in df_rev.columns:\n",
    "            if col != 'position':\n",
    "                non_nan = df_rev[col].dropna()\n",
    "                nan_count = df_rev[col].isna().sum()\n",
    "                df_rev[col] = pd.concat([non_nan, pd.Series([pd.NA] * nan_count)], ignore_index=True)    \n",
    "        return df_rev\n",
    "    \n",
    "    df_sequences = motif_init2df(input_path=input_path)\n",
    "    # print(f\"Initial number of entries: {len(df_sequences)}\")  # Debug print\n",
    "\n",
    "    if fillna_method not in [0, \"ffill\"]:\n",
    "        raise ValueError(\"Invalid fillna_method. Use 0 or 'ffill'.\")\n",
    "\n",
    "    X_train = df_sequences.loc[:, df_sequences.columns != 'position']\n",
    "    if fillna_method==0:\n",
    "        X_train_filled = X_train.fillna(0)\n",
    "    if fillna_method==\"ffill\":\n",
    "        X_train_filled = X_train.fillna(method=fillna_method) \n",
    "    n_columns = X_train_filled.shape[1]\n",
    "    # print(f\"Number of entries after filling NaNs: {n_columns}\")  # Debug print\n",
    "\n",
    "    dtw_distance_matrix = np.zeros((n_columns, n_columns))\n",
    "\n",
    "    df_sequences_rev = dataframe_reverse_and_push_nan(df_sequences)  # Reverse\n",
    "    X_train_rev = df_sequences_rev.loc[:, df_sequences_rev.columns != 'position']\n",
    "    if fillna_method==0:\n",
    "        X_train_filled_rev = X_train_rev.fillna(0)\n",
    "    if fillna_method==\"ffill\":\n",
    "        X_train_filled_rev = X_train_rev.fillna(method=fillna_method) \n",
    "    # X_train_filled_rev = X_train_rev.fillna(fillna_method)  # Fill missing values with zero    \n",
    "\n",
    "    def categorical_distance(a, b):\n",
    "        return 0 if a == b else 1\n",
    "\n",
    "    def dtw_categorical(seq1, seq2):\n",
    "        n, m = len(seq1), len(seq2)\n",
    "        dtw = np.full((n+1, m+1), np.inf)\n",
    "        dtw[0, 0] = 0\n",
    "        for i in range(1, n+1):\n",
    "            for j in range(1, m+1):\n",
    "                cost = categorical_distance(seq1[i-1], seq2[j-1])\n",
    "                dtw[i, j] = cost + min(dtw[i-1, j],    # insertion\n",
    "                                    dtw[i, j-1],    # deletion\n",
    "                                    dtw[i-1, j-1])  # match\n",
    "        return dtw[n, m]\n",
    "    \n",
    "    import time\n",
    "    start_dtw = time.time()\n",
    "    for i in range(n_columns):\n",
    "        for j in range(i, n_columns):  # No need to compute the distance twice for (i, j) and (j, i)\n",
    "            if categorical:\n",
    "                distance_ff=dtw_categorical(X_train_filled.iloc[:, i].values, X_train_filled.iloc[:, j].values)  # Forward-forward\n",
    "                distance_fr=dtw_categorical(X_train_filled.iloc[:, i].values, X_train_filled_rev.iloc[:, j].values)  # Forward-reverse\n",
    "            else:\n",
    "                distance_ff = dtw(X_train_filled.iloc[:, i].values, X_train_filled.iloc[:, j].values)  # Forward-forward\n",
    "                distance_fr = dtw(X_train_filled.iloc[:, i].values, X_train_filled_rev.iloc[:, j].values)  # Forward-reverse\n",
    "\n",
    "                # Select the minimum distance and assign symmetrically\n",
    "            min_distance = min(distance_ff, distance_fr)\n",
    "            dtw_distance_matrix[i, j] = min_distance\n",
    "            dtw_distance_matrix[j, i] = min_distance\n",
    "            \n",
    "    end_dtw = time.time()\n",
    "    print(f\"DTW computation time: {end_dtw - start_dtw} seconds\")\n",
    "\n",
    "    # Measure time for clustering\n",
    "    start_clustering = datetime.now()\n",
    "    \n",
    "    # Use Agglomerative Clustering with the precomputed DTW distance matrix\n",
    "    # for linkage option, \n",
    "    # complete: mazimize the minimum distance between points in different clusters\n",
    "    # average: uses the average distance between all points in the two clusters\n",
    "    # single: minimize the distance between the closest points of the clusters\n",
    "    clustering = AgglomerativeClustering(n_clusters=n_clusters, metric='precomputed', linkage=linkage_method)\n",
    "    y_pred = clustering.fit_predict(dtw_distance_matrix)\n",
    "    \n",
    "    end_clustering = datetime.now()\n",
    "    print('Clustering Duration: {}'.format(end_clustering - start_clustering))\n",
    "    \n",
    "    print(f\"Number of cluster labels: {len(y_pred)}\")  # Debug print\n",
    "    # return X_train_filled, X_train_filled_rev, y_pred\n",
    "    return dtw_distance_matrix, y_pred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ddf37a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test\n",
    "# motif_init2pred(categorical=True)\n",
    "# # test passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f8cab815",
   "metadata": {},
   "outputs": [],
   "source": [
    "def motif_init2class(input_path=\"./init_concat.csv\", categorical=False, fillna_method=\"ffill\", n_clusters=11, linkage_method=\"complete\"): #,fillna_method='ffill'):\n",
    "    df_sequences=motif_init2df(input_path=input_path)\n",
    "\n",
    "    # Transpose df_test so that each entry becomes a row\n",
    "    df_seq_transposed = df_sequences.T  \n",
    "    # The first row will likely contain something other than data (e.g., time points), so let's keep it as a header\n",
    "    new_header = df_seq_transposed.iloc[0]  # Grab the first row for the header\n",
    "    df_seq_transposed = df_seq_transposed[1:]  # Take the data less the header row\n",
    "    df_seq_transposed.columns = new_header  # Set the header row as the df header\n",
    "    # Reset the index to make the entries into a column\n",
    "    df_seq_transposed.reset_index(inplace=True)\n",
    "    # Rename the 'index' column to something more descriptive, like 'Entry'\n",
    "    df_seq_transposed.rename(columns={'index': 'Entry'}, inplace=True)\n",
    "\n",
    "    _, y_pred=motif_init2pred(input_path=input_path,categorical=categorical,fillna_method=fillna_method,  n_clusters=n_clusters, linkage_method=linkage_method)\n",
    "\n",
    "    # Add the cluster labels as a new column\n",
    "    df_seq_transposed['Cluster'] = y_pred\n",
    "    # Sort the DataFrame by the 'Cluster' column\n",
    "    df_sorted_by_cluster = df_seq_transposed.sort_values(by='Cluster')\n",
    "    # Reset the index of the sorted DataFrame\n",
    "    df_sorted_by_cluster.reset_index(drop=True, inplace=True)\n",
    "    # Display the sorted DataFrame\n",
    "    # df_sorted_by_cluster\n",
    "    # Reverse the letter_to_num mapping\n",
    "    letter_to_num = {'A': 1,'B': 2,'C': 3,'D': 4,'E': 5,'F': 6,'G': 7,\n",
    "                        'H': 8,'I': 9,'J': 10,'K': 11,'L': 12,'M': 13,'N': 14,'O': 15}\n",
    "    num_to_letter = {v: k for k, v in letter_to_num.items()}\n",
    "\n",
    "    # Function to convert a series of numbers to a letter string, ignoring NaNs\n",
    "    def series_to_letters(series):\n",
    "        return ''.join([num_to_letter.get(x, '') for x in series if pd.notna(x)])\n",
    "\n",
    "    # Apply the conversion to each row (excluding the 'Cluster' column) and add the result to a new column\n",
    "    df_sorted_by_cluster['LetterSequence'] = df_sorted_by_cluster.drop('Cluster', axis=1).apply(series_to_letters, axis=1)\n",
    "\n",
    "    # Group by 'Cluster' and aggregate 'LetterSequence' into lists\n",
    "    clustered_sequences = df_sorted_by_cluster.groupby('Cluster')['LetterSequence'].apply(list).reset_index()\n",
    "\n",
    "    # Display the result\n",
    "    return clustered_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "412d4590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test\n",
    "# motif_init2class(categorical=True)\n",
    "# # test passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fc7f697f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def motif_init2class_vis(input_path=\"./init_concat.csv\", categorical=False, fillna_method=\"ffill\", n_clusters=11, linkage_method=\"complete\"):\n",
    "    \"\"\"\n",
    "    Read init.csv file and visualize the predicted class using DTW and Agglomerative Clustering.\n",
    "    This version includes forward-reverse comparison.\n",
    "    This version considers the difference in state as categorical if categorical=True.\n",
    "\n",
    "    Parameters:\n",
    "    - input_path (str): Path to the input CSV file.\n",
    "    - categorical (bool): Whether to treat the data as categorical.\n",
    "    - fillna_method (str or int): Method to fill missing values. Use \"ffill\" for forward fill or 0 to fill with zeros.\n",
    "    - n_clusters (int): Number of clusters for Agglomerative Clustering.\n",
    "\n",
    "    \n",
    "    Returns:\n",
    "    - Visualization of each motif clustered in the same line color and line style.\n",
    "    - X axis indicates the position of the chromatin state in a sequence\n",
    "    - Y axis represents the type of chromatin state\n",
    "    \"\"\"\n",
    "    df_sequences = motif_init2df(input_path=input_path)\n",
    "    _, y_pred = motif_init2pred(input_path=input_path, categorical=categorical,fillna_method=fillna_method, n_clusters=n_clusters, linkage_method=linkage_method)\n",
    "\n",
    "    from itertools import cycle\n",
    "    from matplotlib import cm\n",
    "\n",
    "    # Set the figure size and legend location\n",
    "    rcParams[\"figure.figsize\"] = (12, 6)\n",
    "    rcParams[\"legend.loc\"] = 'upper right'\n",
    "\n",
    "    # Assuming df_sequences is your DataFrame and y_pred is your array of predicted cluster labels\n",
    "    item_list = df_sequences.columns.tolist()[1:]\n",
    "\n",
    "    # Define a list of linestyles\n",
    "    linestyles = ['-', '--', '-.', ':']\n",
    "\n",
    "    # Create a cycle object from the linestyles list\n",
    "    linestyle_cycle = cycle(linestyles)\n",
    "\n",
    "    # Assign a linestyle to each cluster, cycling through the available styles\n",
    "    cluster_linestyles = {i: next(linestyle_cycle) for i in range(n_clusters)}\n",
    "    \n",
    "    # Load a colormap\n",
    "    cmap = cm.get_cmap('tab20', n_clusters) \n",
    "    \n",
    "    # Map y_pred to colors\n",
    "    cluster_colors = {i: cmap(i / n_clusters) for i in range(n_clusters)}\n",
    "\n",
    "    # Create a figure and a subplot\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    # Use these colors in your plot\n",
    "    for index, item in enumerate(item_list):\n",
    "        linestyle = cluster_linestyles[y_pred[index]]\n",
    "        color = cluster_colors[y_pred[index]]  # Get color from cluster_colors\n",
    "        ax.plot(df_sequences[\"position\"], df_sequences[item].astype('float'), \n",
    "                label=f\"{item}_cluster{y_pred[index]}\", \n",
    "            linestyle=linestyle, color=color)\n",
    "    \n",
    "\n",
    "    ax.set_xticks(df_sequences[\"position\"])\n",
    "    ax.set_xticklabels(df_sequences['position'].str.extract('(\\d+)')[0].astype(int),fontsize=14)\n",
    "    y_ticks = np.arange(1, len(state_col_dict_num.keys()) + 1)\n",
    "    ax.set_yticks(y_ticks)\n",
    "    ax.set_yticklabels(list(state_col_dict_num.keys()), fontsize=14)  \n",
    "    # fig.savefig(\"./test_vis1.png\",bbox_inches='tight', dpi=300)\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "92685746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test\n",
    "# motif_init2class_vis()\n",
    "# # test passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "45a03067",
   "metadata": {},
   "outputs": [],
   "source": [
    "def motif_init2cluster_vis(input_path=\"./init_concat.csv\", categorical=False, n_clusters=11, fillna_method=\"ffill\", linkage_method=\"complete\", random_state=82, font_scale=0.004,font_v_scale=9, fig_w=12, fig_h=8, node_size=1000, node_dist=0.05):\n",
    "    clustered_sequences=motif_init2class(input_path=input_path, categorical=categorical, fillna_method=fillna_method, n_clusters=n_clusters, linkage_method=linkage_method)\n",
    "    scale_factor = font_scale  # Adjust this to change the font size\n",
    "\n",
    "    def create_text_patch(x, y, text, state_col_dict_num, ax, scale_factor):\n",
    "        # Determine the starting x position for the first letter\n",
    "        x_offset = x\n",
    "        for letter in text:\n",
    "            color = state_col_dict_num.get(letter, (0, 0, 0))\n",
    "            # fp = FontProperties(family=\"Arial\", weight=\"bold\")\n",
    "            fp = FontProperties(family=\"DejaVu Sans\", weight=\"bold\")\n",
    "            tp = TextPath((0, 0), letter, prop=fp)\n",
    "            tp_transformed = transforms.Affine2D().scale(scale_factor).translate(x_offset, y) + ax.transData\n",
    "            letter_patch = PathPatch(tp, color=color, lw=0, transform=tp_transformed)\n",
    "            ax.add_patch(letter_patch)\n",
    "            # Get the width of the letter and add a small margin\n",
    "            letter_width = tp.get_extents().width * scale_factor\n",
    "            x_offset += letter_width  # Increment the x position by the width of the letter\n",
    "\n",
    "    df = clustered_sequences\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(fig_w, fig_h))  # Adjust figure size as needed\n",
    "    \n",
    "    ##### color modification for temporary use #####\n",
    "    # Create a temporary copy with a different name\n",
    "    temp_state_col_dict_num = state_col_dict_num.copy()\n",
    "\n",
    "    # Modify the colors in the temporary dictionary\n",
    "    # Update 'G' to a more visible color, such as a deep orange\n",
    "    temp_state_col_dict_num['G'] = (1.0, 0.647, 0.0)  # Normalized deep orange\n",
    "\n",
    "    # Update 'O' to ensure it stands out more, such as a darker gray\n",
    "    temp_state_col_dict_num['O'] = (0.502, 0.502, 0.502)  # Normalized darker gray\n",
    "\n",
    "    ################################################\n",
    "\n",
    "    # Create a graph\n",
    "    G = nx.Graph()\n",
    "    for index, row in df.iterrows():\n",
    "        G.add_node(row['Cluster'], elements=row['LetterSequence'])\n",
    "\n",
    "    # Significantly increase the base size for each node\n",
    "    base_node_size = node_size  # This increases the node size\n",
    "    node_sizes = [len(elements) * base_node_size for elements in df['LetterSequence']]\n",
    "\n",
    "    # Generate a color palette with a unique color for each node\n",
    "    colors = plt.cm.rainbow(np.linspace(0, 1, len(df)))\n",
    "\n",
    "    np.random.seed(random_state)\n",
    "    # Draw the graph with a spring layout\n",
    "    # Adjust k to manage the distance between nodes, which can be smaller since nodes can overlap\n",
    "    pos = nx.spring_layout(G, k=node_dist, iterations=10)\n",
    "\n",
    "    # Draw the nodes themselves\n",
    "    nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color=colors, alpha=0.3)\n",
    "\n",
    "    # Draw the text\n",
    "    for node, (node_pos, elements) in enumerate(zip(pos.values(), df['LetterSequence'])):      \n",
    "        x_start, y_start = node_pos\n",
    "        for i, element in enumerate(elements):\n",
    "            x_position = x_start - 0.08\n",
    "            y_position = y_start - (i * scale_factor * font_v_scale) + 0.015*len(elements)# Adjust line spacing\n",
    "            create_text_patch(x_position, y_position, element, temp_state_col_dict_num, ax, scale_factor)\n",
    "#             print(\"state_col_dict_num\", state_col_dict_num)\n",
    "\n",
    "    plt.axis('off')\n",
    "    print(n_clusters)\n",
    "    plt.show()\n",
    "    \n",
    "    fig.savefig(\"./cluster_result.png\",bbox_inches='tight', dpi=300,facecolor='white',  # Set the background color to white\n",
    "    edgecolor='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9a225437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test\n",
    "# motif_init2cluster_vis(categorical=True)\n",
    "# # test passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7cb45069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def motif_init2umap(input_path=\"./init_concat.csv\",categorical=False,  n_clusters=11, fillna_method=\"ffill\", linkage_method=\"complete\", n_neighbors=5, min_dist=0.3, random_state=2):\n",
    "    \"\"\"\n",
    "    Generate a UMAP embedding of the given data.\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    - input_path: .csv file of all motifs with high attention score\n",
    "    \n",
    "    - n_clusters: number of clusters\n",
    "\n",
    "    - n_neighbors: int (default=5), The size of local neighborhood (in terms of number of neighboring sample points) \n",
    "      used for manifold approximation. Larger values result in a more global view of the manifold, while smaller values emphasize local data structures. \n",
    "      Adjust according to the desired granularity of the embedding.\n",
    "      \n",
    "    - mid_dist: float (default=0.3), The minimum distance between embedded points in the low-dimensional space. \n",
    "      Smaller values allow points to cluster more tightly in the embedding, which is useful for identifying finer substructures within the data. \n",
    "      Larger values help preserve the overall topology of the data by preventing points from clustering too tightly.\n",
    "    \"\"\"\n",
    "    df_sequences = motif_init2df(input_path=input_path)\n",
    "    X_train = df_sequences.loc[:, df_sequences.columns != 'position']\n",
    "    X_train = X_train.astype('float64')  # Convert to float64\n",
    "    # X_filled = X_train.fillna(X_train.mean())\n",
    "    if fillna_method==0:\n",
    "        X_train_filled = X_train.fillna(0)\n",
    "    if fillna_method==\"ffill\":\n",
    "        X_train_filled = X_train.fillna(method=fillna_method) \n",
    "\n",
    "    dtw_distance_matrix, y_pred = motif_init2pred(input_path=input_path, categorical=categorical, n_clusters=n_clusters, fillna_method=fillna_method, linkage_method=linkage_method)\n",
    "\n",
    "    # Now apply UMAP on the cleaned data\n",
    "    from umap import UMAP\n",
    "    # # seed=111\n",
    "    # # umap_reducer = UMAP(n_neighbors=n_neighbors, min_dist=min_dist, random_state=seed)\n",
    "    # umap_reducer = UMAP(n_neighbors=n_neighbors, min_dist=min_dist)\n",
    "    umap_reducer = UMAP(n_neighbors=n_neighbors, min_dist=min_dist, random_state=random_state, n_jobs=1)\n",
    "\n",
    "    # umap_embedding = umap_reducer.fit_transform(X_train_filled.T)  # Ensure the data is transposed if necessary\n",
    "    umap_embedding = umap_reducer.fit_transform(dtw_distance_matrix)  # Ensure the data is transposed if necessary\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    scatter = plt.scatter(umap_embedding[:, 0], umap_embedding[:, 1], c=y_pred, cmap='Spectral', s=100, edgecolors='white', linewidth=0.6)\n",
    "\n",
    "    # Create a color bar with ticks for each cluster label\n",
    "    colorbar = plt.colorbar(scatter, ticks=np.arange(0, 11))\n",
    "    colorbar.set_label('Cluster label')\n",
    "\n",
    "    # Set the plot title and labels\n",
    "    plt.title('UMAP Projection After Agglomerative Clustering', fontsize=14)\n",
    "    plt.xlabel('UMAP Dimension 1', fontsize=14)\n",
    "    plt.ylabel('UMAP Dimension 2', fontsize=14)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fabe3829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test\n",
    "# motif_init2umap(categorical=True)\n",
    "# # test passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5135af9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b850d96b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2fa4d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad27158d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a18237",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chrombert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
