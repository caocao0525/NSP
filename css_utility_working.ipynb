{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76c36b48",
   "metadata": {},
   "source": [
    "# CSS utility\n",
    "\n",
    "Functions that can be exploited for data pre-processing and downstream analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "706ee542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### To convert the file into .py\n",
    "# !jupyter nbconvert --to script css_utility_working.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "caaf2acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import operator\n",
    "import itertools\n",
    "import pickle\n",
    "import glob\n",
    "import ast\n",
    "import collections\n",
    "from collections import defaultdict, OrderedDict, Counter\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib import rcParams\n",
    "from matplotlib.font_manager import FontProperties\n",
    "from matplotlib.textpath import TextPath\n",
    "from matplotlib.patches import PathPatch\n",
    "import matplotlib.transforms as transforms\n",
    "import networkx as nx\n",
    "\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from tslearn.metrics import dtw\n",
    "\n",
    "from tqdm import tqdm, notebook\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "# import stylecloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d192315",
   "metadata": {},
   "source": [
    "### Useful Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc3d1d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict={1:\"A\", 2:\"B\", 3:\"C\", 4:\"D\", 5:\"E\",6:\"F\",7:\"G\",8:\"H\" ,\n",
    "                9:\"I\" ,10:\"J\",11:\"K\", 12:\"L\", 13:\"M\", 14:\"N\", 15:\"O\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ea36844",
   "metadata": {},
   "outputs": [],
   "source": [
    "css_name=['TssA','TssAFlnk','TxFlnk','Tx','TxWk','EnhG','Enh','ZNF/Rpts',\n",
    "          'Het','TssBiv','BivFlnk','EnhBiv','ReprPC','ReprPcWk','Quies']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6222e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "css_dict=dict(zip(list(state_dict.values()), css_name))  # css_dict={\"A\":\"TssA\", \"B\":\"TssAFlnk\", ... }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2be39465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Color dict update using the info from https://egg2.wustl.edu/roadmap/web_portal/chr_state_learning.html\n",
    "css_color_dict={'TssA':(255,0,0), # Red\n",
    "                'TssAFlnk': (255,69,0), # OrangeRed\n",
    "                'TxFlnk': (50,205,50), # LimeGreen\n",
    "                'Tx': (0,128,0), # Green\n",
    "                'TxWk': (0,100,0), # DarkGreen\n",
    "                'EnhG': (194,225,5), # GreenYellow \n",
    "                'Enh': (255,255,0),# Yellow\n",
    "                'ZNF/Rpts': (102,205,170), # Medium Aquamarine\n",
    "                'Het': (138,145,208), # PaleTurquoise\n",
    "                'TssBiv': (205,92,92), # IndianRed\n",
    "                'BivFlnk': (233,150,122), # DarkSalmon\n",
    "                'EnhBiv': (189,183,107), # DarkKhaki\n",
    "                'ReprPC': (128,128,128), # Silver\n",
    "                'ReprPCWk': (192,192,192), # Gainsboro\n",
    "                'Quies': (240, 240, 240)}  # White -> bright gray "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec02d56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_col_dict_num={'A': (1.0, 0.0, 0.0),\n",
    " 'B': (1.0, 0.271, 0.0),\n",
    " 'C': (0.196, 0.804, 0.196),\n",
    " 'D': (0.0, 0.502, 0.0),\n",
    " 'E': (0.0, 0.392, 0.0),\n",
    " 'F': (0.761, 0.882, 0.02),\n",
    " 'G': (1.0, 1.0, 0.0),\n",
    " 'H': (0.4, 0.804, 0.667),\n",
    " 'I': (0.541, 0.569, 0.816),\n",
    " 'J': (0.804, 0.361, 0.361),\n",
    " 'K': (0.914, 0.588, 0.478),\n",
    " 'L': (0.741, 0.718, 0.42),\n",
    " 'M': (0.502, 0.502, 0.502),\n",
    " 'N': (0.753, 0.753, 0.753),\n",
    " 'O': (0.941, 0.941, 0.941)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c3b8136",
   "metadata": {},
   "outputs": [],
   "source": [
    "def colors2color_dec(css_color_dict):\n",
    "    colors=list(css_color_dict.values())\n",
    "    color_dec_list=[]\n",
    "    for color in colors:\n",
    "        color_dec=tuple(rgb_elm/255 for rgb_elm in color)\n",
    "        color_dec_list.append(color_dec)        \n",
    "    return color_dec_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8075cefe",
   "metadata": {},
   "source": [
    "**scale 0 to 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd2ae382",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_col_dict=dict(zip(list(state_dict.values()),colors2color_dec(css_color_dict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a680b71",
   "metadata": {},
   "source": [
    "**scale 0 to 255**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0997385b",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_col_255_dict=dict(zip(list(state_dict.values()),list(css_color_dict.values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0558bb48",
   "metadata": {},
   "source": [
    "**hexacode**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "053b36be",
   "metadata": {},
   "outputs": [],
   "source": [
    "hexa_state_col_dict={letter: \"#{:02x}{:02x}{:02x}\".format(*rgb) for letter,rgb in state_col_255_dict.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2f9e4f",
   "metadata": {},
   "source": [
    "**name instead of alphabets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6c55c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "css_name_col_dict=dict(zip(css_name,state_col_dict.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83df787",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97bfb8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatLst(lst):\n",
    "    flatten_lst=[elm for sublst in lst for elm in sublst]\n",
    "    return flatten_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c717fd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Produce colorful letter-represented chromatin state sequences\n",
    "def colored_css_str_as_is(sub_str):   # convert space into space\n",
    "    col_str=\"\"\n",
    "    for letter in sub_str:\n",
    "        if letter==\" \":\n",
    "            col_str+=\" \"\n",
    "        else:                \n",
    "            for state in list(state_col_255_dict.keys()):\n",
    "                if letter==state:\n",
    "                    r=state_col_255_dict[letter][0]\n",
    "                    g=state_col_255_dict[letter][1]\n",
    "                    b=state_col_255_dict[letter][2]\n",
    "                    col_letter=\"\\033[38;2;{};{};{}m{}\\033[38;2;255;255;255m\".format(r,g,b,letter)\n",
    "                    col_str+=col_letter\n",
    "    return print(\"\\033[1m\"+col_str+\"\\033[0;0m\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1323fb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2kmer(seq, k):\n",
    "    \"\"\"\n",
    "    Convert original sequence to kmers\n",
    "    \"\"\"\n",
    "    kmer = [seq[x:x+k] for x in range(len(seq)+1-k)]\n",
    "    kmers = \" \".join(kmer)\n",
    "    return kmers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52c8905b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmer2seq(kmers):\n",
    "    \"\"\"\n",
    "    Convert kmers to original sequence\n",
    "    \"\"\"\n",
    "    kmers_list = kmers.split(\" \")\n",
    "    bases = [kmer[0] for kmer in kmers_list[0:-1]]\n",
    "    bases.append(kmers_list[-1])\n",
    "    seq = \"\".join(bases)\n",
    "    assert len(seq) == len(kmers_list) + len(kmers_list[0]) - 1\n",
    "    return seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9414c972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe from bed file\n",
    "# bed file here means: EXXX_15_coreMarks_stateno.bed\n",
    "\n",
    "def bed2df_as_is(filename):    \n",
    "    \n",
    "    \"\"\"Create dataframe from the .bed file, as is.\n",
    "    Dataframe contains following columns:\n",
    "    chromosome |  start |  end  | state \"\"\"\n",
    "    \n",
    "    df_raw=pd.read_csv(filename, sep='\\t', lineterminator='\\n', header=None, low_memory=False)\n",
    "    df=df_raw.rename(columns={0:\"chromosome\",1:\"start\",2:\"end\",3:\"state\"})\n",
    "    df=df[:-1]\n",
    "    df[\"start\"]=pd.to_numeric(df[\"start\"])\n",
    "    df[\"end\"]=pd.to_numeric(df[\"end\"])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf42620d",
   "metadata": {},
   "source": [
    "### Main functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c7cff50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bed2df_expanded(filename):\n",
    "    \n",
    "    \"\"\"Create an expanded dataframe from the .bed file.\n",
    "    Dataframe contains following columns:\n",
    "    chromosome |  start |  end  | state | length | unit | state_seq | state_seq_full\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        raise FileNotFoundError(\"Please provide a valid file path.\")\n",
    "\n",
    "    df_raw=pd.read_csv(filename, sep='\\t', lineterminator='\\n', header=None, low_memory=False)\n",
    "    df=df_raw.rename(columns={0:\"chromosome\",1:\"start\",2:\"end\",3:\"state\"})\n",
    "    df=df[:-1]\n",
    "    df[\"start\"]=pd.to_numeric(df[\"start\"])\n",
    "    df[\"end\"]=pd.to_numeric(df[\"end\"])\n",
    "    df[\"state\"]=pd.to_numeric(df[\"state\"])\n",
    "    df[\"length\"]=df[\"end\"]-df[\"start\"]\n",
    "    df[\"unit\"]=(df[\"length\"]/200).astype(int)  # chromatin state is annotated every 200 bp (18th May 2022)\n",
    "               \n",
    "    df[\"state_seq\"]=df[\"state\"].map(state_dict)\n",
    "    df[\"state_seq_full\"]=df[\"unit\"]*df[\"state_seq\"]\n",
    "    \n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f130e072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test for bed2df_expanded\n",
    "# test_path_bed='../database/bed/unzipped/E001_15_coreMarks_stateno.bed'\n",
    "# test_bed2df_expanded=bed2df_expanded(test_path_bed)\n",
    "# test_bed2df_expanded.head()\n",
    "# # test passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a9050028",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzipped_to_df(path_unzipped, output_path=\"./\"):\n",
    "    \"\"\"\n",
    "    Store the DataFrame converted from .bed file, cell-wise\n",
    "    - path_unzipped: the directory of your .bed files\n",
    "    - output_path: the directory where the file will be saved. Dafaults to the current working directory.\n",
    "    \"\"\"\n",
    "    unzipped_epi=sorted(os.listdir(path_unzipped))\n",
    "    unzipped_epi_files=[os.path.join(path_unzipped,file) for file in unzipped_epi]\n",
    "    for file in unzipped_epi_files:\n",
    "        cell_id=file.split(\"/\")[-1][:4]\n",
    "        # print(cell_id) ###### for test\n",
    "        \n",
    "        output_name=os.path.join(output_path,cell_id+\"_df_pickled.pkl\")\n",
    "        df=bed2df_expanded(file)\n",
    "        df.to_pickle(output_name)\n",
    "        # if cell_id==\"E002\":  ###### for test\n",
    "        #     break\n",
    "    return print(\"Files saved to {}\".format(output_path))\n",
    "# unzipped_to_df(unzipped_epi_files, output_path=\"../database/roadmap/df_pickled/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f4b98c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test for unzipped_to_df\n",
    "# path_unzipped='../database/bed/unzipped'\n",
    "# test_unzipped_to_df=unzipped_to_df(path_unzipped,output_path=\"../database/final_test\")\n",
    "# # test passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "663b00f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, learn where one chromosome ends in the df\n",
    "# this is just a prerequisite function for df2css_chr\n",
    "\n",
    "def df2chr_index(df):\n",
    "    \n",
    "    \"\"\"Create a list of smaller piece of string of the state_seq_full per chromosome\n",
    "    This function generates a list of chromatin state sequence strings chromosome-wise\"\"\"\n",
    "    \n",
    "    total_row=len(df)\n",
    "    chr_len=[]\n",
    "    chr_check=[]\n",
    "    chr_index=[]\n",
    "\n",
    "    for i in range(total_row):\n",
    "        if (df[\"start\"].iloc[i]==0) & (i >0):\n",
    "            chr_len.append(df[\"end\"].iloc[i-1]) # chr_len stores the end position of each chromosome\n",
    "            chr_check.append(df[\"start\"].iloc[i]) # for assertion : later check chr_check are all zero\n",
    "            chr_index.append(i-1) # the index (row number)\n",
    "\n",
    "    end_len=df[\"end\"].iloc[-1] # add the final end position\n",
    "    end_index=total_row-1 # add the final end index (row number)\n",
    " \n",
    "    chr_len.append(end_len)\n",
    "    chr_index.append(end_index)\n",
    "\n",
    "    assert len(chr_len)==df[\"chromosome\"].nunique() #assert the length of the list corresponds to no. of chromosome\n",
    "    assert len(chr_index)==df[\"chromosome\"].nunique()\n",
    "    \n",
    "    return chr_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "80e1e1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df2chr_df(df):\n",
    "   \n",
    "    \"\"\"Create a list of dataframes, each of which containing \n",
    "    the the whole expanded type of dataframe per chromosome\"\"\"\n",
    "    \n",
    "    start=0\n",
    "    df_chr_list=[]\n",
    "    chr_index=df2chr_index(df)\n",
    "    \n",
    "    for index in chr_index:\n",
    "        df_chr=df[start:index+1] # note that python [i:j] means from i to j-1\n",
    "        chr_name=df[\"chromosome\"].iloc[start] # string, such as chr1, chr2, ...\n",
    "        df_name='df_'+chr_name  # the chromosome-wise data stored like df_chr1, df_chr2, ...\n",
    "        locals()[df_name]=df_chr # make a string into a variable name\n",
    "        df_chr_list.append(df_chr)\n",
    "        start=index+1\n",
    "    \n",
    "    return df_chr_list   # elm is the df of each chromosome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d824533e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a long string of the css (unit length, not the real length)\n",
    "def df2unitcss(df):\n",
    "    \"\"\"\n",
    "    Create a list of 24 lists of chromatin states in string, reduced per 200 bps\n",
    "    \"\"\"\n",
    "    df_lst_chr=df2chr_df(df)\n",
    "    # remove the microchondria DNA from df_lst_chr\n",
    "    if df_lst_chr[-3][\"chromosome\"].iloc[0]==\"chrM\":\n",
    "        del df_lst_chr[-3]\n",
    "#         assert df_lst_chr[-3][\"chromosome\"].iloc[0]==\"chr22\"\n",
    "#     else:   \n",
    "#         assert df_lst_chr[-3][\"chromosome\"].iloc[0]==\"chr22\"\n",
    "    all_unit_css=[]\n",
    "    for i in range(len(df_lst_chr)):\n",
    "        df_chr=df_lst_chr[i]\n",
    "        css_chr=''\n",
    "        for j in range(len(df_chr)):\n",
    "            css_chr+=df_chr[\"unit\"].iloc[j]*df_chr[\"state_seq\"].iloc[j]\n",
    "        all_unit_css.append(css_chr)  \n",
    "    return all_unit_css"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ee62c9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test for df2unitcss\n",
    "# with open(\"../database/final_test/E001_df_pickled.pkl\",\"rb\") as f:\n",
    "#     test_df=pickle.load(f)\n",
    "# all_unit_css=df2unitcss(test_df)\n",
    "# print(len(all_unit_css))\n",
    "# print(type(all_unit_css))\n",
    "# # test passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "adb27022",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shorten_string(s, factor):\n",
    "    # This regular expression matches groups of the same character.\n",
    "    pattern = re.compile(r'(.)\\1*')\n",
    "\n",
    "    # This function will be used to replace each match.\n",
    "    def replacer(match):\n",
    "        # The group that was matched.\n",
    "        group = match.group()\n",
    "\n",
    "        # Calculate the new length, rounding as necessary.\n",
    "        new_length = round(len(group) / factor)\n",
    "\n",
    "        # Return the character repeated the new number of times.\n",
    "        return group[0] * new_length\n",
    "\n",
    "    # Use re.sub to replace each match in the string.\n",
    "    return pattern.sub(replacer, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "21719db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Convert2unitCSS_main_new(css_lst_all, unit=200):# should be either css_gene_lst_all or css_Ngene_lst_all\n",
    "    \"\"\"\n",
    "    Input: css_gene_lst_all or css_Ngene_lst_all, the list of chromosome-wise list of the css in genic, intergenic regions.\n",
    "    Output: css_gene_unit_lst_all or css_Ngene_unit_lst_all\n",
    "    \"\"\"\n",
    "    reduced_all=[]\n",
    "    for i in range(len(css_lst_all)):\n",
    "        reduced_chr=[]\n",
    "        for j in range(len(css_lst_all[i])):\n",
    "            reduced=shorten_string(css_lst_all[i][j], unit)\n",
    "            reduced_chr.append(reduced)\n",
    "        reduced_all.append(reduced_chr)\n",
    "    return reduced_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f3da5e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a long string of the css (not using unit, but the real length)\n",
    "def df2longcss(df):\n",
    "    \"\"\"\n",
    "    Create a list of 24 lists of chromatin states in string, in real length\n",
    "    \"\"\"\n",
    "    df_lst_chr=df2chr_df(df)\n",
    "    # remove the microchondria DNA from df_lst_chr\n",
    "    if df_lst_chr[-3][\"chromosome\"].iloc[0]==\"chrM\":\n",
    "        del df_lst_chr[-3]\n",
    "#         assert df_lst_chr[-3][\"chromosome\"].iloc[0]==\"chr22\"\n",
    "    elif df_lst_chr[-2][\"chromosome\"].iloc[0]==\"chrM\":\n",
    "        del df_lst_chr[-2]\n",
    "#         assert df_lst_chr[-3][\"chromosome\"].iloc[0]==\"chr22\"\n",
    "    \n",
    "    all_css=[]\n",
    "    for i in range(len(df_lst_chr)):\n",
    "        df_chr=df_lst_chr[i]\n",
    "        css_chr=''\n",
    "        for j in range(len(df_chr)):\n",
    "            css_chr+=df_chr[\"length\"].iloc[j]*df_chr[\"state_seq\"].iloc[j]\n",
    "        all_css.append(css_chr)  \n",
    "    return all_css"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a7bc9f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for preprocess the whole gene data and produce chromosome-wise gene lists\n",
    "# each element is dataframe\n",
    "\n",
    "# def whGene2GLChr(whole_gene_file='../database/RefSeq/RefSeq.WholeGene.bed'):\n",
    "def whGene2GLChr(whole_gene_file):\n",
    "    \"\"\"\n",
    "    For pre-processing the whole gene data and produce chromosome-wise gene lists\n",
    "    \"\"\"\n",
    "    print(\"Extracting the gene file ...\")\n",
    "    g_fn=whole_gene_file\n",
    "    g_df_raw=pd.read_csv(g_fn, sep='\\t', lineterminator='\\n', header=None, low_memory=False)\n",
    "    g_df_int=g_df_raw.rename(columns={0:\"chromosome\",1:\"TxStart\",2:\"TxEnd\",3:\"name\",4:\"unk0\",\n",
    "                                  5:'strand', 6:'cdsStart', 7:'cdsEnd',8:\"unk1\",9:\"exonCount\",\n",
    "                                  10:\"unk2\",11:\"unk3\"})\n",
    "    g_df=g_df_int[[\"chromosome\",\"TxStart\",\"TxEnd\",\"name\"]]\n",
    "    \n",
    "    # Remove other than regular chromosomes\n",
    "    chr_lst=['chr1','chr2','chr3','chr4','chr5','chr6','chr7','chr8','chr9','chr10',\n",
    "             'chr11','chr12','chr13','chr14','chr15','chr16','chr17','chr18','chr19',\n",
    "             'chr20','chr21','chr22','chrX','chrY']\n",
    "    g_df=g_df.loc[g_df[\"chromosome\"].isin(chr_lst)]\n",
    "    \n",
    "    # Create a list of chromosome-wise dataframe \n",
    "    g_df_chr_lst=[]\n",
    "    for num in range(len(chr_lst)):\n",
    "        chr_num=chr_lst[num]\n",
    "        g_chr_df='g_'+chr_num\n",
    "        locals()[g_chr_df]=g_df[g_df[\"chromosome\"]==chr_num]\n",
    "        g_chr_df=locals()[g_chr_df]\n",
    "        g_chr_df=g_chr_df.sort_values(\"TxStart\")\n",
    "        g_df_chr_lst.append(g_chr_df)\n",
    "    print(\"Done!\")\n",
    "    \n",
    "    return g_df_chr_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "961300db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Merging the gene table #### modified June. 29. 2023\n",
    "\n",
    "def merge_intervals(df_list):\n",
    "    merged_list = []  # List to hold merged DataFrames\n",
    "\n",
    "    for df in df_list:\n",
    "        # Sort by 'TxStart'\n",
    "        df = df.sort_values(by='TxStart')\n",
    "\n",
    "        # Initialize an empty list to store the merged intervals\n",
    "        merged = []\n",
    "\n",
    "        # Iterate through the rows in the DataFrame\n",
    "        for _, row in df.iterrows():\n",
    "            # If the list of merged intervals is empty, or the current interval does not overlap with the previous one,\n",
    "            # append it to the list\n",
    "            if not merged or merged[-1]['TxEnd'] < row['TxStart']:\n",
    "                merged.append({'TxStart': row['TxStart'], 'TxEnd': row['TxEnd']})  # Only keep 'TxStart' and 'TxEnd'\n",
    "            else:\n",
    "                # Otherwise, there is an overlap, so we merge the current and previous intervals\n",
    "                merged[-1]['TxEnd'] = max(merged[-1]['TxEnd'], row['TxEnd'])\n",
    "\n",
    "        # Convert the merged intervals back into a DataFrame and append it to the list\n",
    "        merged_list.append(pd.DataFrame(merged))\n",
    "\n",
    "    return merged_list  # a list of DF, containing only TxStart and TxEnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "78eb1d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_chrM_and_trim_gene_file_accordingly(whole_gene_file,df):\n",
    "    \n",
    "    ########### Gene without overlap ###########\n",
    "    g_df_chr_lst=whGene2GLChr(whole_gene_file)  ##### fixed June 29. 2023\n",
    "    new_gene_lst_all=merge_intervals(g_df_chr_lst) ##### fixed June 29. 2023\n",
    "    ############################################################\n",
    "\n",
    "    #### Remove chrM ###########################################\n",
    "    contains_chrM = df['chromosome'].str.contains('chrM').any()  #check whether it contains M\n",
    "    if contains_chrM:\n",
    "        df= df[~df['chromosome'].str.contains('chrM')]\n",
    "\n",
    "    contains_chrY = df['chromosome'].str.contains('chrY').any()\n",
    "\n",
    "    ##### if the target file does not contain Y, remove Y in the gene list file\n",
    "    if not contains_chrY:\n",
    "        new_gene_lst_all=new_gene_lst_all[:-1] ## the final element is for Y\n",
    "    ############################################################\n",
    "\n",
    "    assert len(df[\"chromosome\"].unique())==len(new_gene_lst_all)\n",
    "    return new_gene_lst_all, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f5bb1c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_TSS_by_loc(whole_gene_file, input_path=\"./\",output_path=\"./\",file_name=\"upNkdownNk\", up_num=2000, down_num=4000, unit=200):\n",
    "    \"\"\"\n",
    "    extract TSS region by location estimation. \n",
    "    input: (1) whole_gene_file: the raw gene bed file (e.g. RefSeq.WholeGene.bed)\n",
    "           (2) input_path: pickled df per cell\n",
    "    output: save tss_by_loc_css_unit_all at the output path\n",
    "    \"\"\"\n",
    "    file_lst=os.listdir(input_path)\n",
    "    all_files=[os.path.join(input_path,file) for file in file_lst]\n",
    "    for file in all_files:\n",
    "        cell_num=file.split(\"/\")[-1][:4]\n",
    "#         if cell_num==\"E002\": break  # for test \n",
    "        with open(file,\"rb\") as f:\n",
    "            df_pickled=pickle.load(f)\n",
    "        # align the gene file and the df file according to their availability(some cells does not have chr Y)\n",
    "        new_gene_lst_all, trimmed_df=remove_chrM_and_trim_gene_file_accordingly(whole_gene_file,df_pickled)\n",
    "        css_lst_chr = df2longcss(trimmed_df) # list of long css per chromosome\n",
    "        total_chr = len(new_gene_lst_all)       \n",
    "        tss_by_loc_css_all = []\n",
    "        for i in range(total_chr):\n",
    "            gene_start_lst = new_gene_lst_all[i][\"TxStart\"]\n",
    "            css_lst = css_lst_chr[i]\n",
    "            tss_by_loc_css_chr = []\n",
    "            for j in range(len(gene_start_lst)):\n",
    "                gene_start = gene_start_lst[j]\n",
    "                win_start = max(0, gene_start - up_num)  # use max to prevent negative index\n",
    "                win_end = min(len(css_lst), gene_start + down_num)  # use min to prevent index out of range\n",
    "                tss_by_loc_css = css_lst[win_start:win_end]\n",
    "                tss_by_loc_css_chr.append(tss_by_loc_css)               \n",
    "            tss_by_loc_css_all.append(tss_by_loc_css_chr)\n",
    "        tss_by_loc_css_unit_all=Convert2unitCSS_main_new(tss_by_loc_css_all, unit=unit)  \n",
    "        output_file_name=os.path.join(output_path,cell_num+\"_prom_\"+file_name+\".pkl\")\n",
    "        with open(output_file_name,\"wb\") as g:\n",
    "            pickle.dump(tss_by_loc_css_unit_all,g)\n",
    "\n",
    "    return print(\"All done!\") #tss_by_loc_css_unit_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b1b48ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test for save_TSS_by_loc\n",
    "# whole_gene_file='../database/RefSeq/RefSeq.WholeGene.bed'\n",
    "# save_TSS_by_loc(whole_gene_file=whole_gene_file, input_path=\"../database/roadmap/df_pickled/\",output_path=\"../database/final_test/\",file_name=\"up2kdown4k\", up_num=2000, down_num=4000, unit=200)\n",
    "# # test passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b1785a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretrain data preprocessing and storing\n",
    "\n",
    "# Preprocessing for removing continuous O state for pretrain dataset\n",
    "# 1. Sace the CSS per cell, per chromosome\n",
    "def save_css_by_cell_wo_continuous_15state(path_to_css_unit_pickled, output_path,k=4):\n",
    "    # read files from css_unit_pickled\n",
    "    files=os.listdir(path_to_css_unit_pickled)\n",
    "    file_path_lst=[os.path.join(path_to_css_unit_pickled,file) for file in files]\n",
    "    for file_path in file_path_lst:\n",
    "        file_name=os.path.basename(file_path)\n",
    "        if file_name[0] == 'E' and file_name[1:4].isdigit():\n",
    "            file_id = file_name[:4]\n",
    "        else:\n",
    "            pass\n",
    "        # ##########################\n",
    "        # if str(file_id)==\"E003\":\n",
    "        #     break  # for test\n",
    "        # ##########################\n",
    "        with open(file_path,\"rb\") as f:\n",
    "            css=pickle.load(f)\n",
    "        css_kmer=[]\n",
    "        for css_chr in css:\n",
    "            css_chr_kmer=seq2kmer(css_chr,k)\n",
    "            target_to_remove=\"O\"*k   # get rid of the word with continuous 15th state \"o\"\n",
    "            css_chr_kmer_trim = css_chr_kmer.replace(target_to_remove, \"\")\n",
    "            # clean up extra spaces\n",
    "            css_chr_kmer_trim = ' '.join(css_chr_kmer_trim.split())\n",
    "            css_kmer.append(css_chr_kmer_trim)\n",
    "        output_file_name=os.path.join(output_path,file_id+\"_unitcss_wo_all\"+str(k)+\"O_state.pkl\")    \n",
    "        with open(output_file_name, \"wb\") as g:\n",
    "            pickle.dump(css_kmer, g)  # note that it is chromosome-wise list (each element corresponds to each chromosome)\n",
    "\n",
    "        print(\"trimmed css by cell saved: \",file_id)\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ea1834fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing for removing continuous O state for pretrain dataset\n",
    "# 2. Concatenate all the cells and create one .txt file\n",
    "# (Note. new line joining chromosome-wise and cell-wise)\n",
    "def kmerCSS_to_pretrain_data(path_to_kmer_css_unit_pickled,output_path):\n",
    "    files=os.listdir(path_to_kmer_css_unit_pickled)\n",
    "    file_path_lst=[os.path.join(path_to_kmer_css_unit_pickled,file) for file in files]\n",
    "\n",
    "    css_all=[]\n",
    "    for file_path in file_path_lst:\n",
    "        file_name=os.path.basename(file_path)\n",
    "        if file_name[0] == 'E' and file_name[1:4].isdigit():\n",
    "            file_id = file_name[:4]\n",
    "        else:\n",
    "            pass\n",
    "        # ##########################\n",
    "        # if str(file_id)==\"E003\":\n",
    "        #     break  # for test\n",
    "        # # ##########################\n",
    "        with open(file_path,\"rb\") as f:\n",
    "            css=pickle.load(f)\n",
    "\n",
    "        css_per_cell='\\n'.join(css)   # join the chromosome by new line\n",
    "\n",
    "        css_all.append(css_per_cell)   \n",
    "\n",
    "    css_all_cell='\\n'.join(css_all)  # join the cell by new line\n",
    "\n",
    "    output_name=os.path.join(output_path,\"pretrain_genome_all.txt\") \n",
    "    with open(output_name, \"w\") as g:\n",
    "        g.write(css_all_cell)\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8586b674",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prom_css_Kmer_by_cell(path=\"./\", output_path=\"./\",k=4):\n",
    "    output_dir=str(k)+\"mer/\"\n",
    "    output_path_fin=os.path.join(output_path, output_dir)\n",
    "    os.makedirs(output_path_fin, exist_ok=True)\n",
    "\n",
    "    all_files=sorted([os.path.join(path, file) for file in os.listdir(path)]) \n",
    "    \n",
    "    for file in all_files:\n",
    "        prom_kmer_all=[]\n",
    "        cell_id=file.split(\"/\")[-1][:4]\n",
    "\n",
    "        # if cell_id==\"E003\": break # for test use\n",
    "        \n",
    "        with open(file, \"rb\") as f:\n",
    "            prom=pickle.load(f)\n",
    "        prom_css=flatLst(prom)  # make a list from list of a list\n",
    "        prom_kmer=[seq2kmer(item,k) for item in prom_css]\n",
    "        prom_kmer_all.append(prom_kmer)\n",
    "        prom_kmer_all_flt=flatLst(prom_kmer_all)\n",
    "        prom_kmer_all_flt_not_zero=[item for item in prom_kmer_all_flt if item!=\"\"]\n",
    "        output_name=cell_id+\"_all_genes_prom_\"+str(k)+\"merized.txt\"\n",
    "        with open(output_path_fin+output_name, \"w\") as g:\n",
    "            g.write(\"\\n\".join(prom_kmer_all_flt_not_zero))\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "51bb37ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test for prom_css_Kmer_by_cell\n",
    "# path=\"../database/roadmap/prom/up2kdown4k/all_genes/\"\n",
    "# output_path=\"../database/final_test/\"\n",
    "# prom_css_Kmer_by_cell(path=path, output_path=output_path, k=4)\n",
    "# test passed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd4d073",
   "metadata": {},
   "source": [
    "#### Pipeline \n",
    "\n",
    "(1) `prom_expGene2css` : cut the prom regions of long css <br>\n",
    "(2) `extProm_wrt_g_exp` : transform css into unit length css <br>\n",
    "(3) `extNsaveProm_g_exp` : load the required file and process all, and save"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62aaf0f",
   "metadata": {},
   "source": [
    "#### Function: `Gexp_Gene2GLChr`\n",
    "\n",
    "* This function only checks a single file.\n",
    "* Usage: After the gene expression files such as `gene_highlyexpressed.refFlat` are acquired by `/database/bed/gene_expression/classifygenes_ROADMAP_RPKM.py`, apply this function to obtain the list of dataframe per chromosome contains the transcription start and end indices.\n",
    "* Input: gene expression (high/low/not) file\n",
    "* Output: a chromosome-wise list of dataframe containing `TxStart` and `TxEnd`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2e7f9232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for preprocess the whole gene data and produce chromosome-wise gene lists\n",
    "# each element is dataframe\n",
    "\n",
    "### this function is not essential, but just to check by create df from .refFlat\n",
    "def Gexp_Gene2GLChr(exp_gene_file='../database/bed/gene_expression/E050/gene_highlyexpressed.refFlat'):\n",
    "    print(\"Extracting the gene file ...\")\n",
    "    g_fn=exp_gene_file\n",
    "    g_df_raw=pd.read_csv(g_fn, sep='\\t', index_col=False, header=0)\n",
    "    g_df=g_df_raw\n",
    "    g_df=g_df.iloc[:,1:]\n",
    "    g_df.rename(columns={\"name\":\"gene_id\"}, inplace=True)\n",
    "    g_df.rename(columns={\"#geneName\":\"geneName\"}, inplace=True)\n",
    "    g_df.rename(columns={\"txStart\":\"TxStart\"}, inplace=True) # to make it coherent to my previous codes\n",
    "    g_df.rename(columns={\"txEnd\":\"TxEnd\"}, inplace=True)\n",
    "#     g_df=g_df_raw.rename(columns={0:\"geneName\",1:\"gene_id\",2:\"chrom\",3:\"strand\",4:\"txStart\",5:\"txEnd\",\n",
    "#                                       6:\"cdsStart\",7:\"cdsEnd\",8:\"exonCount\",9:\"exonStart\",10:\"exonEnds\",\n",
    "#                                       11:\"gene type\",12:\"transcript type\",13:\"reference transcript name\",\n",
    "#                                       14:\"reference transcription id\"})\n",
    "    ## string to the list of \"int\", for exon start/end ##\n",
    "    g_df_temp=g_df # copy for processing\n",
    "    exon_start_int_lst=[]\n",
    "    for i, str_lst in enumerate(g_df_temp[\"exonStarts\"]):\n",
    "        int_lst=[int(elm) for elm in str_lst.replace(\"[\",\"\").replace(\"]\",\"\").split(\",\")]\n",
    "        assert g_df_temp[\"exonCount\"][i]==len(int_lst) # make sure the no. element in exon st count\n",
    "        exon_start_int_lst.append(int_lst)    \n",
    "    g_df_temp[\"exonStarts\"]=exon_start_int_lst\n",
    "\n",
    "    exon_end_int_lst=[]\n",
    "    for i, str_lst in enumerate(g_df_temp[\"exonEnds\"]):\n",
    "        int_lst=[int(elm) for elm in str_lst.replace(\"[\",\"\").replace(\"]\",\"\").split(\",\")]\n",
    "        assert g_df_temp[\"exonCount\"][i]==len(int_lst) # make sure the no. element in exon start = count\n",
    "        exon_end_int_lst.append(int_lst)    \n",
    "    g_df_temp[\"exonEnds\"]=exon_end_int_lst    \n",
    "    g_df=g_df_temp # and make it back the original name\n",
    "        \n",
    "    g_df=g_df[[\"geneName\",\"gene_id\",\"chrom\",\"TxStart\",\"TxEnd\"]] # extract these only\n",
    "    \n",
    "    # Remove other than regular chromosomes\n",
    "    chr_lst=['chr1','chr2','chr3','chr4','chr5','chr6','chr7','chr8','chr9','chr10',\n",
    "             'chr11','chr12','chr13','chr14','chr15','chr16','chr17','chr18','chr19',\n",
    "             'chr20','chr21','chr22','chrX','chrY']\n",
    "    g_df=g_df.loc[g_df[\"chrom\"].isin(chr_lst)]\n",
    "    \n",
    "    # Create a list of chromosome-wise dataframe \n",
    "    g_df_chr_lst=[]\n",
    "    for num in range(len(chr_lst)):\n",
    "        chr_num=chr_lst[num]\n",
    "        g_chr_df='g_'+chr_num  # name it as \"g_\"\n",
    "        locals()[g_chr_df]=g_df[g_df[\"chrom\"]==chr_num]\n",
    "        g_chr_df=locals()[g_chr_df]\n",
    "        g_chr_df=g_chr_df.sort_values(\"TxStart\")\n",
    "        g_df_chr_lst.append(g_chr_df)\n",
    "        \n",
    "    # Remove the overlapped area (using removeOverlapDF function in css_utility.py)\n",
    "    g_df_chr_collapsed_lst=[]\n",
    "    for g_df_chr in g_df_chr_lst:\n",
    "        g_df_chr_collapsed=removeOverlapDF(g_df_chr)\n",
    "        assert len(g_df_chr)>=len(g_df_chr_collapsed)\n",
    "        g_df_chr_collapsed_lst.append(g_df_chr_collapsed)\n",
    "    print(\"Done!\")\n",
    "    \n",
    "    return g_df_chr_collapsed_lst  # list of dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcb0768",
   "metadata": {},
   "source": [
    "#### Function `prom_expGene2css`\n",
    "* This function produces a long list (not unit length) of css according to the gene expression table, per cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "596c4a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prom_expGene2css(g_lst_chr_merged,df, up_num=2000, down_num=4000):   # df indicates css, created by bed2df_expanded\n",
    "    \"\"\"\n",
    "    modified from `compGene2css`\n",
    "    Input: Reference gene file trimmed for gene expresseion level, df (CSS)\n",
    "    Output: list of chromosome-wise list that contains the css at (expressed) genic area with prom only.\n",
    "    \"\"\"\n",
    "    g_lst_chr=g_lst_chr_merged\n",
    "    df = df[df['chromosome'] != 'chrM']\n",
    "    css_lst_chr=df2longcss(df) # list of long css per chromosome\n",
    "    \n",
    "    g_lst_chr = g_lst_chr[:len(css_lst_chr)]  # adjust the length of list according to length of df (might not have chrY)\n",
    "    total_chr=len(css_lst_chr)\n",
    "    \n",
    "    print(\"Matching to the chromatin state sequence data ...\")\n",
    "    css_prom_lst_all=[]\n",
    "    # for i in tqdm_notebook(range(total_chr)):\n",
    "    for i in range(total_chr):\n",
    "        css=css_lst_chr[i]   # long css of i-th chromosome\n",
    "        gene_df=g_lst_chr[i] # gene df of i-th chromosome\n",
    "        \n",
    "        css_prom_lst_chr=[]\n",
    "        for j in range(len(gene_df)):\n",
    "            prom_start=gene_df[\"TxStart\"].iloc[j]-1-up_num  # python counts form 0\n",
    "            prom_end=prom_start+up_num+down_num+1      # python excludes the end\n",
    "            if gene_df[\"TxEnd\"].iloc[j]<prom_end:  # if longer than gene body, then just gene body\n",
    "                prom_end=gene_df[\"TxEnd\"].iloc[j]+1\n",
    "    \n",
    "            css_prom=css[prom_start:prom_end]           # cut the gene area only\n",
    "            css_prom_lst_chr.append(css_prom)     # store in the list\n",
    "          \n",
    "        css_prom_lst_all.append(css_prom_lst_chr)  # list of list\n",
    "    \n",
    "    assert len(css_prom_lst_all)==total_chr\n",
    "    \n",
    "    # remove chromosome if it is empty (e.g. chrY for female)\n",
    "    css_prom_lst_all=[elm for elm in css_prom_lst_all if elm!=[]] \n",
    "    \n",
    "    print(\"Done!\")\n",
    "    return css_prom_lst_all "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7a7e2660",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extProm_wrt_g_exp(exp_gene_file, df, up_num=2000, down_num=4000,unit=200):\n",
    "    \"\"\"\n",
    "    extract promoter regions of genes according to gene expression level\n",
    "    \"\"\"\n",
    "    df = df[df['chromosome'] != 'chrM']\n",
    "    g_lst_chr=Gexp_Gene2GLChr(exp_gene_file)\n",
    "    g_lst_chr_merged=merge_intervals(g_lst_chr)\n",
    "    \n",
    "    css_prom_lst_all=prom_expGene2css(g_lst_chr_merged,df, up_num=up_num, down_num=down_num)\n",
    "    css_prom_lst_unit_all=Convert2unitCSS_main_new(css_prom_lst_all, unit=unit)\n",
    "    return css_prom_lst_unit_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8690a34f",
   "metadata": {},
   "source": [
    "#### Function: `removeOverlapDF` and `gene_removeDupl`\n",
    "\n",
    "* Main function: `gene_removeDupl`\n",
    "* `removeOverlapDF`: function used inside the main function.\n",
    "* To acquire final collapsed gene table, run `gene_removeDupl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "678e4b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeOverlapDF(test_df):    \n",
    "    new_lst=[]\n",
    "    for i in range(len(test_df)):\n",
    "        start=test_df[\"TxStart\"].iloc[i]\n",
    "        end=test_df[\"TxEnd\"].iloc[i]\n",
    "\n",
    "        exist_pair=(start,end)\n",
    "\n",
    "        if i==0:\n",
    "            new_pair=exist_pair\n",
    "            new_lst.append(new_pair)        \n",
    "        else:\n",
    "            start_pre=test_df[\"TxStart\"].iloc[i-1]\n",
    "            end_pre=test_df[\"TxEnd\"].iloc[i-1]\n",
    "\n",
    "            # first, concatenate all the shared start\n",
    "            if start==start_pre:\n",
    "                new_end=max(end, end_pre)\n",
    "                new_pair=(start, new_end)\n",
    "            # second, concatenate all the shared end\n",
    "            elif end==end_pre:\n",
    "                new_start=min(start, start_pre)\n",
    "                new_pair=(new_start, end)\n",
    "            else:    \n",
    "                new_pair=exist_pair\n",
    "\n",
    "        new_lst.append(new_pair) \n",
    "    new_lst=list(dict.fromkeys(new_lst))\n",
    "    \n",
    "    mod_lst=[[start, end] for (start, end) in new_lst] # as a list element\n",
    "\n",
    "    for j, elm in enumerate(mod_lst):\n",
    "        start, end = elm[0], elm[1]\n",
    "\n",
    "        if j==0:\n",
    "            continue\n",
    "        else:\n",
    "            start_pre=mod_lst[j-1][0]\n",
    "            end_pre=mod_lst[j-1][1]\n",
    "\n",
    "            if end_pre>=end:\n",
    "                mod_lst[j][0]=mod_lst[j-1][0]  # if end_pre is larger than end, replace start as start_pre\n",
    "                mod_lst[j][1]=mod_lst[j-1][1]  # if end_pre is larger than end, replace end as end_pre\n",
    "\n",
    "            elif start <=end_pre:\n",
    "                mod_lst[j][0]=mod_lst[j-1][0]  # current start=start_pre\n",
    "                mod_lst[j-1][1]=max(mod_lst[j][1],mod_lst[j-1][1])  # end_pre = end\n",
    "\n",
    "            else:\n",
    "                continue\n",
    "           \n",
    "    mod_lst=[tuple(elm) for elm in mod_lst]\n",
    "    fin_lst=list(dict.fromkeys(mod_lst))\n",
    "    gene_collapsed_df=pd.DataFrame(fin_lst, columns=[\"TxStart\", \"TxEnd\"])\n",
    " \n",
    "    return gene_collapsed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5d13ae3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gene_removeDupl(whole_gene_file='../database/RefSeq/RefSeq.WholeGene.bed'):\n",
    "    g_df_chr_lst=whGene2GLChr(whole_gene_file)\n",
    "    new_gene_lst_all=[]\n",
    "    for chr_no in range(len(g_df_chr_lst)):\n",
    "        gene_df=g_df_chr_lst[chr_no]\n",
    "        gene_collapsed_df=removeOverlapDF(gene_df)\n",
    "        new_gene_lst_all.append(gene_collapsed_df)\n",
    "    return new_gene_lst_all # list of chromosome-wise dataframe for collapsed gene table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a422fc9a",
   "metadata": {},
   "source": [
    "#### Function `extNsaveProm_g_exp`\n",
    "* This function processes the above works (cut the prom region and make it unit length css) per cell\n",
    "* Input\n",
    "    * `exp_gene_dir`: directory where refFlat for each cell (subdir means the sub directory for different gene expression level)\n",
    "    * `df_pickle_dir`: dataframe of each cell\n",
    "    * `rpkm_val`: RPKM value, 10, 20, 30, or 50\n",
    "    * `up_num`: upstream of gene\n",
    "    * `down_num`: from TSS (gene initial part) to cut\n",
    "    * `unit`: because chromatin states are annotated by 200 bps\n",
    "* Output: save the file according to the `rpkm_val` at the output path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b0d7dcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extNsaveProm_g_exp(exp_gene_dir=\"./\", df_pickle_dir=\"./\",output_path=\"./\",file_name=\"up2kdown4k\",rpkm_val=50, up_num=2000, down_num=4000,unit=200):\n",
    "    exp_gene_subdir=os.listdir(exp_gene_dir)\n",
    "    exp_gene_tardir=[os.path.join(exp_gene_dir, subdir) for subdir in exp_gene_subdir if str(rpkm_val) in subdir][0]    \n",
    "       \n",
    "    if rpkm_val==0:\n",
    "        exp_gene_tardir=os.path.join(exp_gene_dir, \"rpkm0\")\n",
    "        \n",
    "    exp_gene_files=sorted([os.path.join(exp_gene_tardir,file) for file in os.listdir(exp_gene_tardir)])\n",
    "\n",
    "    for exp_gene_file in exp_gene_files:\n",
    "        cell_id=exp_gene_file.split(\"/\")[-1][:4]\n",
    "\n",
    "        # print(cell_id)   ## for test\n",
    "        # if cell_id==\"E004\":break ## for test\n",
    "\n",
    "        df_name=[file for file in os.listdir(df_pickle_dir) if cell_id in file][0]\n",
    "        df_path=os.path.join(df_pickle_dir,df_name)\n",
    "        with open(df_path,\"rb\") as f:\n",
    "            df=pickle.load(f)\n",
    "        css_prom_lst_unit_all=extProm_wrt_g_exp(exp_gene_file, df, up_num=up_num, down_num=down_num,unit=unit)\n",
    "           \n",
    "        output_name=output_path+\"rpkm\"+str(rpkm_val)+\"/\"+cell_id+\"_prom_\"+file_name+\".pkl\"\n",
    "        output_dir = os.path.dirname(output_name)\n",
    "\n",
    "        # print(output_name) ### test\n",
    "        \n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "        with open(output_name, \"wb\") as g:\n",
    "            pickle.dump(css_prom_lst_unit_all,g)\n",
    "    return print(\"Saved at \",output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "52222a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test for extNsaveProm_g_exp\n",
    "# extNsaveProm_g_exp(exp_gene_dir=\"../database/roadmap/gene_exp/refFlat_byCellType/\", df_pickle_dir=\"../database/roadmap/df_pickled/\",output_path=\"../database/final_test/\",file_name=\"up2kdown4k\",rpkm_val=50, up_num=2000, down_num=4000,unit=200)\n",
    "# test passed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d2505c",
   "metadata": {},
   "source": [
    "### Extract Promoter regions from not expressed genes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8eb98e1",
   "metadata": {},
   "source": [
    "#### Pipeline \n",
    "\n",
    "(1) `extWholeGeneRef` : Just extract the whole gene location files from `chr.gene.refFlat` <br>\n",
    "(2) `extNOTexp_by_compare` : Extract the not expressed genes by comparing with whole gene with rpkm>0 <br>\n",
    "(3) `extNsaveNOTexp_by_compare` : load the required file and process all, and save refFlat (.pkl) and prom-region css (.pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "59c7ddb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extWholeGeneRef(whole_gene_ref):\n",
    "    ###### modified from Gexp_Gene2GLChr, this function provides the df of whole genes\n",
    "    ###### note that this file contains Y chromosome\n",
    "    g_fn=whole_gene_ref\n",
    "    g_df=pd.read_csv(g_fn, sep='\\t', index_col=False, header=0)\n",
    "    g_df=g_df.iloc[:,1:]\n",
    "    g_df.rename(columns={\"name\":\"gene_id\"}, inplace=True)\n",
    "    g_df.rename(columns={\"#geneName\":\"geneName\"}, inplace=True)\n",
    "    g_df.rename(columns={\"txStart\":\"TxStart\"}, inplace=True) # to make it coherent to my previous codes\n",
    "    g_df.rename(columns={\"txEnd\":\"TxEnd\"}, inplace=True)     \n",
    "    g_df=g_df[[\"chrom\",\"TxStart\",\"TxEnd\"]] # extract these only\n",
    "    # Remove other than regular chromosomes\n",
    "    chr_lst=['chr1','chr2','chr3','chr4','chr5','chr6','chr7','chr8','chr9','chr10',\n",
    "             'chr11','chr12','chr13','chr14','chr15','chr16','chr17','chr18','chr19',\n",
    "             'chr20','chr21','chr22','chrX','chrY']\n",
    "    g_df=g_df.loc[g_df[\"chrom\"].isin(chr_lst)]\n",
    "    \n",
    "    # Create a list of chromosome-wise dataframe \n",
    "    g_df_chr_lst=[]\n",
    "    for num in range(len(chr_lst)):\n",
    "        chr_num=chr_lst[num]\n",
    "        g_chr_df='g_'+chr_num  # name it as \"g_\"\n",
    "        locals()[g_chr_df]=g_df[g_df[\"chrom\"]==chr_num]\n",
    "#         print(chr_num)\n",
    "        g_chr_df=locals()[g_chr_df]\n",
    "        g_chr_df=g_chr_df.sort_values(\"TxStart\")\n",
    "        g_df_chr_lst.append(g_chr_df)\n",
    "    \n",
    "    # remove any overlap\n",
    "    g_df_chr_lst=merge_intervals(g_df_chr_lst)\n",
    "    return g_df_chr_lst  # list of chromosome-wise df for all gene start and end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "88c8fa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extNOTexp_by_compare(whole_gene_ref, cell_exp_ref):\n",
    "    \"\"\"\n",
    "    whole_gene_ref: e.g.) chr.gene.refFlat\"\n",
    "    \"\"\"\n",
    "    whole_gene_ref_lst=extWholeGeneRef(whole_gene_ref)\n",
    "    cell_exp_lst=Gexp_Gene2GLChr(cell_exp_ref)\n",
    "    cell_exp_lst=merge_intervals(cell_exp_lst) \n",
    "    if len(whole_gene_ref_lst)!=len(cell_exp_lst):\n",
    "        whole_gene_ref_lst=whole_gene_ref_lst[:-1]   \n",
    "    non_exp_gene_lst=[]\n",
    "    for i, whole_gene_chr in enumerate(whole_gene_ref_lst):\n",
    "        exp_gene_mark = whole_gene_chr.merge(cell_exp_lst[i], on=['TxStart', 'TxEnd'])\n",
    "        non_exp_gene_chr=whole_gene_chr.drop(exp_gene_mark.index)\n",
    "        non_exp_gene_lst.append(non_exp_gene_chr)\n",
    "    print(\"total length of non_expressed genes in this cell: \",len(pd.concat(non_exp_gene_lst)))\n",
    "    return non_exp_gene_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5438815f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extNsaveNOTexp_by_compare(whole_gene_ref_path,\n",
    "                              exp_ref_path=\"./\",\n",
    "                              df_pickle_dir=\"./\",\n",
    "                              output_path_ref=\"./\",\n",
    "                              output_path_prom=\"./\",\n",
    "                              up_num=2000,down_num=4000,unit=200):\n",
    "    \"\"\"\n",
    "    whole_gene_ref: e.g.) chr.gene.refFlat\"\n",
    "    \"\"\"\n",
    "    exp_ref_file_all=sorted([os.path.join(exp_ref_path,file) for file in os.listdir(exp_ref_path)])\n",
    "    \n",
    "    for exp_ref_file in exp_ref_file_all:\n",
    "        cell_id=exp_ref_file.split(\"/\")[-1][:4]\n",
    "#         if cell_id==\"E004\":break # for test\n",
    "        print(cell_id+\" is now processing...\")\n",
    "            \n",
    "        df_name=[file for file in os.listdir(df_pickle_dir) if cell_id in file][0]\n",
    "        df_path=os.path.join(df_pickle_dir,df_name)\n",
    "        with open(df_path,\"rb\") as f:\n",
    "            df=pickle.load(f)\n",
    "        \n",
    "        non_exp_gene_lst=extNOTexp_by_compare(whole_gene_ref_path, exp_ref_file) # a list of chromosome-wise df\n",
    "        #### refFlat for NOT expressed is pickled as a list of dataframe ####\n",
    "        not_exp_ref_path=output_path_ref+cell_id+\"_gene_not_expressed.pkl\"\n",
    "        with open(not_exp_ref_path,\"wb\") as g:\n",
    "            pickle.dump(non_exp_gene_lst,g)        \n",
    "        \n",
    "        css_prom_lst_all=prom_expGene2css(non_exp_gene_lst, df, up_num=up_num, down_num=down_num)\n",
    "        css_prom_lst_unit_all=Convert2unitCSS_main_new(css_prom_lst_all, unit=unit)\n",
    "        \n",
    "        output_name=output_path_prom+cell_id+\"_not_exp_gene_prom_up2kdown4k.pkl\"\n",
    "        with open(output_name,\"wb\") as h:\n",
    "            pickle.dump(css_prom_lst_unit_all,h)\n",
    "    \n",
    "    return print(\"refFlat is saved at {} and prom is saved at {}.\".format(output_path_ref, output_path_prom))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ddd7213b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # test for extNsaveNOTexp_by_compare\n",
    "# extNsaveNOTexp_by_compare(whole_gene_ref_path=\"../database/roadmap/gene_exp/chr.gene.refFlat\",\n",
    "#                               exp_ref_path=\"../database/roadmap/gene_exp/refFlat_byCellType/rpkm0/\",\n",
    "#                               df_pickle_dir=\"../database/roadmap/df_pickled/\",\n",
    "#                               output_path_ref=\"../database/roadmap/gene_exp/refFlat_byCellType/not_exp/\",\n",
    "#                               output_path_prom=\"../database/final_test/\",\n",
    "#                               up_num=2000,down_num=4000,unit=200)\n",
    "# # # test passed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8426cf",
   "metadata": {},
   "source": [
    "#### Function `prom_css_Kmer_by_cell`\n",
    "* This function saves the kmerized promoter regions (of all genes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d71b349b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prom_css_Kmer_by_cell(path=\"./\", output_path=\"./\",k=4):\n",
    "    output_dir=str(k)+\"mer/\"\n",
    "    output_path_fin=os.path.join(output_path, output_dir)\n",
    "\n",
    "    os.makedirs(output_path_fin, exist_ok=True)\n",
    "\n",
    "    all_files=sorted([os.path.join(path, file) for file in os.listdir(path)]) \n",
    "    \n",
    "    for file in all_files:\n",
    "        prom_kmer_all=[]\n",
    "        cell_id=file.split(\"/\")[-1][:4]\n",
    "        # if cell_id==\"E004\": break # for test use\n",
    "        with open(file, \"rb\") as f:\n",
    "            prom=pickle.load(f)\n",
    "        prom_css=flatLst(prom)  # make a list from list of a list\n",
    "        prom_kmer=[seq2kmer(item,k) for item in prom_css]\n",
    "        prom_kmer_all.append(prom_kmer)\n",
    "        prom_kmer_all_flt=flatLst(prom_kmer_all)\n",
    "        prom_kmer_all_flt_not_zero=[item for item in prom_kmer_all_flt if item!=\"\"]\n",
    "        output_name=cell_id+\"_all_genes_prom_\"+str(k)+\"merized.txt\"\n",
    "        with open(output_path_fin+output_name, \"w\") as g:\n",
    "            g.write(\"\\n\".join(prom_kmer_all_flt_not_zero))\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ed9ee5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test for prom_css_Kmer_by_cell\n",
    "# prom_css_Kmer_by_cell(path=\"../database/roadmap/prom/up2kdown4k/all_genes/\", output_path=\"../database/final_test/\",k=4)\n",
    "# test passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0817863",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb9d340e",
   "metadata": {},
   "source": [
    "#### CRM Dataset preparation\n",
    "\n",
    "The CRM regions are usually very short (for unit css length, everage is almost near 2). So the regions are screened to be longer than 6, 7, 8, 9, 10 (in terms of unit). Following functions extract CRM regions according to user-defined length and save it to the designated path. Final function saves the CRM regions with the designated length, k-mer. \n",
    "\n",
    "(1) `crm_df_maker` : prepare the dataframe of CRM from the raw bed file <br>\n",
    "(2) `extCRMfromCell` : cut the CRM regions of unit css for a sample cell <br>\n",
    "(2) `extCRMfromCell_all` : cut the CRM regions of unit css for all cells <br>\n",
    "(3) `saveCRMforPREall` : save the CRM extracted for various limit length (from 6 to 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "59713882",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crm_df_maker(crm_path=\"../database/remap2022/remap2022_crm_macs2_hg19_v1_0.bed\", limit_len=3):\n",
    "    # Load the data\n",
    "    crm_raw = pd.read_csv(crm_path, sep='\\t', header=None, names=[\"chromosome\", \"start\", \"end\", \"name\", \"score\", \"strand\", \"thickStart\", \"thickEnd\", \"itemRgb\"])\n",
    "    \n",
    "    # Convert start and end locations to units of 200 bps and calculate length\n",
    "    crm_raw['start'] = (crm_raw['start'] / 200).round().astype(int)\n",
    "    crm_raw['end'] = (crm_raw['end'] / 200).round().astype(int)\n",
    "    crm_raw['length'] = crm_raw['end'] - crm_raw['start'] + 1\n",
    "\n",
    "    # Filter by chromosome and length\n",
    "    crm_df = crm_raw[crm_raw[\"chromosome\"].str.contains('^chr[0-9XY]+$') & (crm_raw['length'] >= limit_len)].copy()\n",
    "    \n",
    "    # Define chromosome order\n",
    "    chromosome_order = ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10',\n",
    "                        'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19',\n",
    "                        'chr20', 'chr21', 'chr22', 'chrX', 'chrY']\n",
    "\n",
    "    # Convert 'chromosome' to a categorical type with the defined order\n",
    "    crm_df['chromosome'] = pd.Categorical(crm_df['chromosome'], categories=chromosome_order, ordered=True)\n",
    "\n",
    "    # Sort by 'chromosome' and 'start'\n",
    "    crm_df = crm_df.sort_values(['chromosome', 'start'])\n",
    "    crm_df_fin = crm_df[[\"chromosome\",\"start\" ,\"end\",\"length\",\"name\"]]\n",
    "\n",
    "    # Print summary\n",
    "    print(\"{} out of total {} CRM entries are longer than 200x{}, which is approx. {} %\".format(len(crm_df), len(crm_raw), limit_len, round(len(crm_df)/len(crm_raw), 3)))\n",
    "    \n",
    "    return crm_df_fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ad5dea02",
   "metadata": {},
   "outputs": [],
   "source": [
    "### cut the css according to the CRM position\n",
    "def extCRMfromCell(css_sample_path=\"../database/roadmap/css_unit_pickled/E003_unitcss_woChrM.pkl\",crm_path=\"../database/remap2022/remap2022_crm_macs2_hg19_v1_0.bed\",limit_len=4):\n",
    "    #### load unit-css per chromosome of one cell\n",
    "    with open(css_sample_path, \"rb\") as s:\n",
    "        unit_css=pickle.load(s)\n",
    "    #### make CRM as a dataframe\n",
    "    crm_df_fin=crm_df_maker(crm_path=crm_path,limit_len=limit_len)\n",
    "    \n",
    "    cut_lst_all=[]\n",
    "    for chr in range(len(unit_css)):\n",
    "        unit_css_chr=unit_css[chr]\n",
    "        chromosome_order = ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10',\n",
    "                            'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19',\n",
    "                            'chr20', 'chr21', 'chr22', 'chrX', 'chrY']\n",
    "        crm_df_chr=crm_df_fin[crm_df_fin[\"chromosome\"]==chromosome_order[chr]]\n",
    "\n",
    "        cut_lst=[]\n",
    "        for i in range(len(crm_df_chr)):\n",
    "            start_loc=crm_df_chr[\"start\"].iloc[i]\n",
    "            end_loc=crm_df_chr[\"end\"].iloc[i]\n",
    "            # length=crm_df_fin[\"length\"].iloc[i]     \n",
    "            cut=unit_css_chr[start_loc:end_loc+1]\n",
    "            cut_lst.append(cut)\n",
    "        cut_lst_all.append(cut_lst)\n",
    "    return cut_lst_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6bbb36e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extCRMfromCell_all(input_path=\"../database/roadmap/css_unit_pickled/\", crm_path=\"../database/remap2022/remap2022_crm_macs2_hg19_v1_0.bed\", output_path=\"../database/remap2022/crm/\", limit_len=6):\n",
    "    files=os.listdir(input_path)\n",
    "    css_paths=[os.path.join(input_path, file) for file in files if \"E\" in file]  # list of paths for css of cells\n",
    "    for css_sample_path in css_paths:\n",
    "        cut_lst_all=extCRMfromCell(css_sample_path=css_sample_path,crm_path=crm_path,limit_len=limit_len)\n",
    "         \n",
    "        file_name=re.search(r'E\\d{3}_unitcss_', css_sample_path).group(0)\n",
    "        output_file=os.path.join(output_path,file_name+\"limit_len\"+str(limit_len)+\".pkl\")\n",
    "        with open(output_file,\"wb\") as f:\n",
    "            pickle.dump(cut_lst_all,f)\n",
    "    return print(\"All files are saved at {}, with limit_len={}\".format(output_path, limit_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4dc89765",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveCRMforPREall_mod(input_path=\"../database/remap2022/crm/\",output_path=\"../database/pretrain/crm/\",limit_len=10, k=4): \n",
    "    files=os.listdir(os.path.join(input_path,\"lim\"+str(limit_len)))\n",
    "    css_all=[]\n",
    "    for file in files:\n",
    "        file_name=os.path.basename(file)\n",
    "        if file_name[0] == 'E' and file_name[1:4].isdigit():\n",
    "            file_id = file_name[:4]\n",
    "        else:\n",
    "            pass\n",
    "        # ##########################\n",
    "        # if str(file_id)==\"E003\":\n",
    "        #     break  # for test\n",
    "        # ##########################\n",
    "        with open(os.path.join(input_path,\"lim\"+str(limit_len),file), \"rb\") as f:\n",
    "            css_lst=pickle.load(f)\n",
    "        css=flatLst(css_lst)\n",
    "        css_kmer=[]\n",
    "        for css_chr in css:\n",
    "            css_chr_kmer=seq2kmer(css_chr,k)\n",
    "            target_to_remove=\"O\"*k   # get rid of the word with continuous 15th state \"o\"\n",
    "            css_chr_kmer_trim = css_chr_kmer.replace(target_to_remove, \"\")\n",
    "            # clean up extra spaces\n",
    "            css_chr_kmer_trim = ' '.join(css_chr_kmer_trim.split())\n",
    "            css_kmer.append(css_chr_kmer_trim)\n",
    "        css_all.append(css_kmer)\n",
    "    css_all_flt=flatLst(css_all)\n",
    "    os.makedirs(os.path.join(output_path, \"lim\" + str(limit_len)), exist_ok=True)\n",
    "    output_name=\"crm_lim\"+str(limit_len)+\"_allcell_wo_cnt_o\"+str(k)+\"merized.txt\"\n",
    "    with open(os.path.join(output_path,\"lim\"+str(limit_len),output_name), \"w\") as g:\n",
    "            g.write(\"\\n\".join(css_all_flt))\n",
    "    return print(\"File is saved at {}\".format(output_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1104ac94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b433275f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4e1218f",
   "metadata": {},
   "source": [
    "#### Motif Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9b2dc8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def motif_init2df(input_path=\"./init_concat.csv\"):\n",
    "    \"\"\"\n",
    "    Read init.csv file and convert it to \n",
    "    \"\"\"\n",
    "    df=pd.read_csv(input_path)\n",
    "    data_lst=df[\"motif\"].to_list()\n",
    "    def convert_sequence(sequence, mapping):\n",
    "        return [mapping[letter] for letter in sequence]\n",
    "    letter_to_num = {'A': 1,'B': 2,'C': 3,'D': 4,'E': 5,'F': 6,'G': 7,\n",
    "                     'H': 8,'I': 9,'J': 10,'K': 11,'L': 12,'M': 13,'N': 14,'O': 15}\n",
    "    numerical_sequences=[convert_sequence(seq, letter_to_num) for seq in data_lst]\n",
    "    df_sequences = pd.DataFrame(numerical_sequences).astype('Int64').T\n",
    "    # Add an 'entry' column at the beginning of the DataFrame with labels 'Entry 1', 'Entry 2', etc.\n",
    "    df_sequences.insert(0, 'position', ['Pos ' + str(i+1) for i in range(df_sequences.shape[0])])\n",
    "    return df_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "71c1b96a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>motif</th>\n",
       "      <th>N</th>\n",
       "      <th>K</th>\n",
       "      <th>n</th>\n",
       "      <th>x</th>\n",
       "      <th>p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>EEEEEE</td>\n",
       "      <td>1000</td>\n",
       "      <td>500</td>\n",
       "      <td>350</td>\n",
       "      <td>244</td>\n",
       "      <td>2.282236e-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>DDDEEE</td>\n",
       "      <td>1000</td>\n",
       "      <td>500</td>\n",
       "      <td>62</td>\n",
       "      <td>49</td>\n",
       "      <td>1.216714e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>EEEEE</td>\n",
       "      <td>1000</td>\n",
       "      <td>500</td>\n",
       "      <td>387</td>\n",
       "      <td>271</td>\n",
       "      <td>2.645214e-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>BBGGG</td>\n",
       "      <td>1000</td>\n",
       "      <td>500</td>\n",
       "      <td>58</td>\n",
       "      <td>46</td>\n",
       "      <td>2.242258e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>EEEEEEE</td>\n",
       "      <td>1000</td>\n",
       "      <td>500</td>\n",
       "      <td>306</td>\n",
       "      <td>212</td>\n",
       "      <td>2.632763e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>111</td>\n",
       "      <td>FFDDD</td>\n",
       "      <td>1000</td>\n",
       "      <td>504</td>\n",
       "      <td>48</td>\n",
       "      <td>40</td>\n",
       "      <td>1.277781e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>112</td>\n",
       "      <td>FFCCC</td>\n",
       "      <td>1000</td>\n",
       "      <td>504</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>1.869477e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>113</td>\n",
       "      <td>AAAAACCCCC</td>\n",
       "      <td>1000</td>\n",
       "      <td>504</td>\n",
       "      <td>19</td>\n",
       "      <td>18</td>\n",
       "      <td>3.812041e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>114</td>\n",
       "      <td>BBAAAA</td>\n",
       "      <td>1000</td>\n",
       "      <td>504</td>\n",
       "      <td>97</td>\n",
       "      <td>75</td>\n",
       "      <td>1.036869e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>115</td>\n",
       "      <td>AAAAAAA</td>\n",
       "      <td>1000</td>\n",
       "      <td>504</td>\n",
       "      <td>222</td>\n",
       "      <td>180</td>\n",
       "      <td>1.525199e-26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>116 rows  7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0       motif     N    K    n    x             p\n",
       "0             0      EEEEEE  1000  500  350  244  2.282236e-20\n",
       "1             1      DDDEEE  1000  500   62   49  1.216714e-06\n",
       "2             2       EEEEE  1000  500  387  271  2.645214e-24\n",
       "3             3       BBGGG  1000  500   58   46  2.242258e-06\n",
       "4             4     EEEEEEE  1000  500  306  212  2.632763e-16\n",
       "..          ...         ...   ...  ...  ...  ...           ...\n",
       "111         111       FFDDD  1000  504   48   40  1.277781e-06\n",
       "112         112       FFCCC  1000  504   19   19  1.869477e-06\n",
       "113         113  AAAAACCCCC  1000  504   19   18  3.812041e-05\n",
       "114         114      BBAAAA  1000  504   97   75  1.036869e-08\n",
       "115         115     AAAAAAA  1000  504  222  180  1.525199e-26\n",
       "\n",
       "[116 rows x 7 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"./init_concat.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5270e145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>position</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pos 1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pos 2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pos 3</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pos 4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pos 5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Pos 6</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>5</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>5</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>5</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Pos 7</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>5</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>5</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>3</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Pos 8</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>5</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>3</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Pos 9</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>3</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Pos 10</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>3</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Pos 11</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Pos 12</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows  117 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   position     0     1     2     3     4     5     6     7     8  ...   106  \\\n",
       "0     Pos 1     5     4     5     2     5     1     4     1     5  ...     7   \n",
       "1     Pos 2     5     4     5     2     5     1     5     2     5  ...     2   \n",
       "2     Pos 3     5     4     5     7     5     1     5     2     5  ...     1   \n",
       "3     Pos 4     5     5     5     7     5     1     5     2     5  ...     1   \n",
       "4     Pos 5     5     5     5     7     5     1     5     2     5  ...     1   \n",
       "5     Pos 6     5     5  <NA>  <NA>     5  <NA>     5  <NA>     5  ...     1   \n",
       "6     Pos 7  <NA>  <NA>  <NA>  <NA>     5  <NA>  <NA>  <NA>     5  ...  <NA>   \n",
       "7     Pos 8  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>     5  ...  <NA>   \n",
       "8     Pos 9  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>     5  ...  <NA>   \n",
       "9    Pos 10  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  ...  <NA>   \n",
       "10   Pos 11  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  ...  <NA>   \n",
       "11   Pos 12  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  ...  <NA>   \n",
       "\n",
       "     107   108   109   110   111   112   113   114   115  \n",
       "0      3     7     2     1     6     6     1     2     1  \n",
       "1      3     7     2     1     6     6     1     2     1  \n",
       "2      3     2     1     1     4     3     1     1     1  \n",
       "3      3     1     1     1     4     3     1     1     1  \n",
       "4      3     1     1     1     4     3     1     1     1  \n",
       "5      6     1  <NA>     5  <NA>  <NA>     3     1     1  \n",
       "6   <NA>  <NA>  <NA>     5  <NA>  <NA>     3  <NA>     1  \n",
       "7   <NA>  <NA>  <NA>     5  <NA>  <NA>     3  <NA>  <NA>  \n",
       "8   <NA>  <NA>  <NA>  <NA>  <NA>  <NA>     3  <NA>  <NA>  \n",
       "9   <NA>  <NA>  <NA>  <NA>  <NA>  <NA>     3  <NA>  <NA>  \n",
       "10  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  \n",
       "11  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  \n",
       "\n",
       "[12 rows x 117 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test for motif_init2df\n",
    "df_sequences=motif_init2df(input_path=\"./init_concat.csv\")\n",
    "df_sequences\n",
    "# test passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6c1e0c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_reverse_and_push_nan(df):\n",
    "        # Reverse rows of each column except 'position'\n",
    "        df_rev = df.loc[:, df.columns != 'position'].apply(lambda col: col[::-1].values, axis=0)\n",
    "        # Add the 'position' column back without changing its order\n",
    "        df_rev.insert(0, 'position', df['position'])\n",
    "        # Reset row index to ensure continuous row index\n",
    "        df_rev = df_rev.reset_index(drop=True)\n",
    "\n",
    "        # Push NaN values to the end of each column except 'position'\n",
    "        for col in df_rev.columns:\n",
    "            if col != 'position':\n",
    "                non_nan = df_rev[col].dropna()\n",
    "                nan_count = df_rev[col].isna().sum()\n",
    "                df_rev[col] = pd.concat([non_nan, pd.Series([pd.NA] * nan_count)], ignore_index=True)\n",
    "        \n",
    "        return df_rev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c43cf69f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>position</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pos 1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pos 2</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pos 3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pos 4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pos 5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Pos 6</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>5</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>4</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Pos 7</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>5</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Pos 8</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Pos 9</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Pos 10</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Pos 11</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Pos 12</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows  117 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   position     0     1     2     3     4     5     6     7     8  ...   106  \\\n",
       "0     Pos 1     5     5     5     7     5     1     5     2     5  ...     1   \n",
       "1     Pos 2     5     5     5     7     5     1     5     2     5  ...     1   \n",
       "2     Pos 3     5     5     5     7     5     1     5     2     5  ...     1   \n",
       "3     Pos 4     5     4     5     2     5     1     5     2     5  ...     1   \n",
       "4     Pos 5     5     4     5     2     5     1     5     1     5  ...     2   \n",
       "5     Pos 6     5     4  <NA>  <NA>     5  <NA>     4  <NA>     5  ...     7   \n",
       "6     Pos 7  <NA>  <NA>  <NA>  <NA>     5  <NA>  <NA>  <NA>     5  ...  <NA>   \n",
       "7     Pos 8  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>     5  ...  <NA>   \n",
       "8     Pos 9  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>     5  ...  <NA>   \n",
       "9    Pos 10  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  ...  <NA>   \n",
       "10   Pos 11  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  ...  <NA>   \n",
       "11   Pos 12  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  ...  <NA>   \n",
       "\n",
       "     107   108   109   110   111   112   113   114   115  \n",
       "0      6     1     1     5     4     3     3     1     1  \n",
       "1      3     1     1     5     4     3     3     1     1  \n",
       "2      3     1     1     5     4     3     3     1     1  \n",
       "3      3     2     2     1     6     6     3     1     1  \n",
       "4      3     7     2     1     6     6     3     2     1  \n",
       "5      3     7  <NA>     1  <NA>  <NA>     1     2     1  \n",
       "6   <NA>  <NA>  <NA>     1  <NA>  <NA>     1  <NA>     1  \n",
       "7   <NA>  <NA>  <NA>     1  <NA>  <NA>     1  <NA>  <NA>  \n",
       "8   <NA>  <NA>  <NA>  <NA>  <NA>  <NA>     1  <NA>  <NA>  \n",
       "9   <NA>  <NA>  <NA>  <NA>  <NA>  <NA>     1  <NA>  <NA>  \n",
       "10  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  \n",
       "11  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  \n",
       "\n",
       "[12 rows x 117 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rev=dataframe_reverse_and_push_nan(df_sequences)\n",
    "df_rev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d44672c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def motif_init2pred_with_dendrogram(input_path=\"./init_concat.csv\", fillna_method=\"ffill\", n_clusters=None, linkage_method=\"average\", threshold=35):\n",
    "    \"\"\"\n",
    "    Read init.csv file and directly predict the class using DTW and Agglomerative Clustering.\n",
    "    This version includes forward-reverse comparison. A dendrogram is provided to help the user \n",
    "    determine the optimal number of clusters.\n",
    "\n",
    "    To run without specifying n_clusters, set n_clusters to None and adjust the threshold (e.g., 10 to 100).\n",
    "    To run with a specified number of clusters, set n_clusters to the desired number.\n",
    "    \"\"\"\n",
    "\n",
    "    def dataframe_reverse_and_push_nan(df):\n",
    "        # Reverse rows of each column except 'position'\n",
    "        df_rev = df.loc[:, df.columns != 'position'].apply(lambda col: col[::-1].values, axis=0)\n",
    "        # Add the 'position' column back without changing its order\n",
    "        df_rev.insert(0, 'position', df['position'])\n",
    "        # Reset row index to ensure continuous row index\n",
    "        df_rev = df_rev.reset_index(drop=True)\n",
    "\n",
    "        # Push NaN values to the end of each column except 'position'\n",
    "        for col in df_rev.columns:\n",
    "            if col != 'position':\n",
    "                non_nan = df_rev[col].dropna()\n",
    "                nan_count = df_rev[col].isna().sum()\n",
    "                df_rev[col] = pd.concat([non_nan, pd.Series([pd.NA] * nan_count)], ignore_index=True)\n",
    "        \n",
    "        return df_rev\n",
    "\n",
    "    df_sequences = motif_init2df(input_path=input_path)\n",
    "    # print(f\"Initial number of entries: {len(df_sequences)}\")  # Debug print\n",
    "\n",
    "    X_train = df_sequences.loc[:, df_sequences.columns != 'position']\n",
    "    if fillna_method == 0:\n",
    "        X_train_filled = X_train.fillna(0)\n",
    "    elif fillna_method == \"ffill\":\n",
    "        X_train_filled = X_train.fillna(method=fillna_method)\n",
    "    # Add more conditions for other fillna methods if needed\n",
    "    n_columns = X_train_filled.shape[1]\n",
    "    # print(f\"Number of entries after filling NaNs: {n_columns}\")  # Debug print\n",
    "\n",
    "    dtw_distance_matrix = np.zeros((n_columns, n_columns))\n",
    "\n",
    "    df_sequences_rev = dataframe_reverse_and_push_nan(df_sequences)  # Reverse\n",
    "    X_train_rev = df_sequences_rev.loc[:, df_sequences_rev.columns != 'position']\n",
    "    if fillna_method == 0:\n",
    "        X_train_filled_rev = X_train_rev.fillna(0)\n",
    "    elif fillna_method == \"ffill\":\n",
    "        X_train_filled_rev = X_train_rev.fillna(method=fillna_method)\n",
    "    # Add more conditions for other fillna methods if needed\n",
    "\n",
    "    import time\n",
    "    start_dtw = time.time()\n",
    "    for i in range(n_columns):\n",
    "        for j in range(i, n_columns):  # No need to compute the distance twice for (i, j) and (j, i)\n",
    "            distance_ff = dtw(X_train_filled.iloc[:, i].values, X_train_filled.iloc[:, j].values)  # Forward-forward\n",
    "            distance_fr = dtw(X_train_filled.iloc[:, i].values, X_train_filled_rev.iloc[:, j].values)  # Forward-reverse\n",
    "\n",
    "            # Select the minimum distance and assign symmetrically\n",
    "            min_distance = min(distance_ff, distance_fr)\n",
    "            dtw_distance_matrix[i, j] = min_distance\n",
    "            dtw_distance_matrix[j, i] = min_distance\n",
    "            \n",
    "    end_dtw = time.time()\n",
    "    print(f\"DTW computation time: {end_dtw - start_dtw} seconds\")\n",
    "\n",
    "    # Measure time for clustering\n",
    "    start_clustering = datetime.now()\n",
    "    \n",
    "    # Added part for dendrogram\n",
    "    # Create the linkage matrix\n",
    "    Z = linkage(dtw_distance_matrix, method=linkage_method)\n",
    "    \n",
    "    # Plot the dendrogram\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    dendrogram(Z)\n",
    "    plt.title('Dendrogram')\n",
    "    plt.xlabel('Sample index')\n",
    "    plt.ylabel('Distance')\n",
    "    plt.show()\n",
    "    # End of added part for dendrogram\n",
    "\n",
    "    if n_clusters is None:\n",
    "        # Determine clusters using fcluster with a distance threshold\n",
    "        y_pred = fcluster(Z, threshold, criterion='distance')\n",
    "        estimated_clusters = len(np.unique(y_pred))\n",
    "        print(f\"Estimated number of clusters: {estimated_clusters}\")\n",
    "    else:\n",
    "        # Use Agglomerative Clustering with the precomputed DTW distance matrix\n",
    "        clustering = AgglomerativeClustering(n_clusters=n_clusters, metric='precomputed', linkage=linkage_method)\n",
    "        y_pred = clustering.fit_predict(dtw_distance_matrix)\n",
    "        estimated_clusters = n_clusters\n",
    "    \n",
    "    end_clustering = datetime.now()\n",
    "    print('Clustering Duration: {}'.format(end_clustering - start_clustering))\n",
    "    \n",
    "    print(f\"Number of cluster estimated by dendrogram with designated threshold {threshold}: [{estimated_clusters}] clusters\")\n",
    "\n",
    "    return y_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bcd804c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test for motif_init2pred_with_dendrogram\n",
    "# motif_init2pred_with_dendrogram(input_path=\"./init_concat.csv\")\n",
    "# # test passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "64ff2faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def motif_init2pred_incl_ff_fr(input_path=\"./init_concat.csv\", fillna_method=\"ffill\", n_clusters=11, linkage_method=\"complete\"):\n",
    "    \"\"\"\n",
    "    Read init.csv file and directly predict the class using DTW and Agglomerative Clustering.\n",
    "    This version includes forward-reverse comparison.\n",
    "    \"\"\"\n",
    "\n",
    "    def dataframe_reverse_and_push_nan(df):\n",
    "        # Reverse rows of each column except 'position'\n",
    "        df_rev = df.loc[:, df.columns != 'position'].apply(lambda col: col[::-1].values, axis=0)\n",
    "        # Add the 'position' column back without changing its order\n",
    "        df_rev.insert(0, 'position', df['position'])\n",
    "        # Reset row index to ensure continuous row index\n",
    "        df_rev = df_rev.reset_index(drop=True)\n",
    "\n",
    "        # Push NaN values to the end of each column except 'position'\n",
    "        for col in df_rev.columns:\n",
    "            if col != 'position':\n",
    "                non_nan = df_rev[col].dropna()\n",
    "                nan_count = df_rev[col].isna().sum()\n",
    "                df_rev[col] = pd.concat([non_nan, pd.Series([pd.NA] * nan_count)], ignore_index=True)\n",
    "        \n",
    "        return df_rev\n",
    "\n",
    "    df_sequences = motif_init2df(input_path=input_path)\n",
    "    # print(f\"Initial number of entries: {len(df_sequences)}\")  # Debug print\n",
    "\n",
    "    X_train = df_sequences.loc[:, df_sequences.columns != 'position']\n",
    "    if fillna_method==0:\n",
    "        X_train_filled = X_train.fillna(0)\n",
    "    if fillna_method==\"ffill\":\n",
    "        X_train_filled = X_train.fillna(method=fillna_method) \n",
    "    n_columns = X_train_filled.shape[1]\n",
    "    # print(f\"Number of entries after filling NaNs: {n_columns}\")  # Debug print\n",
    "\n",
    "    dtw_distance_matrix = np.zeros((n_columns, n_columns))\n",
    "\n",
    "    df_sequences_rev = dataframe_reverse_and_push_nan(df_sequences)  # Reverse\n",
    "    X_train_rev = df_sequences_rev.loc[:, df_sequences_rev.columns != 'position']\n",
    "    if fillna_method==0:\n",
    "        X_train_filled_rev = X_train_rev.fillna(0)\n",
    "    if fillna_method==\"ffill\":\n",
    "        X_train_filled_rev = X_train_rev.fillna(method=fillna_method) \n",
    "    # X_train_filled_rev = X_train_rev.fillna(fillna_method)  # Fill missing values with zero\n",
    "    \n",
    "    import time\n",
    "    start_dtw = time.time()\n",
    "    for i in range(n_columns):\n",
    "        for j in range(i, n_columns):  # No need to compute the distance twice for (i, j) and (j, i)\n",
    "            distance_ff = dtw(X_train_filled.iloc[:, i].values, X_train_filled.iloc[:, j].values)  # Forward-forward\n",
    "            distance_fr = dtw(X_train_filled.iloc[:, i].values, X_train_filled_rev.iloc[:, j].values)  # Forward-reverse\n",
    "\n",
    "            # Select the minimum distance and assign symmetrically\n",
    "            min_distance = min(distance_ff, distance_fr)\n",
    "            dtw_distance_matrix[i, j] = min_distance\n",
    "            dtw_distance_matrix[j, i] = min_distance\n",
    "            \n",
    "    end_dtw = time.time()\n",
    "    print(f\"DTW computation time: {end_dtw - start_dtw} seconds\")\n",
    "\n",
    "    # Measure time for clustering\n",
    "    start_clustering = datetime.now()\n",
    "    \n",
    "    # Use Agglomerative Clustering with the precomputed DTW distance matrix\n",
    "    # for linkage option, \n",
    "    # complete: mazimize the minimum distance between points in different clusters\n",
    "    # average: uses the average distance between all points in the two clusters\n",
    "    # single: minimize the distance between the closest points of the clusters\n",
    "    clustering = AgglomerativeClustering(n_clusters=n_clusters, metric='precomputed', linkage=linkage_method)\n",
    "    y_pred = clustering.fit_predict(dtw_distance_matrix)\n",
    "    \n",
    "    end_clustering = datetime.now()\n",
    "    print('Clustering Duration: {}'.format(end_clustering - start_clustering))\n",
    "    \n",
    "    print(f\"Number of cluster labels: {len(y_pred)}\")  # Debug print\n",
    "    # return X_train_filled, X_train_filled_rev, y_pred\n",
    "    return y_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1b703e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def motif_init2class_df_incl_ff_fr(input_path=\"./init_concat.csv\", fillna_method=\"ffill\", n_clusters=11, linkage_method=\"complete\"): #,fillna_method='ffill'):\n",
    "    df_sequences=motif_init2df(input_path=input_path)\n",
    "\n",
    "    # Transpose df_test so that each entry becomes a row\n",
    "    df_seq_transposed = df_sequences.T  \n",
    "    # The first row will likely contain something other than data (e.g., time points), so let's keep it as a header\n",
    "    new_header = df_seq_transposed.iloc[0]  # Grab the first row for the header\n",
    "    df_seq_transposed = df_seq_transposed[1:]  # Take the data less the header row\n",
    "    df_seq_transposed.columns = new_header  # Set the header row as the df header\n",
    "    # Reset the index to make the entries into a column\n",
    "    df_seq_transposed.reset_index(inplace=True)\n",
    "    # Rename the 'index' column to something more descriptive, like 'Entry'\n",
    "    df_seq_transposed.rename(columns={'index': 'Entry'}, inplace=True)\n",
    "\n",
    "    y_pred=motif_init2pred_incl_ff_fr(input_path=input_path,fillna_method=fillna_method,  n_clusters=n_clusters, linkage_method=linkage_method)\n",
    "\n",
    "    # Add the cluster labels as a new column\n",
    "    df_seq_transposed['Cluster'] = y_pred\n",
    "    # Sort the DataFrame by the 'Cluster' column\n",
    "    df_sorted_by_cluster = df_seq_transposed.sort_values(by='Cluster')\n",
    "    # Reset the index of the sorted DataFrame\n",
    "    df_sorted_by_cluster.reset_index(drop=True, inplace=True)\n",
    "    # Display the sorted DataFrame\n",
    "    # df_sorted_by_cluster\n",
    "    # Reverse the letter_to_num mapping\n",
    "    letter_to_num = {'A': 1,'B': 2,'C': 3,'D': 4,'E': 5,'F': 6,'G': 7,\n",
    "                        'H': 8,'I': 9,'J': 10,'K': 11,'L': 12,'M': 13,'N': 14,'O': 15}\n",
    "    num_to_letter = {v: k for k, v in letter_to_num.items()}\n",
    "\n",
    "    # Function to convert a series of numbers to a letter string, ignoring NaNs\n",
    "    def series_to_letters(series):\n",
    "        return ''.join([num_to_letter.get(x, '') for x in series if pd.notna(x)])\n",
    "\n",
    "    # Apply the conversion to each row (excluding the 'Cluster' column) and add the result to a new column\n",
    "    df_sorted_by_cluster['LetterSequence'] = df_sorted_by_cluster.drop('Cluster', axis=1).apply(series_to_letters, axis=1)\n",
    "\n",
    "    # Group by 'Cluster' and aggregate 'LetterSequence' into lists\n",
    "    clustered_sequences = df_sorted_by_cluster.groupby('Cluster')['LetterSequence'].apply(list).reset_index()\n",
    "\n",
    "    # Display the result\n",
    "    return clustered_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d8f726d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def motif_init2cluster_vis_all(input_path=\"./init_concat.csv\", n_clusters=11, fillna_method=\"ffill\", linkage_method=\"complete\", random_state=95, font_scale=0.004,font_v_scale=9, fig_w=10, fig_h=10, node_size=600, node_dist=0.05):\n",
    "    clustered_sequences=motif_init2class_df_incl_ff_fr(input_path=input_path, fillna_method=fillna_method, n_clusters=n_clusters, linkage_method=linkage_method)\n",
    "    scale_factor = font_scale  # Adjust this to change the font size\n",
    "\n",
    "    def create_text_patch(x, y, text, state_col_dict_num, ax, scale_factor):\n",
    "        # Determine the starting x position for the first letter\n",
    "        x_offset = x\n",
    "        for letter in text:\n",
    "            color = state_col_dict_num.get(letter, (0, 0, 0))\n",
    "            # fp = FontProperties(family=\"Arial\", weight=\"bold\")\n",
    "            fp = FontProperties(family=\"DejaVu Sans\", weight=\"bold\")\n",
    "            tp = TextPath((0, 0), letter, prop=fp)\n",
    "            tp_transformed = transforms.Affine2D().scale(scale_factor).translate(x_offset, y) + ax.transData\n",
    "            letter_patch = PathPatch(tp, color=color, lw=0, transform=tp_transformed)\n",
    "            ax.add_patch(letter_patch)\n",
    "            # Get the width of the letter and add a small margin\n",
    "            letter_width = tp.get_extents().width * scale_factor\n",
    "            x_offset += letter_width  # Increment the x position by the width of the letter\n",
    "\n",
    "    df = clustered_sequences\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(fig_w, fig_h))  # Adjust figure size as needed\n",
    "    \n",
    "    ##### color modification for temporary use #####\n",
    "    # Create a temporary copy with a different name\n",
    "    temp_state_col_dict_num = state_col_dict_num.copy()\n",
    "\n",
    "    # Modify the colors in the temporary dictionary\n",
    "    # Update 'G' to a more visible color, such as a deep orange\n",
    "    temp_state_col_dict_num['G'] = (1.0, 0.647, 0.0)  # Normalized deep orange\n",
    "\n",
    "    # Update 'O' to ensure it stands out more, such as a darker gray\n",
    "    temp_state_col_dict_num['O'] = (0.502, 0.502, 0.502)  # Normalized darker gray\n",
    "\n",
    "    ################################################\n",
    "\n",
    "    # Create a graph\n",
    "    G = nx.Graph()\n",
    "    for index, row in df.iterrows():\n",
    "        G.add_node(row['Cluster'], elements=row['LetterSequence'])\n",
    "\n",
    "    # Significantly increase the base size for each node\n",
    "    base_node_size = node_size  # This increases the node size\n",
    "    node_sizes = [len(elements) * base_node_size for elements in df['LetterSequence']]\n",
    "\n",
    "    # Generate a color palette with a unique color for each node\n",
    "    colors = plt.cm.rainbow(np.linspace(0, 1, len(df)))\n",
    "\n",
    "    np.random.seed(random_state)\n",
    "    # Draw the graph with a spring layout\n",
    "    # Adjust k to manage the distance between nodes, which can be smaller since nodes can overlap\n",
    "    pos = nx.spring_layout(G, k=node_dist, iterations=10)\n",
    "\n",
    "    # Draw the nodes themselves\n",
    "    nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color=colors, alpha=0.3)\n",
    "\n",
    "    # Draw the text\n",
    "    for node, (node_pos, elements) in enumerate(zip(pos.values(), df['LetterSequence'])):      \n",
    "        x_start, y_start = node_pos\n",
    "        for i, element in enumerate(elements):\n",
    "            x_position = x_start - 0.08\n",
    "            y_position = y_start - (i * scale_factor * font_v_scale) + 0.015*len(elements)# Adjust line spacing\n",
    "            create_text_patch(x_position, y_position, element, temp_state_col_dict_num, ax, scale_factor)\n",
    "#             print(\"state_col_dict_num\", state_col_dict_num)\n",
    "\n",
    "    plt.axis('off')\n",
    "    print(n_clusters)\n",
    "    plt.show()\n",
    "    \n",
    "    fig.savefig(\"./cluster_result.png\",bbox_inches='tight', dpi=300,facecolor='white',  # Set the background color to white\n",
    "    edgecolor='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bdd9dec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test\n",
    "# motif_init2cluster_vis_all(input_path=\"./init_concat.csv\", n_clusters=11, fillna_method=\"ffill\", linkage_method=\"complete\", random_state=95, font_scale=0.004,font_v_scale=9, fig_w=10, fig_h=10, node_size=800, node_dist=0.05)\n",
    "# # test passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "cd8ba84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def motif_init2umap(input_path=\"./init_concat.csv\", n_clusters=11, n_neighbors=5, min_dist=0.3, random_state=111):\n",
    "    \"\"\"\n",
    "    Generate a UMAP embedding of the given data.\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    - input_path: .csv file of all motifs with high attention score\n",
    "    \n",
    "    - n_clusters: number of clusters\n",
    "\n",
    "    - n_neighbors: int (default=5), The size of local neighborhood (in terms of number of neighboring sample points) \n",
    "      used for manifold approximation. Larger values result in a more global view of the manifold, while smaller values emphasize local data structures. \n",
    "      Adjust according to the desired granularity of the embedding.\n",
    "      \n",
    "    - mid_dist: float (default=0.3), The minimum distance between embedded points in the low-dimensional space. \n",
    "      Smaller values allow points to cluster more tightly in the embedding, which is useful for identifying finer substructures within the data. \n",
    "      Larger values help preserve the overall topology of the data by preventing points from clustering too tightly.\n",
    "    \"\"\"\n",
    "    df_sequences = motif_init2df(input_path=input_path)\n",
    "    X_train = df_sequences.loc[:, df_sequences.columns != 'position']\n",
    "    X_train = X_train.astype('float64')  # Convert to float64\n",
    "    X_filled = X_train.fillna(X_train.mean())\n",
    "\n",
    "    y_pred = motif_init2pred_incl_ff_fr(input_path=input_path, n_clusters=n_clusters)\n",
    "\n",
    "    # Now apply UMAP on the cleaned data\n",
    "    from umap import UMAP\n",
    "    # # seed=111\n",
    "    # # umap_reducer = UMAP(n_neighbors=n_neighbors, min_dist=min_dist, random_state=seed)\n",
    "    # umap_reducer = UMAP(n_neighbors=n_neighbors, min_dist=min_dist)\n",
    "    umap_reducer = UMAP(n_neighbors=n_neighbors, min_dist=min_dist, random_state=random_state, n_jobs=1)\n",
    "\n",
    "    umap_embedding = umap_reducer.fit_transform(X_filled.T)  # Ensure the data is transposed if necessary\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    scatter = plt.scatter(umap_embedding[:, 0], umap_embedding[:, 1], c=y_pred, cmap='Spectral', s=100, edgecolors='white', linewidth=0.6)\n",
    "\n",
    "    # Create a color bar with ticks for each cluster label\n",
    "    colorbar = plt.colorbar(scatter, ticks=np.arange(0, 11))\n",
    "    colorbar.set_label('Cluster label')\n",
    "\n",
    "    # Set the plot title and labels\n",
    "    plt.title('UMAP Projection After K-means clustering', fontsize=20)\n",
    "    plt.xlabel('UMAP Dimension 1', fontsize=15)\n",
    "    plt.ylabel('UMAP Dimension 2', fontsize=15)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "98d21b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test for motif_init2umap\n",
    "# motif_init2umap(input_path=\"./init_concat.csv\", n_clusters=11, n_neighbors=5, min_dist=0.3, random_state=111)\n",
    "# test passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9696e3ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (chrombert_test_v2)",
   "language": "python",
   "name": "chrombert_test_v2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
